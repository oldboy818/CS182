{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Algorithms\n",
    "In the DQN algorithm, we learned a Q-function by minimizing Bellman errors for an implicit policy that always took the action that maximized the Q-function. However, this scheme requires a discrete action spaces to allow for us to easily compute the optimal action at each state, unlike generic policy gradient algorithms that also worked with continuous action spaces.\n",
    "\n",
    "In this section, we will explore actor-critic algorithms which maintain an explicit policy (actor) like the policy gradient algorithms, learns a Q-function (critic) capturing the values of the _current policy_, and uses this learned Q-function to update the policy. Using a learned critic can provide much lower variance updates for the policy compared to using Monte-Carlo retun estimates, and also allows us to reuse our data by training the actor and critic on _off-policy_ data for more sample efficiency. We can thus take many more policy updates with an actor critic algorithm using our learned critic, instead of needing to wait and gather fresh samples every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import deeprl.infrastructure.pytorch_util as ptu\n",
    "from deeprl.infrastructure.rl_trainer import RL_Trainer\n",
    "from deeprl.infrastructure.trainers import AC_Trainer\n",
    "\n",
    "from deeprl.policies.MLP_policy import MLPPolicyAC\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def remove_folder(path):\n",
    "    # check if folder exists\n",
    "    if os.path.exists(path): \n",
    "        print(\"Clearing old results at {}\".format(path))\n",
    "        # remove if exists\n",
    "        shutil.rmtree(path)\n",
    "    else:\n",
    "        print(\"Folder {} does not exist yet. No old results to delete\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_base_args_dict = dict(\n",
    "    env_name = 'Hopper-v2', #@param ['Ant-v2', 'Humanoid-v2', 'Walker2d-v2', 'HalfCheetah-v2', 'Hopper-v2']\n",
    "    exp_name = 'test_ac', #@param\n",
    "    save_params = False, #@param {type: \"boolean\"}\n",
    "    \n",
    "    ## PDF will tell you how to set ep_len\n",
    "    ## and discount for each environment\n",
    "    ep_len = 200, #@param {type: \"integer\"}\n",
    "    discount = 0.99, #@param {type: \"number\"}\n",
    "\n",
    "    # Training\n",
    "    num_agent_train_steps_per_iter = 1000, #@param {type: \"integer\"})\n",
    "    n_iter = 100, #@param {type: \"integer\"})\n",
    "\n",
    "    # batches & buffers\n",
    "    batch_size = 1000, #@param {type: \"integer\"})\n",
    "    eval_batch_size = 1000, #@param {type: \"integer\"}\n",
    "    train_batch_size = 256, #@param {type: \"integer\"}\n",
    "    max_replay_buffer_size = 1000000, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown actor network\n",
    "    n_layers = 2, #@param {type: \"integer\"}\n",
    "    size = 256, #@param {type: \"integer\"}\n",
    "    entropy_weight=0, #@param {type: \"number\"}\n",
    "    learning_rate = 3e-4, #@param {type: \"number\"}\n",
    "    \n",
    "    # critic network\n",
    "    critic_n_layers = 2, #@param {type: \"integer\"}\n",
    "    critic_size = 256, #@param {type: \"integer\"}\n",
    "    target_update_rate = 5e-3,\n",
    "\n",
    "    #@markdown logging\n",
    "    video_log_freq = -1, #@param {type: \"integer\"}\n",
    "    scalar_log_freq = 1, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown gpu & run-time settings\n",
    "    no_gpu = False, #@param {type: \"boolean\"}\n",
    "    which_gpu = 0, #@param {type: \"integer\"}\n",
    "    seed = 2, #@param {type: \"integer\"}\n",
    "    logdir = 'test',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First fill out the target value calculation in the compute_target_value method of <code>critics/bootstrapped_continuous_critic.py</code>. Compared to the DQN critic, the key difference is that we are now estimating the value of the current policy, instead of the optimal policy as in DQN or Q-learning.\n",
    "\n",
    "To train our critic to evaluate the current policy $\\pi$, we simply sample actions from the current policy in our target value. For each sample $(s,a,s')$, our loss will be\n",
    "$$L(Q_{\\theta}(s, a), r(s,a) + \\gamma \\mathbb{E}_{a'\\sim \\pi(s')}[Q_{\\bar \\theta} (s', a')]),$$\n",
    "where $L$ is our loss function (for example squared error or the smooth L1 loss).\n",
    "In this assignment, we will simply sample a single action from the policy to estimate the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  test\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Import error. Trying to rebuild mujoco_py.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ld: warning: duplicate -rpath '/Users/jangdong-eon/miniforge3/envs/hw4/lib' ignored\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py:12\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/mujoco_py/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cymj, ignore_mujoco_warnings, functions, MujocoException\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m const\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/mujoco_py/builder.py:504\u001b[0m\n\u001b[1;32m    503\u001b[0m mujoco_path \u001b[38;5;241m=\u001b[39m discover_mujoco()\n\u001b[0;32m--> 504\u001b[0m cymj \u001b[38;5;241m=\u001b[39m \u001b[43mload_cython_ext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmujoco_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Trick to expose all mj* functions from mujoco in mujoco_py.*\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/mujoco_py/builder.py:111\u001b[0m, in \u001b[0;36mload_cython_ext\u001b[0;34m(mujoco_path)\u001b[0m\n\u001b[1;32m    110\u001b[0m         cext_so_path \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[0;32m--> 111\u001b[0m         mod \u001b[38;5;241m=\u001b[39m \u001b[43mload_dynamic_ext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcymj\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcext_so_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mod\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/mujoco_py/builder.py:130\u001b[0m, in \u001b[0;36mload_dynamic_ext\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m    129\u001b[0m loader \u001b[38;5;241m=\u001b[39m ExtensionFileLoader(name, path)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mminiforge3/envs/hw4/lib/python3.8/site-packages/mujoco_py/cymj.pyx:1\u001b[0m, in \u001b[0;36minit mujoco_py.cymj\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m ac_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-v2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env_str)\n\u001b[1;32m     18\u001b[0m ac_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy_weight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m---> 19\u001b[0m actrainer \u001b[38;5;241m=\u001b[39m \u001b[43mAC_Trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mac_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m critic \u001b[38;5;241m=\u001b[39m actrainer\u001b[38;5;241m.\u001b[39mrl_trainer\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mcritic\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDummyDist\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UOS/MAC/AI/CS182/CS182_HW/CS182/assignment4/deeprl/infrastructure/trainers.py:165\u001b[0m, in \u001b[0;36mAC_Trainer.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size_initial\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m################\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m## RL TRAINER\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m################\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrl_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mRL_Trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UOS/MAC/AI/CS182/CS182_HW/CS182/assignment4/deeprl/infrastructure/rl_trainer.py:59\u001b[0m, in \u001b[0;36mRL_Trainer.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 지정된 이름의 gym 환경 개체를 생성\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menv_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Atari 환경에서 비디오 로깅을 위해 환경에 추가적인 처리를 위한 래퍼들을 적용\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_wrappers\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# These operations are currently only for Atari envs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:145\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:90\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaking new env: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, path)\n\u001b[1;32m     89\u001b[0m spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec(path)\n\u001b[0;32m---> 90\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# We used to have people override _reset/_step rather than\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# your environment if you use these methods and don't want\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# compatibility code to be invoked.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_reset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_step\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_gym_disable_underscore_compat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:59\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Make the environment aware of which spec it came from.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:18\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(name):\n\u001b[1;32m     17\u001b[0m     mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/mujoco/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MujocoEnv\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ^^^^^ so that user gets the correct error\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# message if mujoco is not installed correctly\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AntEnv\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n\u001b[1;32m     16\u001b[0m DEFAULT_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_observation_to_space\u001b[39m(observation):\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)"
     ]
    }
   ],
   "source": [
    "# Test bellman error for policy evaluation\n",
    "ac_dim = 3\n",
    "ob_dim = 11\n",
    "N = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(N, ob_dim))\n",
    "acts = np.random.choice(ac_dim, size=(N,))\n",
    "next_obs = np.random.normal(size=(N, ob_dim))\n",
    "rewards = np.random.normal(size=N)\n",
    "terminals = np.zeros(N)\n",
    "terminals[0] = 1\n",
    "\n",
    "ac_args = dict(ac_base_args_dict)\n",
    "\n",
    "env_str = 'Hopper'\n",
    "ac_args['env_name'] = '{}-v2'.format(env_str)\n",
    "ac_args['entropy_weight'] = 0.1\n",
    "actrainer = AC_Trainer(ac_args)\n",
    "critic = actrainer.rl_trainer.agent.critic\n",
    "\n",
    "class DummyDist:\n",
    "    def sample(self):\n",
    "        return ptu.from_numpy(1 + np.zeros(shape=(N, ac_dim)))\n",
    "\n",
    "def dummy_actor(next_obs):\n",
    "    return DummyDist()\n",
    "\n",
    "# assumes you call actor(next_obs) to get the distribution, then call distribution.sample()\n",
    "target_vals = critic.compute_target_value(ptu.from_numpy(next_obs), \n",
    "                                          ptu.from_numpy(rewards), \n",
    "                                          ptu.from_numpy(terminals), \n",
    "                                          dummy_actor)\n",
    "target_vals = ptu.to_numpy(target_vals)\n",
    "expected_targets = np.array([-0.9167948, -0.11123351, -0.36787638, -2.1131861,  -0.13868617])\n",
    "\n",
    "target_error = rel_error(target_vals, expected_targets)\n",
    "print(\"Target value error\", target_error, \"should be on the order of 1e-6 or lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we will also update our target network parameters as an exponential moving average of the critic parameters, instead of simply copying the current parameters periodically as in DQN. Generally, either method for target networks tends to work with appropriately chosen update rates.\n",
    "\n",
    "Fill out the update_target_parameter_ema method in <code>critics/bootstrapped_continuous_critic.py</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test target network update\n",
    "ac_args = dict(ac_base_args_dict)\n",
    "\n",
    "env_str = 'Hopper'\n",
    "ac_args['env_name'] = '{}-v2'.format(env_str)\n",
    "ac_args['entropy_weight'] = 0.1\n",
    "actrainer = AC_Trainer(ac_args)\n",
    "critic = actrainer.rl_trainer.agent.critic\n",
    "\n",
    "critic.target_update_rate = 0.5\n",
    "\n",
    "# at initialization, target and critic networks are the same\n",
    "for p in critic.critic_network.parameters():\n",
    "    p.data += 1.\n",
    "    \n",
    "critic.update_target_network_ema()\n",
    "\n",
    "for p, target_p in zip(critic.critic_network.parameters(), critic.target_network.parameters()):\n",
    "    assert np.all(ptu.to_numpy((p-target_p)) == 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement the actor update using the learned critic instead of Monte Carlo returns. \n",
    "\n",
    "To update our policy at a particular state $s$, our previous policy gradient (using the reward to go estimator) took a step on the objective (treating $Q^{\\pi}$ as a function that didn't depend on $\\pi$ and using the results of a single trajectory to estimate $Q^\\pi$)\n",
    "$$\\mathbb E_{a \\sim \\pi_{\\theta}(s)}[Q^{\\pi_\\theta}(s, a)],$$\n",
    "using the REINFORCE gradient estimator \n",
    "$$\\mathbb E_{a \\sim \\pi_{\\theta}(s)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a\\vert s) Q^{\\pi}(s, a)].$$\n",
    "This estimator only relied on the estimated value $Q^{\\pi_{\\theta}}(s,a)$, so was very general and could be applied with Monte Carlo estimates of $Q^{\\pi_\\theta}$.\n",
    "\n",
    "One way to estimate policy gradients with an actor critic algorithm would be to directly replace the Monte Carlo estimate of $Q$ with the learned critic $Q_{\\phi}$, and continue using the REINFORCE gradient estimator.\n",
    "However, we note that we can explicitly compute derivatives of our learned critic $Q(s, a)$ with respect to the action $a$, which can enable potentially better gradient estimates. \n",
    "\n",
    "In order to take advantage of this, we would also need to differentiate sampled actions $a$ with respect to our policy parameters, which we can through a technique known as the _reparameterization trick_ or the _pathwise_ estimator. \n",
    "The idea is that if our policy sampled actions according to $a \\sim \\mathcal N(\\mu_{\\theta}(s), \\sigma^2_{\\theta}(s))$, we can rewrite $a = f_{\\theta}(z)$, where $z \\sim \\mathcal N(0, 1)$, and $f(z) = \\mu_{\\theta}(s) + z \\cdot \\sigma_{\\theta}(s)$. Now all the randomness comes from sampling $z$, which doesn't depend on our policy, so we can now differentiate the sampled action $a$ with respect to our policy parameters $\\theta$ by simply differentiating through the function $f$ applied at the random noise $z$. \n",
    "\n",
    "Using the chain rule then allows to directly estimate gradients of \n",
    "$$\\mathbb{E}_{a \\sim \\pi_{\\theta}(s)}[Q_{\\phi}(s,a)]$$\n",
    "by drawing samples from $\\pi$ and differentiating $Q_{\\phi}(s,a)$ on the samples. \n",
    "\n",
    "Implement the actor update using this pathwise estimator in the update method of the MLPPolicyAC class in <code>policies/MLP_policy.py</code> (Hint: see the rsample function in for torch.distributions). Note that our implementation samples states uniformly from the entire replay buffer, not necessarily from the state distribution of the current policy. While this means we are no longer taking unbiased policy gradients (our estimates were already biased anyways due to using a learned critic), it works well in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actor update using the policy gradient. \n",
    "# For this test to pass, make sure you only call sample once per actor update to not throw off \n",
    "# the actor samples expected for the updates in the this test.\n",
    "torch.manual_seed(0)\n",
    "ac_dim = 2\n",
    "ob_dim = 3\n",
    "batch_size = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(N, ob_dim))\n",
    "\n",
    "policy = MLPPolicyAC(\n",
    "            ac_dim=ac_dim,\n",
    "            ob_dim=ob_dim,\n",
    "            n_layers=1,\n",
    "            size=2,\n",
    "            learning_rate=0.25,\n",
    "            entropy_weight=0.)\n",
    "\n",
    "def dummy_critic(obs, acts):\n",
    "    return torch.sum(acts + 1) + torch.sum(obs)\n",
    "\n",
    "initial_loss = policy.update(obs, dummy_critic)['Actor Training Loss']\n",
    "expected_initial_loss = -17.083496\n",
    "\n",
    "print(\"Initial loss error\", rel_error(expected_initial_loss, initial_loss), \"should be on the order of 1e-6 or less.\")\n",
    "for i in range(5):\n",
    "    loss = policy.update(obs, dummy_critic)['Actor Training Loss']\n",
    "    print(loss)\n",
    "\n",
    "expected_final_loss = -30.103575\n",
    "\n",
    "print(\"Final loss error\", rel_error(expected_final_loss, loss), \"should be on the order of 1e-6 or less.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train our actor critic agent on the HalfCheetah task. You should see your policies generally get over 600 returns. \n",
    "\n",
    "We note that these actor critic algorithms, since they make use of off-policy updates, can be much more sample efficient than the basic policy gradient algorith we saw earlier. In our actor critic algorithms here, we only take 1000 new samples from the environment per iteration, while the policy gradient algorithms often needed many more samples per iteration to estimate the Monte Carlo returns (for example, we used 10000 in the Hopper experiments with policy gradient).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_args = dict(ac_base_args_dict)\n",
    "\n",
    "env_str = 'HalfCheetah'\n",
    "ac_args['env_name'] = '{}-v2'.format(env_str)\n",
    "ac_args['n_iter'] = 50\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/actor_critic/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running actor critic experiment with seed\", seed)\n",
    "    ac_args['seed'] = seed\n",
    "    ac_args['logdir'] = 'logs/actor_critic/{}/seed{}'.format(env_str, seed)\n",
    "    actrainer = AC_Trainer(ac_args)\n",
    "    actrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize Actor Critic results on Halfheetah\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/actor_critic/HalfCheetah"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
