{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks\n",
    "Previously, we trained policies using policy gradient algorithms, which directly estimated the gradient of the returns for the policy and performed stochastic gradient ascent. In this section, we will now implement deep Q-learning, which does not explicitly optimize a policy, but simply infers the policy from a learned Q function that is trained via dynamic programming.\n",
    "\n",
    "We will assume discrete action spaces for this notebook as to enable us to easily select actions that maximize the Q-function at given states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import deeprl.infrastructure.pytorch_util as ptu\n",
    "\n",
    "from deeprl.infrastructure.rl_trainer import RL_Trainer\n",
    "from deeprl.infrastructure.trainers import PG_Trainer\n",
    "from deeprl.infrastructure.trainers import DQN_Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def remove_folder(path):\n",
    "    # check if folder exists\n",
    "    if os.path.exists(path): \n",
    "        print(\"Clearing old results at {}\".format(path))\n",
    "        # remove if exists\n",
    "        shutil.rmtree(path)\n",
    "    else:\n",
    "        print(\"Folder {} does not exist yet. No old results to delete\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_base_args_dict = dict(\n",
    "    env_name = 'LunarLander-v3', #@param \n",
    "    exp_name = 'test_dqn', #@param\n",
    "    save_params = False, #@param {type: \"boolean\"}\n",
    "    \n",
    "    ## PDF will tell you how to set ep_len\n",
    "    ## and discount for each environment\n",
    "    ep_len = 200, #@param {type: \"integer\"}\n",
    "    # discount = 0.95, #@param {type: \"number\"}\n",
    "\n",
    "    # Training\n",
    "    num_agent_train_steps_per_iter = 1, #@param {type: \"integer\"})\n",
    "    num_critic_updates_per_agent_update = 1, #@param {type: \"integer\"}\n",
    "  \n",
    "    #@markdown Q-learning parameters\n",
    "    double_q = False, #@param {type: \"boolean\"}\n",
    "\n",
    "    # batches & buffers\n",
    "    batch_size = 32, #@param {type: \"integer\"})\n",
    "    batch_size_initial=1000,\n",
    "\n",
    "    #@markdown logging\n",
    "    video_log_freq = -1, #@param {type: \"integer\"}\n",
    "    scalar_log_freq = 1000, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown gpu & run-time settings\n",
    "    no_gpu = False, #@param {type: \"boolean\"}\n",
    "    which_gpu = 0, #@param {type: \"integer\"}\n",
    "    seed = 2, #@param {type: \"integer\"}\n",
    "    logdir = 'test',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN updates\n",
    "Recall in Q-learning, we attempt to solve the optimal state-action values (which we refer to as Q-values $Q(s,a)$), by finding solutions to the Bellman equation given by\n",
    "$$Q(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(s'\\vert s,a)}[\\max_{a'}Q(s', a')].$$\n",
    "\n",
    "Regular tabular Q-learning would take sample transitions $(s, a, r, s')$ and perform updates according to\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r(s,a) + \\gamma \\max_{a'} Q(s', a') - Q(s,a)),$$\n",
    "where $\\alpha$ is a stepsize parameter.\n",
    "\n",
    "This can be interpreted as updating $Q(s,a)$ by taking one gradient step on a squared Bellman error objective\n",
    "$$(r(s, a) +\\gamma \\max_{a'} \\tilde Q(s', a') - Q(s,a))^2,$$\n",
    "where $\\tilde Q$ is a copy of $Q$, but is not differentiated when taking the gradient step.\n",
    "\n",
    "Adapting this update to the setting where we use a neural network with parameters $\\theta$ to approximate $Q(s,a)$, we then train $\\theta$ with the loss function \n",
    "$$\\min_{\\theta} \\mathbb{E}_{s, a, s' \\sim D} [L(Q_{\\theta}(s,a), r(s,a) + \\gamma \\max_{a'} Q_{\\tilde \\theta}(s', a'))]$$\n",
    "where $D$ is our replay buffer containing past transitions we've experienced, $L$ is some loss function capturing how far the predicted Q-values are from the target values, and $\\tilde \\theta$ are the target Q function parameters, which are usually a delayed copy of $\\theta$ for stability reasons.\n",
    "\n",
    "We note our previous policy gradient algorithms were _on-policy_ algoritms, which meant they updated the policy using only the data collected from the most recent policy, and discard all the data after using it just once. In contrast, DQN uses _off-policy_ updates by sampling data from all past interactions, allowing for data reuse over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the missing components for the basic Q-learning update in <code>critics/dqn_critic.py</code> (not including the double_q section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  test\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/Box2D/Box2D.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module '_Box2D' has no attribute 'RAND_LIMIT_swigconstant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m dqn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-v3\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env_str)\n\u001b[1;32m      6\u001b[0m dqn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble_q\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m dqntrainer \u001b[38;5;241m=\u001b[39m \u001b[43mDQN_Trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdqn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m dqnagent \u001b[38;5;241m=\u001b[39m dqntrainer\u001b[38;5;241m.\u001b[39mrl_trainer\u001b[38;5;241m.\u001b[39magent\n\u001b[1;32m      9\u001b[0m critic \u001b[38;5;241m=\u001b[39m dqnagent\u001b[38;5;241m.\u001b[39mcritic\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UOS/MAC/AI/CS182/CS182_HW/CS182/assignment4/deeprl/infrastructure/trainers.py:123\u001b[0m, in \u001b[0;36mDQN_Trainer.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_wrappers\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m env_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_wrappers\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrl_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mRL_Trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UOS/MAC/AI/CS182/CS182_HW/CS182/assignment4/deeprl/infrastructure/rl_trainer.py:59\u001b[0m, in \u001b[0;36mRL_Trainer.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 지정된 이름의 gym 환경 개체를 생성\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menv_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Atari 환경에서 비디오 로깅을 위해 환경에 추가적인 처리를 위한 래퍼들을 적용\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_wrappers\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# These operations are currently only for Atari envs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:619\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 619\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m mode \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    622\u001b[0m apply_human_rendering \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:62\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name and returns an environment creation function\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    Calls the environment constructor\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UOS/MAC/AI/CS182/CS182_HW/CS182/assignment4/deeprl/envs/box2d/lunar_lander.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/Box2D/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# C++ version copyright 2010 Erin Catto http://www.gphysics.com\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 3. This notice may not be removed or altered from any source distribution.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBox2D\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     21\u001b[0m __author__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$Date$\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/hw4/lib/python3.8/site-packages/Box2D/Box2D.py:435\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"b2CheckPolygon(b2PolygonShape shape, bool additional_checks=True) -> bool\"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _Box2D\u001b[38;5;241m.\u001b[39mb2CheckPolygon(shape, additional_checks)\n\u001b[0;32m--> 435\u001b[0m \u001b[43m_Box2D\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRAND_LIMIT_swigconstant\u001b[49m(_Box2D)\n\u001b[1;32m    436\u001b[0m RAND_LIMIT \u001b[38;5;241m=\u001b[39m _Box2D\u001b[38;5;241m.\u001b[39mRAND_LIMIT\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mb2Random\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module '_Box2D' has no attribute 'RAND_LIMIT_swigconstant'"
     ]
    }
   ],
   "source": [
    "#### Test DQN updates\n",
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = False\n",
    "dqntrainer = DQN_Trainer(dqn_args)\n",
    "dqnagent = dqntrainer.rl_trainer.agent\n",
    "critic = dqnagent.critic\n",
    "\n",
    "ob_dim = critic.ob_dim\n",
    "ac_dim = 6\n",
    "N = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(N, ob_dim))\n",
    "acts = np.random.choice(ac_dim, size=(N,))\n",
    "next_obs = np.random.normal(size=(N, ob_dim))\n",
    "rewards = np.random.normal(size=N)\n",
    "terminals = np.zeros(N)\n",
    "terminals[0] = 1\n",
    "\n",
    "first_weight_before = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n",
    "print(\"Weight before update (first row)\", first_weight_before[0])\n",
    "\n",
    "\n",
    "loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n",
    "expected_loss = 0.9408444\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Initial loss\", loss)\n",
    "print(\"Initial Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "for i in range(4):\n",
    "    loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n",
    "    print(loss)\n",
    "\n",
    "expected_loss = 0.7889254\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "\n",
    "first_weight_after = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n",
    "print(\"Weight after update (first row)\", first_weight_after.shape)\n",
    "# Test DQN gradient\n",
    "print(first_weight_after[0])\n",
    "weight_change_partial = first_weight_after[0] - first_weight_before[0]\n",
    "expected_weight_change = np.array([-0.00491365, -0.00500049, -0.00499149, -0.00491229, -0.00490125,  0.00489534,\n",
    " -0.00282785, -0.00171614,  0.00485604])\n",
    "\n",
    "\n",
    "updated_weight_error = rel_error(weight_change_partial, expected_weight_change)\n",
    "print(\"Weight Update Error\", updated_weight_error, \"should be on the order of 1e-6 or lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the missing components in the get_action method of <code>policies/argmax_policy.py</code> and the step_env method in <code>agents/dqn_agent.py</code> to allow our agent to interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test argmax policy\n",
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = False\n",
    "dqntrainer = DQN_Trainer(dqn_args)\n",
    "dqnagent = dqntrainer.rl_trainer.agent\n",
    "actor = dqnagent.actor\n",
    "\n",
    "ob_dim = critic.ob_dim\n",
    "ac_dim = 6\n",
    "N = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(N, ob_dim))\n",
    "\n",
    "actions = actor.get_action(obs)\n",
    "correct_actions = np.array([1, 0, 1, 0, 1])\n",
    "\n",
    "assert np.all(correct_actions == actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test our DQN implementation on the LunarLander environment. These experiments can take a while to run (over 10 minutes per seed) on CPU, so start early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = False\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/dqn/{}/vanilla_dqn'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running DQN experiment with seed\", seed)\n",
    "    dqn_args['seed'] = seed\n",
    "    dqn_args['logdir'] = 'logs/dqn/{}/vanilla_dqn/seed{}'.format(env_str, seed)\n",
    "    dqntrainer = DQN_Trainer(dqn_args)\n",
    "    dqntrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize vanilla DQN results on Lunar Lander\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/dqn/LunarLander/vanilla_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN\n",
    "One potential issue with learning our Q functions with bootstrapping is _maximization bias_, where the learned Q-values tend to overestimate the actual expected future returns. The main idea is that when there is estimation error in the next state's Q-values, even if the values were correct on average, picking the action with the maximum Q-value would tend to select one where the value is overestimated. This overoptimistic value would then also get propagated via the Bellman backups to other states and actions, and can potentially slow down learning.\n",
    "\n",
    "Double DQN (https://arxiv.org/abs/1509.06461) proposes a simple solution to alleviate this _maximization bias_. Instead of taking the next action that maximizes the target network's Q-value, it selects the action to maximize the _current_ Q function at the next state, and then takes the target network's estimate of that action's value. \n",
    "\n",
    "Implement the double DQN target value in the update method in <code>critics/dqn_critic.py</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test DQN target value with double Q\n",
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = True\n",
    "dqntrainer = DQN_Trainer(dqn_args)\n",
    "dqnagent = dqntrainer.rl_trainer.agent\n",
    "critic = dqnagent.critic\n",
    "\n",
    "ob_dim = critic.ob_dim\n",
    "ac_dim = 6\n",
    "N = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(N, ob_dim))\n",
    "acts = np.random.choice(ac_dim, size=(N,))\n",
    "next_obs = np.random.normal(size=(N, ob_dim))\n",
    "rewards = np.random.normal(size=N)\n",
    "terminals = np.zeros(N)\n",
    "terminals[0] = 1\n",
    "\n",
    "first_weight_before = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n",
    "print(\"Weight before update (first row)\", first_weight_before[0])\n",
    "\n",
    "\n",
    "loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n",
    "expected_loss = 0.93894196\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Initial loss\", loss)\n",
    "print(\"Initial Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "for i in range(4):\n",
    "    loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n",
    "    print(loss)\n",
    "\n",
    "expected_loss = 0.7871182\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "\n",
    "first_weight_after = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n",
    "print(\"Weight after update (first row)\", first_weight_after.shape)\n",
    "# Test DQN gradient\n",
    "print(first_weight_after[0])\n",
    "weight_change_partial = first_weight_after[0] - first_weight_before[0]\n",
    "print(weight_change_partial)\n",
    "expected_weight_change = np.array([-0.0049137, -0.00500057, -0.00499138, -0.00491226, -0.00490116,  0.00489506,\n",
    " -0.00284088, -0.00171939,  0.00485736])\n",
    "\n",
    "\n",
    "updated_weight_error = rel_error(weight_change_partial, expected_weight_change)\n",
    "print(\"Weight Update Error\", updated_weight_error, \"should be on the order of 1e-6 or lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now also run some experiments on LunarLander with Double DQN. You may be able to see that double DQN performs slightly better and more stably, but as there is very high variance, dont' worry if you do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with double DQN\n",
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = True\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/dqn/{}/double_dqn'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running DQN experiment with seed\", seed)\n",
    "    dqn_args['seed'] = seed\n",
    "    dqn_args['logdir'] = 'logs/dqn/{}/double_dqn/seed{}'.format(env_str, seed)\n",
    "    dqntrainer = DQN_Trainer(dqn_args)\n",
    "    dqntrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize all DQN results on Lunar Lander\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/dqn/LunarLander/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
