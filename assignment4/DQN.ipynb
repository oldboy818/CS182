{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks\n",
    "Previously, we trained policies using policy gradient algorithms, which directly estimated the gradient of the returns for the policy and performed stochastic gradient ascent. In this section, we will now implement deep Q-learning, which does not explicitly optimize a policy, but simply infers the policy from a learned Q function that is trained via dynamic programming.\n",
    "\n",
    "We will assume discrete action spaces for this notebook as to enable us to easily select actions that maximize the Q-function at given states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import deeprl.infrastructure.pytorch_util as ptu\n",
    "\n",
    "from deeprl.infrastructure.rl_trainer import RL_Trainer\n",
    "from deeprl.infrastructure.trainers import PG_Trainer\n",
    "from deeprl.infrastructure.trainers import DQN_Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def remove_folder(path):\n",
    "    # check if folder exists\n",
    "    if os.path.exists(path): \n",
    "        print(\"Clearing old results at {}\".format(path))\n",
    "        # remove if exists\n",
    "        shutil.rmtree(path)\n",
    "    else:\n",
    "        print(\"Folder {} does not exist yet. No old results to delete\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_base_args_dict = dict(\n",
    "    env_name = 'LunarLander-v3', #@param \n",
    "    exp_name = 'test_dqn', #@param\n",
    "    save_params = False, #@param {type: \"boolean\"}\n",
    "    \n",
    "    ## PDF will tell you how to set ep_len\n",
    "    ## and discount for each environment\n",
    "    ep_len = 200, #@param {type: \"integer\"}\n",
    "    # discount = 0.95, #@param {type: \"number\"}\n",
    "\n",
    "    # Training\n",
    "    num_agent_train_steps_per_iter = 1, #@param {type: \"integer\"})\n",
    "    num_critic_updates_per_agent_update = 1, #@param {type: \"integer\"}\n",
    "  \n",
    "    #@markdown Q-learning parameters\n",
    "    double_q = False, #@param {type: \"boolean\"}\n",
    "\n",
    "    # batches & buffers\n",
    "    batch_size = 32, #@param {type: \"integer\"})\n",
    "    batch_size_initial=1000,\n",
    "\n",
    "    #@markdown logging\n",
    "    video_log_freq = -1, #@param {type: \"integer\"}\n",
    "    scalar_log_freq = 1000, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown gpu & run-time settings\n",
    "    no_gpu = False, #@param {type: \"boolean\"}\n",
    "    which_gpu = 0, #@param {type: \"integer\"}\n",
    "    seed = 2, #@param {type: \"integer\"}\n",
    "    logdir = 'test',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN updates\n",
    "Recall in Q-learning, we attempt to solve the optimal state-action values (which we refer to as Q-values $Q(s,a)$), by finding solutions to the Bellman equation given by\n",
    "$$Q(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(s'\\vert s,a)}[\\max_{a'}Q(s', a')].$$\n",
    "\n",
    "Regular tabular Q-learning would take sample transitions $(s, a, r, s')$ and perform updates according to\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r(s,a) + \\gamma \\max_{a'} Q(s', a') - Q(s,a)),$$\n",
    "where $\\alpha$ is a stepsize parameter.\n",
    "\n",
    "This can be interpreted as updating $Q(s,a)$ by taking one gradient step on a squared Bellman error objective\n",
    "$$(r(s, a) +\\gamma \\max_{a'} \\tilde Q(s', a') - Q(s,a))^2,$$\n",
    "where $\\tilde Q$ is a copy of $Q$, but is not differentiated when taking the gradient step.\n",
    "\n",
    "Adapting this update to the setting where we use a neural network with parameters $\\theta$ to approximate $Q(s,a)$, we then train $\\theta$ with the loss function \n",
    "$$\\min_{\\theta} \\mathbb{E}_{s, a, s' \\sim D} [L(Q_{\\theta}(s,a), r(s,a) + \\gamma \\max_{a'} Q_{\\tilde \\theta}(s', a'))]$$\n",
    "where $D$ is our replay buffer containing past transitions we've experienced, $L$ is some loss function capturing how far the predicted Q-values are from the target values, and $\\tilde \\theta$ are the target Q function parameters, which are usually a delayed copy of $\\theta$ for stability reasons.\n",
    "\n",
    "We note our previous policy gradient algorithms were _on-policy_ algoritms, which meant they updated the policy using only the data collected from the most recent policy, and discard all the data after using it just once. In contrast, DQN uses _off-policy_ updates by sampling data from all past interactions, allowing for data reuse over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the missing components for the basic Q-learning update in <code>critics/dqn_critic.py</code> (not including the double_q section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  test\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n",
      "Weight before update (first row) [ 0.07646337 -0.07932475  0.09140956 -0.01702595  0.14239588  0.07935759\n",
      " -0.03831157 -0.2694876   0.07610479]\n",
      "Initial loss 0.9408444\n",
      "Initial Loss Error 8.831612855468697e-09 should be on the order of 1e-6 or lower\n",
      "0.9007309\n",
      "0.8621321\n",
      "0.82467556\n",
      "0.7889254\n",
      "Loss Error 5.904878045285727e-09 should be on the order of 1e-6 or lower\n",
      "Weight after update (first row) (64, 9)\n",
      "[ 0.07154972 -0.08432525  0.08641808 -0.02193824  0.13749464  0.08425293\n",
      " -0.04113942 -0.27120373  0.08096083]\n",
      "Weight Update Error 8.937585787696078e-07 should be on the order of 1e-6 or lower\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "#### Test DQN updates\n",
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = False\n",
    "dqntrainer = DQN_Trainer(dqn_args)\n",
    "dqnagent = dqntrainer.rl_trainer.agent\n",
    "critic = dqnagent.critic\n",
    "\n",
    "ob_dim = critic.ob_dim\n",
    "ac_dim = 6\n",
    "N = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(N, ob_dim))\n",
    "acts = np.random.choice(ac_dim, size=(N,))\n",
    "next_obs = np.random.normal(size=(N, ob_dim))\n",
    "rewards = np.random.normal(size=N)\n",
    "terminals = np.zeros(N)\n",
    "terminals[0] = 1\n",
    "\n",
    "first_weight_before = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n",
    "print(\"Weight before update (first row)\", first_weight_before[0])\n",
    "\n",
    "\n",
    "loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n",
    "expected_loss = 0.9408444\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Initial loss\", loss)\n",
    "print(\"Initial Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "for i in range(4):\n",
    "    loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n",
    "    print(loss)\n",
    "\n",
    "expected_loss = 0.7889254\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "\n",
    "first_weight_after = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n",
    "print(\"Weight after update (first row)\", first_weight_after.shape)\n",
    "# Test DQN gradient\n",
    "print(first_weight_after[0])\n",
    "weight_change_partial = first_weight_after[0] - first_weight_before[0]\n",
    "expected_weight_change = np.array([-0.00491365, -0.00500049, -0.00499149, -0.00491229, -0.00490125,  0.00489534,\n",
    " -0.00282785, -0.00171614,  0.00485604])\n",
    "\n",
    "\n",
    "updated_weight_error = rel_error(weight_change_partial, expected_weight_change)\n",
    "print(\"Weight Update Error\", updated_weight_error, \"should be on the order of 1e-6 or lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the missing components in the get_action method of <code>policies/argmax_policy.py</code> and the step_env method in <code>agents/dqn_agent.py</code> to allow our agent to interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  test\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n"
     ]
    }
   ],
   "source": [
    "### Test argmax policy\n",
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = False\n",
    "dqntrainer = DQN_Trainer(dqn_args)\n",
    "dqnagent = dqntrainer.rl_trainer.agent\n",
    "actor = dqnagent.actor\n",
    "\n",
    "ob_dim = critic.ob_dim\n",
    "ac_dim = 6\n",
    "N = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(N, ob_dim))\n",
    "\n",
    "actions = actor.get_action(obs)\n",
    "correct_actions = np.array([1, 0, 1, 0, 1])\n",
    "\n",
    "assert np.all(correct_actions == actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test our DQN implementation on the LunarLander environment. These experiments can take a while to run (over 10 minutes per seed) on CPU, so start early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder logs/dqn/LunarLander/vanilla_dqn does not exist yet. No old results to delete\n",
      "Running DQN experiment with seed 0\n",
      "########################\n",
      "logging outputs to  logs/dqn/LunarLander/vanilla_dqn/seed0\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.001021\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.0010209083557128906\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1001\n",
      "mean reward (100 episodes) -361.459899\n",
      "best mean reward -inf\n",
      "running time 0.256882\n",
      "Train_EnvstepsSoFar : 1001\n",
      "Train_AverageReturn : -361.45989908312015\n",
      "TimeSinceStart : 0.2568819522857666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2001\n",
      "mean reward (100 episodes) -299.838972\n",
      "best mean reward -inf\n",
      "running time 1.272609\n",
      "Train_EnvstepsSoFar : 2001\n",
      "Train_AverageReturn : -299.83897241677244\n",
      "TimeSinceStart : 1.272608995437622\n",
      "Training Loss : 0.4039880633354187\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3001\n",
      "mean reward (100 episodes) -281.584265\n",
      "best mean reward -inf\n",
      "running time 1.959219\n",
      "Train_EnvstepsSoFar : 3001\n",
      "Train_AverageReturn : -281.58426452192367\n",
      "TimeSinceStart : 1.959218978881836\n",
      "Training Loss : 0.8244561553001404\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4001\n",
      "mean reward (100 episodes) -268.720634\n",
      "best mean reward -inf\n",
      "running time 2.650842\n",
      "Train_EnvstepsSoFar : 4001\n",
      "Train_AverageReturn : -268.7206338275019\n",
      "TimeSinceStart : 2.6508419513702393\n",
      "Training Loss : 3.5311620235443115\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5001\n",
      "mean reward (100 episodes) -264.926471\n",
      "best mean reward -inf\n",
      "running time 3.334923\n",
      "Train_EnvstepsSoFar : 5001\n",
      "Train_AverageReturn : -264.92647142992723\n",
      "TimeSinceStart : 3.334923028945923\n",
      "Training Loss : 3.6905319690704346\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6001\n",
      "mean reward (100 episodes) -277.268322\n",
      "best mean reward -inf\n",
      "running time 4.039511\n",
      "Train_EnvstepsSoFar : 6001\n",
      "Train_AverageReturn : -277.2683222311815\n",
      "TimeSinceStart : 4.03951096534729\n",
      "Training Loss : 0.3707176446914673\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7001\n",
      "mean reward (100 episodes) -243.030424\n",
      "best mean reward -inf\n",
      "running time 4.756069\n",
      "Train_EnvstepsSoFar : 7001\n",
      "Train_AverageReturn : -243.03042357240508\n",
      "TimeSinceStart : 4.75606894493103\n",
      "Training Loss : 0.3751882314682007\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8001\n",
      "mean reward (100 episodes) -243.178775\n",
      "best mean reward -inf\n",
      "running time 5.467605\n",
      "Train_EnvstepsSoFar : 8001\n",
      "Train_AverageReturn : -243.17877511577998\n",
      "TimeSinceStart : 5.467604875564575\n",
      "Training Loss : 0.34047532081604004\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9001\n",
      "mean reward (100 episodes) -240.722607\n",
      "best mean reward -inf\n",
      "running time 6.196865\n",
      "Train_EnvstepsSoFar : 9001\n",
      "Train_AverageReturn : -240.72260681354973\n",
      "TimeSinceStart : 6.19686484336853\n",
      "Training Loss : 1.9949017763137817\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -238.714323\n",
      "best mean reward -inf\n",
      "running time 6.899878\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -238.71432277039762\n",
      "TimeSinceStart : 6.899878025054932\n",
      "Training Loss : 0.4450238347053528\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 11001\n",
      "mean reward (100 episodes) -240.490410\n",
      "best mean reward -inf\n",
      "running time 7.696762\n",
      "Train_EnvstepsSoFar : 11001\n",
      "Train_AverageReturn : -240.49040957238913\n",
      "TimeSinceStart : 7.6967620849609375\n",
      "Training Loss : 0.7025690674781799\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 12001\n",
      "mean reward (100 episodes) -235.093174\n",
      "best mean reward -inf\n",
      "running time 8.534847\n",
      "Train_EnvstepsSoFar : 12001\n",
      "Train_AverageReturn : -235.093173854223\n",
      "TimeSinceStart : 8.534846782684326\n",
      "Training Loss : 0.3380526006221771\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 13001\n",
      "mean reward (100 episodes) -236.039901\n",
      "best mean reward -inf\n",
      "running time 9.407995\n",
      "Train_EnvstepsSoFar : 13001\n",
      "Train_AverageReturn : -236.03990061359875\n",
      "TimeSinceStart : 9.407994985580444\n",
      "Training Loss : 0.45106667280197144\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 14001\n",
      "mean reward (100 episodes) -232.218039\n",
      "best mean reward -inf\n",
      "running time 10.219111\n",
      "Train_EnvstepsSoFar : 14001\n",
      "Train_AverageReturn : -232.21803868358745\n",
      "TimeSinceStart : 10.21911072731018\n",
      "Training Loss : 0.3792095482349396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 15001\n",
      "mean reward (100 episodes) -220.757937\n",
      "best mean reward -inf\n",
      "running time 11.063303\n",
      "Train_EnvstepsSoFar : 15001\n",
      "Train_AverageReturn : -220.75793685907732\n",
      "TimeSinceStart : 11.063302755355835\n",
      "Training Loss : 0.6840996742248535\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 16001\n",
      "mean reward (100 episodes) -220.302871\n",
      "best mean reward -inf\n",
      "running time 13.579019\n",
      "Train_EnvstepsSoFar : 16001\n",
      "Train_AverageReturn : -220.30287128317113\n",
      "TimeSinceStart : 13.579018831253052\n",
      "Training Loss : 1.3406248092651367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 17001\n",
      "mean reward (100 episodes) -220.888209\n",
      "best mean reward -inf\n",
      "running time 15.480574\n",
      "Train_EnvstepsSoFar : 17001\n",
      "Train_AverageReturn : -220.88820853136005\n",
      "TimeSinceStart : 15.480573892593384\n",
      "Training Loss : 0.7020846605300903\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 18001\n",
      "mean reward (100 episodes) -218.886063\n",
      "best mean reward -inf\n",
      "running time 16.836374\n",
      "Train_EnvstepsSoFar : 18001\n",
      "Train_AverageReturn : -218.8860629216866\n",
      "TimeSinceStart : 16.836374044418335\n",
      "Training Loss : 1.123772382736206\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 19001\n",
      "mean reward (100 episodes) -213.311213\n",
      "best mean reward -213.311213\n",
      "running time 18.848971\n",
      "Train_EnvstepsSoFar : 19001\n",
      "Train_AverageReturn : -213.3112134083119\n",
      "Train_BestReturn : -213.3112134083119\n",
      "TimeSinceStart : 18.848970890045166\n",
      "Training Loss : 1.2085462808609009\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -208.637912\n",
      "best mean reward -208.637912\n",
      "running time 20.971569\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -208.63791248955036\n",
      "Train_BestReturn : -208.63791248955036\n",
      "TimeSinceStart : 20.971569061279297\n",
      "Training Loss : 4.441314220428467\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 21001\n",
      "mean reward (100 episodes) -208.276876\n",
      "best mean reward -208.276876\n",
      "running time 22.153046\n",
      "Train_EnvstepsSoFar : 21001\n",
      "Train_AverageReturn : -208.27687593870715\n",
      "Train_BestReturn : -208.27687593870715\n",
      "TimeSinceStart : 22.153045892715454\n",
      "Training Loss : 2.692592144012451\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 22001\n",
      "mean reward (100 episodes) -204.878264\n",
      "best mean reward -204.878264\n",
      "running time 24.639532\n",
      "Train_EnvstepsSoFar : 22001\n",
      "Train_AverageReturn : -204.87826388290526\n",
      "Train_BestReturn : -204.87826388290526\n",
      "TimeSinceStart : 24.63953185081482\n",
      "Training Loss : 0.4774516224861145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 23001\n",
      "mean reward (100 episodes) -199.255038\n",
      "best mean reward -199.255038\n",
      "running time 26.363241\n",
      "Train_EnvstepsSoFar : 23001\n",
      "Train_AverageReturn : -199.25503780564196\n",
      "Train_BestReturn : -199.25503780564196\n",
      "TimeSinceStart : 26.363240957260132\n",
      "Training Loss : 0.43600955605506897\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 24001\n",
      "mean reward (100 episodes) -190.344530\n",
      "best mean reward -190.344530\n",
      "running time 28.174971\n",
      "Train_EnvstepsSoFar : 24001\n",
      "Train_AverageReturn : -190.34453025875507\n",
      "Train_BestReturn : -190.34453025875507\n",
      "TimeSinceStart : 28.174971103668213\n",
      "Training Loss : 0.39372822642326355\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 25001\n",
      "mean reward (100 episodes) -192.370991\n",
      "best mean reward -190.344530\n",
      "running time 30.898000\n",
      "Train_EnvstepsSoFar : 25001\n",
      "Train_AverageReturn : -192.37099149792732\n",
      "Train_BestReturn : -190.34453025875507\n",
      "TimeSinceStart : 30.89800000190735\n",
      "Training Loss : 0.34288641810417175\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 26001\n",
      "mean reward (100 episodes) -189.573348\n",
      "best mean reward -189.573348\n",
      "running time 32.759380\n",
      "Train_EnvstepsSoFar : 26001\n",
      "Train_AverageReturn : -189.57334803677762\n",
      "Train_BestReturn : -189.57334803677762\n",
      "TimeSinceStart : 32.759379863739014\n",
      "Training Loss : 0.23321112990379333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 27001\n",
      "mean reward (100 episodes) -188.286377\n",
      "best mean reward -188.286377\n",
      "running time 35.002840\n",
      "Train_EnvstepsSoFar : 27001\n",
      "Train_AverageReturn : -188.28637721095595\n",
      "Train_BestReturn : -188.28637721095595\n",
      "TimeSinceStart : 35.00284004211426\n",
      "Training Loss : 1.3750578165054321\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 28001\n",
      "mean reward (100 episodes) -185.617999\n",
      "best mean reward -185.617999\n",
      "running time 36.899952\n",
      "Train_EnvstepsSoFar : 28001\n",
      "Train_AverageReturn : -185.61799872988078\n",
      "Train_BestReturn : -185.61799872988078\n",
      "TimeSinceStart : 36.89995193481445\n",
      "Training Loss : 1.245599389076233\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 29001\n",
      "mean reward (100 episodes) -184.704460\n",
      "best mean reward -184.704460\n",
      "running time 39.175911\n",
      "Train_EnvstepsSoFar : 29001\n",
      "Train_AverageReturn : -184.70445965583167\n",
      "Train_BestReturn : -184.70445965583167\n",
      "TimeSinceStart : 39.17591094970703\n",
      "Training Loss : 0.3499055504798889\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -185.126077\n",
      "best mean reward -184.704460\n",
      "running time 40.877926\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -185.1260768810663\n",
      "Train_BestReturn : -184.70445965583167\n",
      "TimeSinceStart : 40.87792611122131\n",
      "Training Loss : 1.255910873413086\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 31001\n",
      "mean reward (100 episodes) -183.648852\n",
      "best mean reward -183.648852\n",
      "running time 42.388566\n",
      "Train_EnvstepsSoFar : 31001\n",
      "Train_AverageReturn : -183.64885182039012\n",
      "Train_BestReturn : -183.64885182039012\n",
      "TimeSinceStart : 42.38856601715088\n",
      "Training Loss : 0.6066957116127014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 32001\n",
      "mean reward (100 episodes) -183.000533\n",
      "best mean reward -183.000533\n",
      "running time 43.985894\n",
      "Train_EnvstepsSoFar : 32001\n",
      "Train_AverageReturn : -183.00053285795812\n",
      "Train_BestReturn : -183.00053285795812\n",
      "TimeSinceStart : 43.985893964767456\n",
      "Training Loss : 0.8000628352165222\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 33001\n",
      "mean reward (100 episodes) -180.884247\n",
      "best mean reward -180.884247\n",
      "running time 45.557909\n",
      "Train_EnvstepsSoFar : 33001\n",
      "Train_AverageReturn : -180.88424686505823\n",
      "Train_BestReturn : -180.88424686505823\n",
      "TimeSinceStart : 45.55790901184082\n",
      "Training Loss : 3.881324052810669\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 34001\n",
      "mean reward (100 episodes) -180.959606\n",
      "best mean reward -180.884247\n",
      "running time 47.107255\n",
      "Train_EnvstepsSoFar : 34001\n",
      "Train_AverageReturn : -180.9596057034486\n",
      "Train_BestReturn : -180.88424686505823\n",
      "TimeSinceStart : 47.10725498199463\n",
      "Training Loss : 0.714976966381073\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 35001\n",
      "mean reward (100 episodes) -178.384680\n",
      "best mean reward -178.384680\n",
      "running time 48.792381\n",
      "Train_EnvstepsSoFar : 35001\n",
      "Train_AverageReturn : -178.38467972107614\n",
      "Train_BestReturn : -178.38467972107614\n",
      "TimeSinceStart : 48.792380809783936\n",
      "Training Loss : 0.4492531716823578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 36001\n",
      "mean reward (100 episodes) -179.224718\n",
      "best mean reward -178.384680\n",
      "running time 50.761873\n",
      "Train_EnvstepsSoFar : 36001\n",
      "Train_AverageReturn : -179.22471828174363\n",
      "Train_BestReturn : -178.38467972107614\n",
      "TimeSinceStart : 50.76187300682068\n",
      "Training Loss : 4.927097797393799\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 37001\n",
      "mean reward (100 episodes) -178.776426\n",
      "best mean reward -178.384680\n",
      "running time 52.362747\n",
      "Train_EnvstepsSoFar : 37001\n",
      "Train_AverageReturn : -178.776426071446\n",
      "Train_BestReturn : -178.38467972107614\n",
      "TimeSinceStart : 52.36274695396423\n",
      "Training Loss : 0.5075268745422363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 38001\n",
      "mean reward (100 episodes) -177.844665\n",
      "best mean reward -177.844665\n",
      "running time 54.276746\n",
      "Train_EnvstepsSoFar : 38001\n",
      "Train_AverageReturn : -177.84466484794675\n",
      "Train_BestReturn : -177.84466484794675\n",
      "TimeSinceStart : 54.27674579620361\n",
      "Training Loss : 0.25936996936798096\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 39001\n",
      "mean reward (100 episodes) -177.302071\n",
      "best mean reward -177.302071\n",
      "running time 56.477539\n",
      "Train_EnvstepsSoFar : 39001\n",
      "Train_AverageReturn : -177.30207123937203\n",
      "Train_BestReturn : -177.30207123937203\n",
      "TimeSinceStart : 56.4775390625\n",
      "Training Loss : 0.3108523488044739\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -175.328894\n",
      "best mean reward -175.328894\n",
      "running time 58.356809\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -175.32889373583868\n",
      "Train_BestReturn : -175.32889373583868\n",
      "TimeSinceStart : 58.35680890083313\n",
      "Training Loss : 0.7998031973838806\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 41001\n",
      "mean reward (100 episodes) -176.088897\n",
      "best mean reward -175.328894\n",
      "running time 60.412149\n",
      "Train_EnvstepsSoFar : 41001\n",
      "Train_AverageReturn : -176.088896650241\n",
      "Train_BestReturn : -175.32889373583868\n",
      "TimeSinceStart : 60.41214895248413\n",
      "Training Loss : 0.3474563658237457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 42001\n",
      "mean reward (100 episodes) -173.648085\n",
      "best mean reward -173.648085\n",
      "running time 62.325989\n",
      "Train_EnvstepsSoFar : 42001\n",
      "Train_AverageReturn : -173.64808478492878\n",
      "Train_BestReturn : -173.64808478492878\n",
      "TimeSinceStart : 62.32598900794983\n",
      "Training Loss : 0.963370680809021\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 43001\n",
      "mean reward (100 episodes) -170.600563\n",
      "best mean reward -170.600563\n",
      "running time 64.140791\n",
      "Train_EnvstepsSoFar : 43001\n",
      "Train_AverageReturn : -170.6005633719898\n",
      "Train_BestReturn : -170.6005633719898\n",
      "TimeSinceStart : 64.14079093933105\n",
      "Training Loss : 1.471798300743103\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 44001\n",
      "mean reward (100 episodes) -168.774020\n",
      "best mean reward -168.774020\n",
      "running time 65.725338\n",
      "Train_EnvstepsSoFar : 44001\n",
      "Train_AverageReturn : -168.7740195398935\n",
      "Train_BestReturn : -168.7740195398935\n",
      "TimeSinceStart : 65.72533798217773\n",
      "Training Loss : 0.4704245328903198\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 45001\n",
      "mean reward (100 episodes) -167.603886\n",
      "best mean reward -167.603886\n",
      "running time 67.008371\n",
      "Train_EnvstepsSoFar : 45001\n",
      "Train_AverageReturn : -167.60388617871646\n",
      "Train_BestReturn : -167.60388617871646\n",
      "TimeSinceStart : 67.00837087631226\n",
      "Training Loss : 0.4293050467967987\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 46001\n",
      "mean reward (100 episodes) -166.618337\n",
      "best mean reward -166.618337\n",
      "running time 68.475457\n",
      "Train_EnvstepsSoFar : 46001\n",
      "Train_AverageReturn : -166.61833735129133\n",
      "Train_BestReturn : -166.61833735129133\n",
      "TimeSinceStart : 68.4754569530487\n",
      "Training Loss : 0.3754621744155884\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 47001\n",
      "mean reward (100 episodes) -165.066540\n",
      "best mean reward -165.066540\n",
      "running time 70.010059\n",
      "Train_EnvstepsSoFar : 47001\n",
      "Train_AverageReturn : -165.06654040520468\n",
      "Train_BestReturn : -165.06654040520468\n",
      "TimeSinceStart : 70.0100588798523\n",
      "Training Loss : 0.3319873511791229\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 48001\n",
      "mean reward (100 episodes) -162.596098\n",
      "best mean reward -162.596098\n",
      "running time 71.461854\n",
      "Train_EnvstepsSoFar : 48001\n",
      "Train_AverageReturn : -162.59609779036603\n",
      "Train_BestReturn : -162.59609779036603\n",
      "TimeSinceStart : 71.46185398101807\n",
      "Training Loss : 0.6658356189727783\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 49001\n",
      "mean reward (100 episodes) -160.559593\n",
      "best mean reward -160.559593\n",
      "running time 72.896306\n",
      "Train_EnvstepsSoFar : 49001\n",
      "Train_AverageReturn : -160.5595934865977\n",
      "Train_BestReturn : -160.5595934865977\n",
      "TimeSinceStart : 72.89630579948425\n",
      "Training Loss : 0.4268280863761902\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -158.165901\n",
      "best mean reward -158.165901\n",
      "running time 74.591811\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -158.16590057181853\n",
      "Train_BestReturn : -158.16590057181853\n",
      "TimeSinceStart : 74.59181094169617\n",
      "Training Loss : 5.080491542816162\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 51001\n",
      "mean reward (100 episodes) -156.141507\n",
      "best mean reward -156.141507\n",
      "running time 75.845060\n",
      "Train_EnvstepsSoFar : 51001\n",
      "Train_AverageReturn : -156.14150669608952\n",
      "Train_BestReturn : -156.14150669608952\n",
      "TimeSinceStart : 75.84506011009216\n",
      "Training Loss : 0.4369984269142151\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 52001\n",
      "mean reward (100 episodes) -155.628947\n",
      "best mean reward -155.628947\n",
      "running time 77.678107\n",
      "Train_EnvstepsSoFar : 52001\n",
      "Train_AverageReturn : -155.62894662563684\n",
      "Train_BestReturn : -155.62894662563684\n",
      "TimeSinceStart : 77.67810702323914\n",
      "Training Loss : 0.3889290988445282\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 53001\n",
      "mean reward (100 episodes) -144.607965\n",
      "best mean reward -144.607965\n",
      "running time 79.024057\n",
      "Train_EnvstepsSoFar : 53001\n",
      "Train_AverageReturn : -144.60796457257254\n",
      "Train_BestReturn : -144.60796457257254\n",
      "TimeSinceStart : 79.0240569114685\n",
      "Training Loss : 0.30107003450393677\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 54001\n",
      "mean reward (100 episodes) -140.091566\n",
      "best mean reward -140.091566\n",
      "running time 80.569078\n",
      "Train_EnvstepsSoFar : 54001\n",
      "Train_AverageReturn : -140.09156575089955\n",
      "Train_BestReturn : -140.09156575089955\n",
      "TimeSinceStart : 80.56907796859741\n",
      "Training Loss : 0.8217000365257263\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 55001\n",
      "mean reward (100 episodes) -140.880124\n",
      "best mean reward -140.091566\n",
      "running time 82.224737\n",
      "Train_EnvstepsSoFar : 55001\n",
      "Train_AverageReturn : -140.88012395672584\n",
      "Train_BestReturn : -140.09156575089955\n",
      "TimeSinceStart : 82.22473692893982\n",
      "Training Loss : 0.3644773066043854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 56001\n",
      "mean reward (100 episodes) -139.681848\n",
      "best mean reward -139.681848\n",
      "running time 83.599113\n",
      "Train_EnvstepsSoFar : 56001\n",
      "Train_AverageReturn : -139.6818480852127\n",
      "Train_BestReturn : -139.6818480852127\n",
      "TimeSinceStart : 83.59911298751831\n",
      "Training Loss : 0.22105002403259277\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 57001\n",
      "mean reward (100 episodes) -142.539894\n",
      "best mean reward -139.681848\n",
      "running time 85.483242\n",
      "Train_EnvstepsSoFar : 57001\n",
      "Train_AverageReturn : -142.53989371175712\n",
      "Train_BestReturn : -139.6818480852127\n",
      "TimeSinceStart : 85.48324179649353\n",
      "Training Loss : 0.30068886280059814\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 58001\n",
      "mean reward (100 episodes) -141.667425\n",
      "best mean reward -139.681848\n",
      "running time 87.593515\n",
      "Train_EnvstepsSoFar : 58001\n",
      "Train_AverageReturn : -141.66742467772494\n",
      "Train_BestReturn : -139.6818480852127\n",
      "TimeSinceStart : 87.593514919281\n",
      "Training Loss : 0.24572832882404327\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 59001\n",
      "mean reward (100 episodes) -143.856717\n",
      "best mean reward -139.681848\n",
      "running time 89.689264\n",
      "Train_EnvstepsSoFar : 59001\n",
      "Train_AverageReturn : -143.8567168101821\n",
      "Train_BestReturn : -139.6818480852127\n",
      "TimeSinceStart : 89.68926405906677\n",
      "Training Loss : 0.7891553044319153\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -142.698406\n",
      "best mean reward -139.681848\n",
      "running time 91.429031\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -142.69840578953986\n",
      "Train_BestReturn : -139.6818480852127\n",
      "TimeSinceStart : 91.42903089523315\n",
      "Training Loss : 1.7146309614181519\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 61001\n",
      "mean reward (100 episodes) -141.645751\n",
      "best mean reward -139.681848\n",
      "running time 92.846071\n",
      "Train_EnvstepsSoFar : 61001\n",
      "Train_AverageReturn : -141.64575062047945\n",
      "Train_BestReturn : -139.6818480852127\n",
      "TimeSinceStart : 92.84607100486755\n",
      "Training Loss : 0.4534631073474884\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 62001\n",
      "mean reward (100 episodes) -139.607817\n",
      "best mean reward -139.607817\n",
      "running time 94.787308\n",
      "Train_EnvstepsSoFar : 62001\n",
      "Train_AverageReturn : -139.60781675075742\n",
      "Train_BestReturn : -139.60781675075742\n",
      "TimeSinceStart : 94.78730797767639\n",
      "Training Loss : 0.2938005030155182\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 63001\n",
      "mean reward (100 episodes) -132.312208\n",
      "best mean reward -132.312208\n",
      "running time 96.311655\n",
      "Train_EnvstepsSoFar : 63001\n",
      "Train_AverageReturn : -132.31220818550565\n",
      "Train_BestReturn : -132.31220818550565\n",
      "TimeSinceStart : 96.31165480613708\n",
      "Training Loss : 1.3026974201202393\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 64001\n",
      "mean reward (100 episodes) -132.709416\n",
      "best mean reward -132.312208\n",
      "running time 97.896759\n",
      "Train_EnvstepsSoFar : 64001\n",
      "Train_AverageReturn : -132.7094155859456\n",
      "Train_BestReturn : -132.31220818550565\n",
      "TimeSinceStart : 97.89675903320312\n",
      "Training Loss : 0.3514064848423004\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 65001\n",
      "mean reward (100 episodes) -126.805725\n",
      "best mean reward -126.805725\n",
      "running time 99.140382\n",
      "Train_EnvstepsSoFar : 65001\n",
      "Train_AverageReturn : -126.80572467015665\n",
      "Train_BestReturn : -126.80572467015665\n",
      "TimeSinceStart : 99.14038181304932\n",
      "Training Loss : 0.4142954647541046\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 66001\n",
      "mean reward (100 episodes) -126.963505\n",
      "best mean reward -126.805725\n",
      "running time 100.977143\n",
      "Train_EnvstepsSoFar : 66001\n",
      "Train_AverageReturn : -126.96350479818106\n",
      "Train_BestReturn : -126.80572467015665\n",
      "TimeSinceStart : 100.97714304924011\n",
      "Training Loss : 0.6695938110351562\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 67001\n",
      "mean reward (100 episodes) -122.364600\n",
      "best mean reward -122.364600\n",
      "running time 102.506746\n",
      "Train_EnvstepsSoFar : 67001\n",
      "Train_AverageReturn : -122.36459985017822\n",
      "Train_BestReturn : -122.36459985017822\n",
      "TimeSinceStart : 102.50674605369568\n",
      "Training Loss : 0.27300751209259033\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 68001\n",
      "mean reward (100 episodes) -118.972870\n",
      "best mean reward -118.972870\n",
      "running time 103.607501\n",
      "Train_EnvstepsSoFar : 68001\n",
      "Train_AverageReturn : -118.97287016291362\n",
      "Train_BestReturn : -118.97287016291362\n",
      "TimeSinceStart : 103.60750102996826\n",
      "Training Loss : 0.28255221247673035\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 69001\n",
      "mean reward (100 episodes) -117.294595\n",
      "best mean reward -117.294595\n",
      "running time 105.664241\n",
      "Train_EnvstepsSoFar : 69001\n",
      "Train_AverageReturn : -117.29459506318439\n",
      "Train_BestReturn : -117.29459506318439\n",
      "TimeSinceStart : 105.66424107551575\n",
      "Training Loss : 0.38637906312942505\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -117.569481\n",
      "best mean reward -117.294595\n",
      "running time 107.192443\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -117.56948109330371\n",
      "Train_BestReturn : -117.29459506318439\n",
      "TimeSinceStart : 107.19244313240051\n",
      "Training Loss : 0.27232784032821655\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 71001\n",
      "mean reward (100 episodes) -119.782793\n",
      "best mean reward -117.294595\n",
      "running time 108.755302\n",
      "Train_EnvstepsSoFar : 71001\n",
      "Train_AverageReturn : -119.78279264910958\n",
      "Train_BestReturn : -117.29459506318439\n",
      "TimeSinceStart : 108.75530195236206\n",
      "Training Loss : 0.6042792797088623\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 72001\n",
      "mean reward (100 episodes) -118.630163\n",
      "best mean reward -117.294595\n",
      "running time 110.086026\n",
      "Train_EnvstepsSoFar : 72001\n",
      "Train_AverageReturn : -118.6301625162827\n",
      "Train_BestReturn : -117.29459506318439\n",
      "TimeSinceStart : 110.08602595329285\n",
      "Training Loss : 0.1345151960849762\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 73001\n",
      "mean reward (100 episodes) -111.914669\n",
      "best mean reward -111.914669\n",
      "running time 111.561783\n",
      "Train_EnvstepsSoFar : 73001\n",
      "Train_AverageReturn : -111.91466878221138\n",
      "Train_BestReturn : -111.91466878221138\n",
      "TimeSinceStart : 111.56178307533264\n",
      "Training Loss : 0.46526533365249634\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 74001\n",
      "mean reward (100 episodes) -108.392911\n",
      "best mean reward -108.392911\n",
      "running time 113.185115\n",
      "Train_EnvstepsSoFar : 74001\n",
      "Train_AverageReturn : -108.39291066481556\n",
      "Train_BestReturn : -108.39291066481556\n",
      "TimeSinceStart : 113.18511509895325\n",
      "Training Loss : 0.22847123444080353\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 75001\n",
      "mean reward (100 episodes) -104.433864\n",
      "best mean reward -104.433864\n",
      "running time 114.325833\n",
      "Train_EnvstepsSoFar : 75001\n",
      "Train_AverageReturn : -104.43386363460628\n",
      "Train_BestReturn : -104.43386363460628\n",
      "TimeSinceStart : 114.32583284378052\n",
      "Training Loss : 0.29959577322006226\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 76001\n",
      "mean reward (100 episodes) -102.972177\n",
      "best mean reward -102.972177\n",
      "running time 116.340312\n",
      "Train_EnvstepsSoFar : 76001\n",
      "Train_AverageReturn : -102.97217718726573\n",
      "Train_BestReturn : -102.97217718726573\n",
      "TimeSinceStart : 116.34031200408936\n",
      "Training Loss : 0.20329295098781586\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 77001\n",
      "mean reward (100 episodes) -100.221116\n",
      "best mean reward -100.221116\n",
      "running time 117.877651\n",
      "Train_EnvstepsSoFar : 77001\n",
      "Train_AverageReturn : -100.22111556413115\n",
      "Train_BestReturn : -100.22111556413115\n",
      "TimeSinceStart : 117.87765097618103\n",
      "Training Loss : 0.17697590589523315\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 78001\n",
      "mean reward (100 episodes) -97.206224\n",
      "best mean reward -97.206224\n",
      "running time 119.644608\n",
      "Train_EnvstepsSoFar : 78001\n",
      "Train_AverageReturn : -97.20622448848859\n",
      "Train_BestReturn : -97.20622448848859\n",
      "TimeSinceStart : 119.64460802078247\n",
      "Training Loss : 0.10688579827547073\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 79001\n",
      "mean reward (100 episodes) -92.582846\n",
      "best mean reward -92.582846\n",
      "running time 120.597433\n",
      "Train_EnvstepsSoFar : 79001\n",
      "Train_AverageReturn : -92.58284553346422\n",
      "Train_BestReturn : -92.58284553346422\n",
      "TimeSinceStart : 120.59743309020996\n",
      "Training Loss : 0.19256342947483063\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -91.147681\n",
      "best mean reward -91.147681\n",
      "running time 121.838347\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -91.14768137382984\n",
      "Train_BestReturn : -91.14768137382984\n",
      "TimeSinceStart : 121.8383469581604\n",
      "Training Loss : 0.17944227159023285\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 81001\n",
      "mean reward (100 episodes) -90.400526\n",
      "best mean reward -90.400526\n",
      "running time 123.694111\n",
      "Train_EnvstepsSoFar : 81001\n",
      "Train_AverageReturn : -90.40052608145328\n",
      "Train_BestReturn : -90.40052608145328\n",
      "TimeSinceStart : 123.69411087036133\n",
      "Training Loss : 0.1819428652524948\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 82001\n",
      "mean reward (100 episodes) -86.073778\n",
      "best mean reward -86.073778\n",
      "running time 124.984669\n",
      "Train_EnvstepsSoFar : 82001\n",
      "Train_AverageReturn : -86.07377837360109\n",
      "Train_BestReturn : -86.07377837360109\n",
      "TimeSinceStart : 124.98466873168945\n",
      "Training Loss : 0.33119961619377136\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 83001\n",
      "mean reward (100 episodes) -85.997266\n",
      "best mean reward -85.997266\n",
      "running time 126.180851\n",
      "Train_EnvstepsSoFar : 83001\n",
      "Train_AverageReturn : -85.9972664029487\n",
      "Train_BestReturn : -85.9972664029487\n",
      "TimeSinceStart : 126.18085098266602\n",
      "Training Loss : 0.17698794603347778\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 84001\n",
      "mean reward (100 episodes) -77.622279\n",
      "best mean reward -77.622279\n",
      "running time 127.392211\n",
      "Train_EnvstepsSoFar : 84001\n",
      "Train_AverageReturn : -77.62227920192632\n",
      "Train_BestReturn : -77.62227920192632\n",
      "TimeSinceStart : 127.39221096038818\n",
      "Training Loss : 0.16262026131153107\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 85001\n",
      "mean reward (100 episodes) -77.183483\n",
      "best mean reward -77.183483\n",
      "running time 128.953094\n",
      "Train_EnvstepsSoFar : 85001\n",
      "Train_AverageReturn : -77.18348342450201\n",
      "Train_BestReturn : -77.18348342450201\n",
      "TimeSinceStart : 128.95309400558472\n",
      "Training Loss : 0.27352669835090637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 86001\n",
      "mean reward (100 episodes) -76.851294\n",
      "best mean reward -76.851294\n",
      "running time 130.348430\n",
      "Train_EnvstepsSoFar : 86001\n",
      "Train_AverageReturn : -76.85129356000726\n",
      "Train_BestReturn : -76.85129356000726\n",
      "TimeSinceStart : 130.34842991828918\n",
      "Training Loss : 0.14434263110160828\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 87001\n",
      "mean reward (100 episodes) -74.264123\n",
      "best mean reward -74.264123\n",
      "running time 131.464454\n",
      "Train_EnvstepsSoFar : 87001\n",
      "Train_AverageReturn : -74.26412278504291\n",
      "Train_BestReturn : -74.26412278504291\n",
      "TimeSinceStart : 131.46445417404175\n",
      "Training Loss : 0.10530879348516464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 88001\n",
      "mean reward (100 episodes) -73.925796\n",
      "best mean reward -73.925796\n",
      "running time 132.893315\n",
      "Train_EnvstepsSoFar : 88001\n",
      "Train_AverageReturn : -73.92579620585356\n",
      "Train_BestReturn : -73.92579620585356\n",
      "TimeSinceStart : 132.893315076828\n",
      "Training Loss : 0.16880397498607635\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 89001\n",
      "mean reward (100 episodes) -70.900435\n",
      "best mean reward -70.900435\n",
      "running time 134.464422\n",
      "Train_EnvstepsSoFar : 89001\n",
      "Train_AverageReturn : -70.9004348794226\n",
      "Train_BestReturn : -70.9004348794226\n",
      "TimeSinceStart : 134.46442198753357\n",
      "Training Loss : 0.487788587808609\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -70.282952\n",
      "best mean reward -70.282952\n",
      "running time 135.819410\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -70.28295182670932\n",
      "Train_BestReturn : -70.28295182670932\n",
      "TimeSinceStart : 135.81940984725952\n",
      "Training Loss : 0.6366502642631531\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 91001\n",
      "mean reward (100 episodes) -64.942051\n",
      "best mean reward -64.942051\n",
      "running time 137.147489\n",
      "Train_EnvstepsSoFar : 91001\n",
      "Train_AverageReturn : -64.9420506176348\n",
      "Train_BestReturn : -64.9420506176348\n",
      "TimeSinceStart : 137.14748883247375\n",
      "Training Loss : 0.7376590967178345\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 92001\n",
      "mean reward (100 episodes) -65.739250\n",
      "best mean reward -64.942051\n",
      "running time 138.846259\n",
      "Train_EnvstepsSoFar : 92001\n",
      "Train_AverageReturn : -65.73924963176738\n",
      "Train_BestReturn : -64.9420506176348\n",
      "TimeSinceStart : 138.84625887870789\n",
      "Training Loss : 0.6542156934738159\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 93001\n",
      "mean reward (100 episodes) -63.261552\n",
      "best mean reward -63.261552\n",
      "running time 140.264113\n",
      "Train_EnvstepsSoFar : 93001\n",
      "Train_AverageReturn : -63.26155155717509\n",
      "Train_BestReturn : -63.26155155717509\n",
      "TimeSinceStart : 140.26411318778992\n",
      "Training Loss : 0.36661577224731445\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 94001\n",
      "mean reward (100 episodes) -66.711252\n",
      "best mean reward -63.261552\n",
      "running time 141.776247\n",
      "Train_EnvstepsSoFar : 94001\n",
      "Train_AverageReturn : -66.7112521348732\n",
      "Train_BestReturn : -63.26155155717509\n",
      "TimeSinceStart : 141.77624702453613\n",
      "Training Loss : 0.09531942754983902\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 95001\n",
      "mean reward (100 episodes) -63.967170\n",
      "best mean reward -63.261552\n",
      "running time 143.301161\n",
      "Train_EnvstepsSoFar : 95001\n",
      "Train_AverageReturn : -63.967169742749874\n",
      "Train_BestReturn : -63.26155155717509\n",
      "TimeSinceStart : 143.30116081237793\n",
      "Training Loss : 0.09046176820993423\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 96001\n",
      "mean reward (100 episodes) -61.269442\n",
      "best mean reward -61.269442\n",
      "running time 144.581953\n",
      "Train_EnvstepsSoFar : 96001\n",
      "Train_AverageReturn : -61.26944205256237\n",
      "Train_BestReturn : -61.26944205256237\n",
      "TimeSinceStart : 144.58195304870605\n",
      "Training Loss : 1.5087254047393799\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 97001\n",
      "mean reward (100 episodes) -60.980676\n",
      "best mean reward -60.980676\n",
      "running time 145.884938\n",
      "Train_EnvstepsSoFar : 97001\n",
      "Train_AverageReturn : -60.980675619890654\n",
      "Train_BestReturn : -60.980675619890654\n",
      "TimeSinceStart : 145.8849380016327\n",
      "Training Loss : 0.31214001774787903\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 98001\n",
      "mean reward (100 episodes) -62.720961\n",
      "best mean reward -60.980676\n",
      "running time 147.005158\n",
      "Train_EnvstepsSoFar : 98001\n",
      "Train_AverageReturn : -62.72096149087373\n",
      "Train_BestReturn : -60.980675619890654\n",
      "TimeSinceStart : 147.00515794754028\n",
      "Training Loss : 0.1858968585729599\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 99001\n",
      "mean reward (100 episodes) -62.078647\n",
      "best mean reward -60.980676\n",
      "running time 148.649323\n",
      "Train_EnvstepsSoFar : 99001\n",
      "Train_AverageReturn : -62.07864736575106\n",
      "Train_BestReturn : -60.980675619890654\n",
      "TimeSinceStart : 148.64932298660278\n",
      "Training Loss : 0.07108231633901596\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -59.505810\n",
      "best mean reward -59.505810\n",
      "running time 150.033417\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -59.505809652880814\n",
      "Train_BestReturn : -59.505809652880814\n",
      "TimeSinceStart : 150.03341698646545\n",
      "Training Loss : 0.08301953971385956\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 101001\n",
      "mean reward (100 episodes) -62.470706\n",
      "best mean reward -59.505810\n",
      "running time 151.515009\n",
      "Train_EnvstepsSoFar : 101001\n",
      "Train_AverageReturn : -62.47070600021125\n",
      "Train_BestReturn : -59.505809652880814\n",
      "TimeSinceStart : 151.5150089263916\n",
      "Training Loss : 0.1472804844379425\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 102001\n",
      "mean reward (100 episodes) -60.790904\n",
      "best mean reward -59.505810\n",
      "running time 152.797745\n",
      "Train_EnvstepsSoFar : 102001\n",
      "Train_AverageReturn : -60.79090423105767\n",
      "Train_BestReturn : -59.505809652880814\n",
      "TimeSinceStart : 152.79774498939514\n",
      "Training Loss : 0.0904579907655716\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 103001\n",
      "mean reward (100 episodes) -58.667204\n",
      "best mean reward -58.667204\n",
      "running time 154.173925\n",
      "Train_EnvstepsSoFar : 103001\n",
      "Train_AverageReturn : -58.66720430548896\n",
      "Train_BestReturn : -58.66720430548896\n",
      "TimeSinceStart : 154.17392492294312\n",
      "Training Loss : 0.28860825300216675\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 104001\n",
      "mean reward (100 episodes) -57.842699\n",
      "best mean reward -57.842699\n",
      "running time 156.315784\n",
      "Train_EnvstepsSoFar : 104001\n",
      "Train_AverageReturn : -57.84269934068451\n",
      "Train_BestReturn : -57.84269934068451\n",
      "TimeSinceStart : 156.31578373908997\n",
      "Training Loss : 0.4456908404827118\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 105001\n",
      "mean reward (100 episodes) -57.745351\n",
      "best mean reward -57.745351\n",
      "running time 158.207442\n",
      "Train_EnvstepsSoFar : 105001\n",
      "Train_AverageReturn : -57.745351485345765\n",
      "Train_BestReturn : -57.745351485345765\n",
      "TimeSinceStart : 158.2074418067932\n",
      "Training Loss : 0.12120335549116135\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 106001\n",
      "mean reward (100 episodes) -55.018720\n",
      "best mean reward -55.018720\n",
      "running time 160.089506\n",
      "Train_EnvstepsSoFar : 106001\n",
      "Train_AverageReturn : -55.01872029447232\n",
      "Train_BestReturn : -55.01872029447232\n",
      "TimeSinceStart : 160.0895059108734\n",
      "Training Loss : 0.16076618432998657\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 107001\n",
      "mean reward (100 episodes) -54.044969\n",
      "best mean reward -54.044969\n",
      "running time 162.452250\n",
      "Train_EnvstepsSoFar : 107001\n",
      "Train_AverageReturn : -54.04496930345392\n",
      "Train_BestReturn : -54.04496930345392\n",
      "TimeSinceStart : 162.4522500038147\n",
      "Training Loss : 0.24581435322761536\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 108001\n",
      "mean reward (100 episodes) -53.465793\n",
      "best mean reward -53.465793\n",
      "running time 164.235550\n",
      "Train_EnvstepsSoFar : 108001\n",
      "Train_AverageReturn : -53.46579329714368\n",
      "Train_BestReturn : -53.46579329714368\n",
      "TimeSinceStart : 164.2355499267578\n",
      "Training Loss : 0.6710928678512573\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 109001\n",
      "mean reward (100 episodes) -52.895300\n",
      "best mean reward -52.895300\n",
      "running time 166.367783\n",
      "Train_EnvstepsSoFar : 109001\n",
      "Train_AverageReturn : -52.895299989919685\n",
      "Train_BestReturn : -52.895299989919685\n",
      "TimeSinceStart : 166.36778283119202\n",
      "Training Loss : 0.16763219237327576\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -48.792146\n",
      "best mean reward -48.792146\n",
      "running time 168.423471\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -48.79214570116927\n",
      "Train_BestReturn : -48.79214570116927\n",
      "TimeSinceStart : 168.4234709739685\n",
      "Training Loss : 0.11316476762294769\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 111001\n",
      "mean reward (100 episodes) -48.128358\n",
      "best mean reward -48.128358\n",
      "running time 170.448963\n",
      "Train_EnvstepsSoFar : 111001\n",
      "Train_AverageReturn : -48.12835782127519\n",
      "Train_BestReturn : -48.12835782127519\n",
      "TimeSinceStart : 170.44896292686462\n",
      "Training Loss : 0.17583370208740234\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 112001\n",
      "mean reward (100 episodes) -45.997663\n",
      "best mean reward -45.997663\n",
      "running time 172.584337\n",
      "Train_EnvstepsSoFar : 112001\n",
      "Train_AverageReturn : -45.9976630699412\n",
      "Train_BestReturn : -45.9976630699412\n",
      "TimeSinceStart : 172.5843369960785\n",
      "Training Loss : 0.0818011462688446\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 113001\n",
      "mean reward (100 episodes) -41.842285\n",
      "best mean reward -41.842285\n",
      "running time 174.338447\n",
      "Train_EnvstepsSoFar : 113001\n",
      "Train_AverageReturn : -41.84228549658447\n",
      "Train_BestReturn : -41.84228549658447\n",
      "TimeSinceStart : 174.33844709396362\n",
      "Training Loss : 0.20247532427310944\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 114001\n",
      "mean reward (100 episodes) -37.790081\n",
      "best mean reward -37.790081\n",
      "running time 175.973927\n",
      "Train_EnvstepsSoFar : 114001\n",
      "Train_AverageReturn : -37.79008084478391\n",
      "Train_BestReturn : -37.79008084478391\n",
      "TimeSinceStart : 175.97392678260803\n",
      "Training Loss : 0.13907849788665771\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 115001\n",
      "mean reward (100 episodes) -36.732173\n",
      "best mean reward -36.732173\n",
      "running time 177.594380\n",
      "Train_EnvstepsSoFar : 115001\n",
      "Train_AverageReturn : -36.73217314034331\n",
      "Train_BestReturn : -36.73217314034331\n",
      "TimeSinceStart : 177.594379901886\n",
      "Training Loss : 0.18342411518096924\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 116001\n",
      "mean reward (100 episodes) -32.476442\n",
      "best mean reward -32.476442\n",
      "running time 178.770561\n",
      "Train_EnvstepsSoFar : 116001\n",
      "Train_AverageReturn : -32.47644170333255\n",
      "Train_BestReturn : -32.47644170333255\n",
      "TimeSinceStart : 178.77056097984314\n",
      "Training Loss : 0.2673422694206238\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 117001\n",
      "mean reward (100 episodes) -32.080952\n",
      "best mean reward -32.080952\n",
      "running time 180.345322\n",
      "Train_EnvstepsSoFar : 117001\n",
      "Train_AverageReturn : -32.0809523328592\n",
      "Train_BestReturn : -32.0809523328592\n",
      "TimeSinceStart : 180.34532189369202\n",
      "Training Loss : 0.17027626931667328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 118001\n",
      "mean reward (100 episodes) -31.615699\n",
      "best mean reward -31.615699\n",
      "running time 181.791569\n",
      "Train_EnvstepsSoFar : 118001\n",
      "Train_AverageReturn : -31.61569944562797\n",
      "Train_BestReturn : -31.61569944562797\n",
      "TimeSinceStart : 181.7915689945221\n",
      "Training Loss : 0.08142801374197006\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 119001\n",
      "mean reward (100 episodes) -30.339732\n",
      "best mean reward -30.339732\n",
      "running time 183.418017\n",
      "Train_EnvstepsSoFar : 119001\n",
      "Train_AverageReturn : -30.339731745323146\n",
      "Train_BestReturn : -30.339731745323146\n",
      "TimeSinceStart : 183.41801691055298\n",
      "Training Loss : 0.1793414056301117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -30.190272\n",
      "best mean reward -30.190272\n",
      "running time 184.809452\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -30.19027162462692\n",
      "Train_BestReturn : -30.19027162462692\n",
      "TimeSinceStart : 184.80945205688477\n",
      "Training Loss : 0.25148245692253113\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 121001\n",
      "mean reward (100 episodes) -29.731602\n",
      "best mean reward -29.731602\n",
      "running time 186.082596\n",
      "Train_EnvstepsSoFar : 121001\n",
      "Train_AverageReturn : -29.731601721577675\n",
      "Train_BestReturn : -29.731601721577675\n",
      "TimeSinceStart : 186.0825960636139\n",
      "Training Loss : 0.21230392158031464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 122001\n",
      "mean reward (100 episodes) -26.980399\n",
      "best mean reward -26.980399\n",
      "running time 187.144383\n",
      "Train_EnvstepsSoFar : 122001\n",
      "Train_AverageReturn : -26.980399291879017\n",
      "Train_BestReturn : -26.980399291879017\n",
      "TimeSinceStart : 187.1443829536438\n",
      "Training Loss : 0.16287168860435486\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 123001\n",
      "mean reward (100 episodes) -26.628866\n",
      "best mean reward -26.628866\n",
      "running time 189.048885\n",
      "Train_EnvstepsSoFar : 123001\n",
      "Train_AverageReturn : -26.628865899841443\n",
      "Train_BestReturn : -26.628865899841443\n",
      "TimeSinceStart : 189.04888486862183\n",
      "Training Loss : 0.07057950645685196\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 124001\n",
      "mean reward (100 episodes) -26.956689\n",
      "best mean reward -26.628866\n",
      "running time 190.771170\n",
      "Train_EnvstepsSoFar : 124001\n",
      "Train_AverageReturn : -26.956688730132623\n",
      "Train_BestReturn : -26.628865899841443\n",
      "TimeSinceStart : 190.77116990089417\n",
      "Training Loss : 0.1393885761499405\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 125001\n",
      "mean reward (100 episodes) -27.627240\n",
      "best mean reward -26.628866\n",
      "running time 192.512461\n",
      "Train_EnvstepsSoFar : 125001\n",
      "Train_AverageReturn : -27.627240401755884\n",
      "Train_BestReturn : -26.628865899841443\n",
      "TimeSinceStart : 192.51246094703674\n",
      "Training Loss : 0.12211531400680542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 126001\n",
      "mean reward (100 episodes) -22.577667\n",
      "best mean reward -22.577667\n",
      "running time 193.771214\n",
      "Train_EnvstepsSoFar : 126001\n",
      "Train_AverageReturn : -22.577666916957096\n",
      "Train_BestReturn : -22.577666916957096\n",
      "TimeSinceStart : 193.7712140083313\n",
      "Training Loss : 0.17868590354919434\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 127001\n",
      "mean reward (100 episodes) -22.571044\n",
      "best mean reward -22.571044\n",
      "running time 196.037611\n",
      "Train_EnvstepsSoFar : 127001\n",
      "Train_AverageReturn : -22.571044315234513\n",
      "Train_BestReturn : -22.571044315234513\n",
      "TimeSinceStart : 196.03761100769043\n",
      "Training Loss : 0.1556406170129776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 128001\n",
      "mean reward (100 episodes) -23.569082\n",
      "best mean reward -22.571044\n",
      "running time 198.579643\n",
      "Train_EnvstepsSoFar : 128001\n",
      "Train_AverageReturn : -23.569081891889056\n",
      "Train_BestReturn : -22.571044315234513\n",
      "TimeSinceStart : 198.57964301109314\n",
      "Training Loss : 0.11926393955945969\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 129001\n",
      "mean reward (100 episodes) -22.498406\n",
      "best mean reward -22.498406\n",
      "running time 200.167916\n",
      "Train_EnvstepsSoFar : 129001\n",
      "Train_AverageReturn : -22.498405537372314\n",
      "Train_BestReturn : -22.498405537372314\n",
      "TimeSinceStart : 200.16791605949402\n",
      "Training Loss : 0.10097964107990265\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -22.662600\n",
      "best mean reward -22.498406\n",
      "running time 201.643574\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -22.662599728501505\n",
      "Train_BestReturn : -22.498405537372314\n",
      "TimeSinceStart : 201.6435739994049\n",
      "Training Loss : 0.5764037370681763\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 131001\n",
      "mean reward (100 episodes) -23.896624\n",
      "best mean reward -22.498406\n",
      "running time 203.096121\n",
      "Train_EnvstepsSoFar : 131001\n",
      "Train_AverageReturn : -23.89662430180174\n",
      "Train_BestReturn : -22.498405537372314\n",
      "TimeSinceStart : 203.09612107276917\n",
      "Training Loss : 0.14870770275592804\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 132001\n",
      "mean reward (100 episodes) -22.045246\n",
      "best mean reward -22.045246\n",
      "running time 204.540507\n",
      "Train_EnvstepsSoFar : 132001\n",
      "Train_AverageReturn : -22.045246419246624\n",
      "Train_BestReturn : -22.045246419246624\n",
      "TimeSinceStart : 204.54050707817078\n",
      "Training Loss : 0.22376245260238647\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 133001\n",
      "mean reward (100 episodes) -21.842703\n",
      "best mean reward -21.842703\n",
      "running time 206.194435\n",
      "Train_EnvstepsSoFar : 133001\n",
      "Train_AverageReturn : -21.842703304076657\n",
      "Train_BestReturn : -21.842703304076657\n",
      "TimeSinceStart : 206.19443488121033\n",
      "Training Loss : 0.17804093658924103\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 134001\n",
      "mean reward (100 episodes) -19.959301\n",
      "best mean reward -19.959301\n",
      "running time 207.888699\n",
      "Train_EnvstepsSoFar : 134001\n",
      "Train_AverageReturn : -19.959300788405994\n",
      "Train_BestReturn : -19.959300788405994\n",
      "TimeSinceStart : 207.88869905471802\n",
      "Training Loss : 0.06053698807954788\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 135001\n",
      "mean reward (100 episodes) -20.020543\n",
      "best mean reward -19.959301\n",
      "running time 209.576342\n",
      "Train_EnvstepsSoFar : 135001\n",
      "Train_AverageReturn : -20.020543051815135\n",
      "Train_BestReturn : -19.959300788405994\n",
      "TimeSinceStart : 209.5763418674469\n",
      "Training Loss : 0.16876539587974548\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 136001\n",
      "mean reward (100 episodes) -17.665551\n",
      "best mean reward -17.665551\n",
      "running time 210.786936\n",
      "Train_EnvstepsSoFar : 136001\n",
      "Train_AverageReturn : -17.66555114324096\n",
      "Train_BestReturn : -17.66555114324096\n",
      "TimeSinceStart : 210.786936044693\n",
      "Training Loss : 0.06874442845582962\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 137001\n",
      "mean reward (100 episodes) -15.479913\n",
      "best mean reward -15.479913\n",
      "running time 212.123020\n",
      "Train_EnvstepsSoFar : 137001\n",
      "Train_AverageReturn : -15.47991300577232\n",
      "Train_BestReturn : -15.47991300577232\n",
      "TimeSinceStart : 212.12301993370056\n",
      "Training Loss : 0.17264671623706818\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 138001\n",
      "mean reward (100 episodes) -15.425565\n",
      "best mean reward -15.425565\n",
      "running time 213.336014\n",
      "Train_EnvstepsSoFar : 138001\n",
      "Train_AverageReturn : -15.425564835772002\n",
      "Train_BestReturn : -15.425564835772002\n",
      "TimeSinceStart : 213.3360140323639\n",
      "Training Loss : 3.742847442626953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 139001\n",
      "mean reward (100 episodes) -18.493178\n",
      "best mean reward -15.425565\n",
      "running time 214.795021\n",
      "Train_EnvstepsSoFar : 139001\n",
      "Train_AverageReturn : -18.493178024283303\n",
      "Train_BestReturn : -15.425564835772002\n",
      "TimeSinceStart : 214.7950210571289\n",
      "Training Loss : 0.07181442528963089\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) -16.718162\n",
      "best mean reward -15.425565\n",
      "running time 216.292429\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : -16.71816171920888\n",
      "Train_BestReturn : -15.425564835772002\n",
      "TimeSinceStart : 216.2924289703369\n",
      "Training Loss : 0.13221292197704315\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 141001\n",
      "mean reward (100 episodes) -16.452648\n",
      "best mean reward -15.425565\n",
      "running time 218.247726\n",
      "Train_EnvstepsSoFar : 141001\n",
      "Train_AverageReturn : -16.452648344563162\n",
      "Train_BestReturn : -15.425564835772002\n",
      "TimeSinceStart : 218.24772596359253\n",
      "Training Loss : 0.8569134473800659\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 142001\n",
      "mean reward (100 episodes) -15.584227\n",
      "best mean reward -15.425565\n",
      "running time 219.940827\n",
      "Train_EnvstepsSoFar : 142001\n",
      "Train_AverageReturn : -15.584227268753606\n",
      "Train_BestReturn : -15.425564835772002\n",
      "TimeSinceStart : 219.94082713127136\n",
      "Training Loss : 0.3803580403327942\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 143001\n",
      "mean reward (100 episodes) -14.217256\n",
      "best mean reward -14.217256\n",
      "running time 221.237975\n",
      "Train_EnvstepsSoFar : 143001\n",
      "Train_AverageReturn : -14.217256057558862\n",
      "Train_BestReturn : -14.217256057558862\n",
      "TimeSinceStart : 221.23797488212585\n",
      "Training Loss : 0.16949710249900818\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 144001\n",
      "mean reward (100 episodes) -12.790698\n",
      "best mean reward -12.790698\n",
      "running time 222.792453\n",
      "Train_EnvstepsSoFar : 144001\n",
      "Train_AverageReturn : -12.790698076607821\n",
      "Train_BestReturn : -12.790698076607821\n",
      "TimeSinceStart : 222.7924530506134\n",
      "Training Loss : 0.10021468251943588\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 145001\n",
      "mean reward (100 episodes) -12.089501\n",
      "best mean reward -12.089501\n",
      "running time 224.311329\n",
      "Train_EnvstepsSoFar : 145001\n",
      "Train_AverageReturn : -12.089501467737845\n",
      "Train_BestReturn : -12.089501467737845\n",
      "TimeSinceStart : 224.31132888793945\n",
      "Training Loss : 0.7830524444580078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 146001\n",
      "mean reward (100 episodes) -8.148271\n",
      "best mean reward -8.148271\n",
      "running time 225.709905\n",
      "Train_EnvstepsSoFar : 146001\n",
      "Train_AverageReturn : -8.148270825714059\n",
      "Train_BestReturn : -8.148270825714059\n",
      "TimeSinceStart : 225.7099051475525\n",
      "Training Loss : 0.2521228790283203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 147001\n",
      "mean reward (100 episodes) -7.594739\n",
      "best mean reward -7.594739\n",
      "running time 227.047285\n",
      "Train_EnvstepsSoFar : 147001\n",
      "Train_AverageReturn : -7.594739483747003\n",
      "Train_BestReturn : -7.594739483747003\n",
      "TimeSinceStart : 227.04728484153748\n",
      "Training Loss : 0.07390733063220978\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 148001\n",
      "mean reward (100 episodes) -3.158783\n",
      "best mean reward -3.158783\n",
      "running time 228.425184\n",
      "Train_EnvstepsSoFar : 148001\n",
      "Train_AverageReturn : -3.158783082820834\n",
      "Train_BestReturn : -3.158783082820834\n",
      "TimeSinceStart : 228.42518401145935\n",
      "Training Loss : 0.9982399940490723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 149001\n",
      "mean reward (100 episodes) -2.148149\n",
      "best mean reward -2.148149\n",
      "running time 229.813839\n",
      "Train_EnvstepsSoFar : 149001\n",
      "Train_AverageReturn : -2.1481494308119657\n",
      "Train_BestReturn : -2.1481494308119657\n",
      "TimeSinceStart : 229.81383895874023\n",
      "Training Loss : 0.08155574649572372\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 1.860623\n",
      "best mean reward 1.860623\n",
      "running time 231.422353\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 1.8606228484335536\n",
      "Train_BestReturn : 1.8606228484335536\n",
      "TimeSinceStart : 231.4223530292511\n",
      "Training Loss : 0.0700269490480423\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 151001\n",
      "mean reward (100 episodes) 1.501584\n",
      "best mean reward 1.860623\n",
      "running time 233.216229\n",
      "Train_EnvstepsSoFar : 151001\n",
      "Train_AverageReturn : 1.5015836827297597\n",
      "Train_BestReturn : 1.8606228484335536\n",
      "TimeSinceStart : 233.21622896194458\n",
      "Training Loss : 0.14116916060447693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 152001\n",
      "mean reward (100 episodes) 0.684169\n",
      "best mean reward 1.860623\n",
      "running time 234.777095\n",
      "Train_EnvstepsSoFar : 152001\n",
      "Train_AverageReturn : 0.684168694032574\n",
      "Train_BestReturn : 1.8606228484335536\n",
      "TimeSinceStart : 234.77709484100342\n",
      "Training Loss : 0.6671502590179443\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 153001\n",
      "mean reward (100 episodes) 5.269281\n",
      "best mean reward 5.269281\n",
      "running time 235.781350\n",
      "Train_EnvstepsSoFar : 153001\n",
      "Train_AverageReturn : 5.269280809290849\n",
      "Train_BestReturn : 5.269280809290849\n",
      "TimeSinceStart : 235.78135013580322\n",
      "Training Loss : 0.3498696982860565\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 154001\n",
      "mean reward (100 episodes) 1.628312\n",
      "best mean reward 5.269281\n",
      "running time 236.916841\n",
      "Train_EnvstepsSoFar : 154001\n",
      "Train_AverageReturn : 1.6283122043179672\n",
      "Train_BestReturn : 5.269280809290849\n",
      "TimeSinceStart : 236.91684079170227\n",
      "Training Loss : 0.958717942237854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 155001\n",
      "mean reward (100 episodes) 5.177941\n",
      "best mean reward 5.269281\n",
      "running time 238.527454\n",
      "Train_EnvstepsSoFar : 155001\n",
      "Train_AverageReturn : 5.177941124193349\n",
      "Train_BestReturn : 5.269280809290849\n",
      "TimeSinceStart : 238.52745389938354\n",
      "Training Loss : 0.2498445212841034\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 156001\n",
      "mean reward (100 episodes) 3.542285\n",
      "best mean reward 5.269281\n",
      "running time 240.120576\n",
      "Train_EnvstepsSoFar : 156001\n",
      "Train_AverageReturn : 3.5422852790659953\n",
      "Train_BestReturn : 5.269280809290849\n",
      "TimeSinceStart : 240.1205759048462\n",
      "Training Loss : 0.22469967603683472\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 157001\n",
      "mean reward (100 episodes) 3.630678\n",
      "best mean reward 5.269281\n",
      "running time 241.449553\n",
      "Train_EnvstepsSoFar : 157001\n",
      "Train_AverageReturn : 3.630678300968512\n",
      "Train_BestReturn : 5.269280809290849\n",
      "TimeSinceStart : 241.4495530128479\n",
      "Training Loss : 0.054853539913892746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 158001\n",
      "mean reward (100 episodes) 5.134647\n",
      "best mean reward 5.269281\n",
      "running time 242.719804\n",
      "Train_EnvstepsSoFar : 158001\n",
      "Train_AverageReturn : 5.134647350704259\n",
      "Train_BestReturn : 5.269280809290849\n",
      "TimeSinceStart : 242.71980381011963\n",
      "Training Loss : 0.10820750892162323\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 159001\n",
      "mean reward (100 episodes) 4.354876\n",
      "best mean reward 5.269281\n",
      "running time 243.886111\n",
      "Train_EnvstepsSoFar : 159001\n",
      "Train_AverageReturn : 4.354875750998906\n",
      "Train_BestReturn : 5.269280809290849\n",
      "TimeSinceStart : 243.8861107826233\n",
      "Training Loss : 1.1268033981323242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 0.629406\n",
      "best mean reward 5.269281\n",
      "running time 245.888456\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 0.6294061989509249\n",
      "Train_BestReturn : 5.269280809290849\n",
      "TimeSinceStart : 245.88845586776733\n",
      "Training Loss : 0.22496388852596283\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 161001\n",
      "mean reward (100 episodes) 2.147478\n",
      "best mean reward 5.269281\n",
      "running time 247.679537\n",
      "Train_EnvstepsSoFar : 161001\n",
      "Train_AverageReturn : 2.147477532374928\n",
      "Train_BestReturn : 5.269280809290849\n",
      "TimeSinceStart : 247.6795370578766\n",
      "Training Loss : 0.5903938412666321\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 162001\n",
      "mean reward (100 episodes) 6.951820\n",
      "best mean reward 6.951820\n",
      "running time 249.544780\n",
      "Train_EnvstepsSoFar : 162001\n",
      "Train_AverageReturn : 6.951820435379829\n",
      "Train_BestReturn : 6.951820435379829\n",
      "TimeSinceStart : 249.54478001594543\n",
      "Training Loss : 0.2948496341705322\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 163001\n",
      "mean reward (100 episodes) 11.932630\n",
      "best mean reward 11.932630\n",
      "running time 251.128398\n",
      "Train_EnvstepsSoFar : 163001\n",
      "Train_AverageReturn : 11.93262964245207\n",
      "Train_BestReturn : 11.93262964245207\n",
      "TimeSinceStart : 251.12839794158936\n",
      "Training Loss : 0.07507467269897461\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 164001\n",
      "mean reward (100 episodes) 13.234820\n",
      "best mean reward 13.234820\n",
      "running time 252.357210\n",
      "Train_EnvstepsSoFar : 164001\n",
      "Train_AverageReturn : 13.234820023764382\n",
      "Train_BestReturn : 13.234820023764382\n",
      "TimeSinceStart : 252.35720992088318\n",
      "Training Loss : 0.06331637501716614\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 165001\n",
      "mean reward (100 episodes) 14.765960\n",
      "best mean reward 14.765960\n",
      "running time 253.942927\n",
      "Train_EnvstepsSoFar : 165001\n",
      "Train_AverageReturn : 14.76595981901014\n",
      "Train_BestReturn : 14.76595981901014\n",
      "TimeSinceStart : 253.9429271221161\n",
      "Training Loss : 0.10724122077226639\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 166001\n",
      "mean reward (100 episodes) 14.419809\n",
      "best mean reward 14.765960\n",
      "running time 256.952976\n",
      "Train_EnvstepsSoFar : 166001\n",
      "Train_AverageReturn : 14.419808728918422\n",
      "Train_BestReturn : 14.76595981901014\n",
      "TimeSinceStart : 256.95297598838806\n",
      "Training Loss : 0.16953057050704956\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 167001\n",
      "mean reward (100 episodes) 14.750190\n",
      "best mean reward 14.765960\n",
      "running time 258.853256\n",
      "Train_EnvstepsSoFar : 167001\n",
      "Train_AverageReturn : 14.750189971634349\n",
      "Train_BestReturn : 14.76595981901014\n",
      "TimeSinceStart : 258.8532557487488\n",
      "Training Loss : 0.06842730939388275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 168001\n",
      "mean reward (100 episodes) 19.022773\n",
      "best mean reward 19.022773\n",
      "running time 260.922518\n",
      "Train_EnvstepsSoFar : 168001\n",
      "Train_AverageReturn : 19.022772716412067\n",
      "Train_BestReturn : 19.022772716412067\n",
      "TimeSinceStart : 260.92251801490784\n",
      "Training Loss : 0.2957906723022461\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 169001\n",
      "mean reward (100 episodes) 20.585453\n",
      "best mean reward 20.585453\n",
      "running time 262.384567\n",
      "Train_EnvstepsSoFar : 169001\n",
      "Train_AverageReturn : 20.585453206181352\n",
      "Train_BestReturn : 20.585453206181352\n",
      "TimeSinceStart : 262.3845670223236\n",
      "Training Loss : 0.14250287413597107\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 22.524001\n",
      "best mean reward 22.524001\n",
      "running time 263.594593\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 22.524001166221293\n",
      "Train_BestReturn : 22.524001166221293\n",
      "TimeSinceStart : 263.5945930480957\n",
      "Training Loss : 0.09407076239585876\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 171001\n",
      "mean reward (100 episodes) 23.889611\n",
      "best mean reward 23.889611\n",
      "running time 264.737226\n",
      "Train_EnvstepsSoFar : 171001\n",
      "Train_AverageReturn : 23.88961100796872\n",
      "Train_BestReturn : 23.88961100796872\n",
      "TimeSinceStart : 264.7372260093689\n",
      "Training Loss : 3.022169589996338\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 172001\n",
      "mean reward (100 episodes) 24.442677\n",
      "best mean reward 24.442677\n",
      "running time 266.154022\n",
      "Train_EnvstepsSoFar : 172001\n",
      "Train_AverageReturn : 24.44267726019569\n",
      "Train_BestReturn : 24.44267726019569\n",
      "TimeSinceStart : 266.1540219783783\n",
      "Training Loss : 0.1688825786113739\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 173001\n",
      "mean reward (100 episodes) 25.133637\n",
      "best mean reward 25.133637\n",
      "running time 268.077102\n",
      "Train_EnvstepsSoFar : 173001\n",
      "Train_AverageReturn : 25.133637436462887\n",
      "Train_BestReturn : 25.133637436462887\n",
      "TimeSinceStart : 268.0771019458771\n",
      "Training Loss : 1.2560824155807495\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 174001\n",
      "mean reward (100 episodes) 24.920139\n",
      "best mean reward 25.133637\n",
      "running time 270.503168\n",
      "Train_EnvstepsSoFar : 174001\n",
      "Train_AverageReturn : 24.92013888758713\n",
      "Train_BestReturn : 25.133637436462887\n",
      "TimeSinceStart : 270.5031678676605\n",
      "Training Loss : 0.07847371697425842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 175001\n",
      "mean reward (100 episodes) 24.109304\n",
      "best mean reward 25.133637\n",
      "running time 271.921017\n",
      "Train_EnvstepsSoFar : 175001\n",
      "Train_AverageReturn : 24.109304177640464\n",
      "Train_BestReturn : 25.133637436462887\n",
      "TimeSinceStart : 271.9210169315338\n",
      "Training Loss : 0.1395568698644638\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 176001\n",
      "mean reward (100 episodes) 23.871641\n",
      "best mean reward 25.133637\n",
      "running time 272.807298\n",
      "Train_EnvstepsSoFar : 176001\n",
      "Train_AverageReturn : 23.87164132810186\n",
      "Train_BestReturn : 25.133637436462887\n",
      "TimeSinceStart : 272.8072979450226\n",
      "Training Loss : 0.04779103770852089\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 177001\n",
      "mean reward (100 episodes) 25.594208\n",
      "best mean reward 25.594208\n",
      "running time 274.323091\n",
      "Train_EnvstepsSoFar : 177001\n",
      "Train_AverageReturn : 25.59420768466358\n",
      "Train_BestReturn : 25.59420768466358\n",
      "TimeSinceStart : 274.32309079170227\n",
      "Training Loss : 0.03347799926996231\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 178001\n",
      "mean reward (100 episodes) 26.096018\n",
      "best mean reward 26.096018\n",
      "running time 275.747292\n",
      "Train_EnvstepsSoFar : 178001\n",
      "Train_AverageReturn : 26.096018206814534\n",
      "Train_BestReturn : 26.096018206814534\n",
      "TimeSinceStart : 275.74729204177856\n",
      "Training Loss : 0.9467405676841736\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 179001\n",
      "mean reward (100 episodes) 23.426240\n",
      "best mean reward 26.096018\n",
      "running time 277.161182\n",
      "Train_EnvstepsSoFar : 179001\n",
      "Train_AverageReturn : 23.426239862283705\n",
      "Train_BestReturn : 26.096018206814534\n",
      "TimeSinceStart : 277.1611819267273\n",
      "Training Loss : 0.23309507966041565\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 34.963352\n",
      "best mean reward 34.963352\n",
      "running time 278.025041\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 34.96335175669645\n",
      "Train_BestReturn : 34.96335175669645\n",
      "TimeSinceStart : 278.02504110336304\n",
      "Training Loss : 0.5570061206817627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 181001\n",
      "mean reward (100 episodes) 39.233403\n",
      "best mean reward 39.233403\n",
      "running time 278.996923\n",
      "Train_EnvstepsSoFar : 181001\n",
      "Train_AverageReturn : 39.233402893483365\n",
      "Train_BestReturn : 39.233402893483365\n",
      "TimeSinceStart : 278.9969229698181\n",
      "Training Loss : 0.1955505758523941\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 182001\n",
      "mean reward (100 episodes) 42.779049\n",
      "best mean reward 42.779049\n",
      "running time 280.241271\n",
      "Train_EnvstepsSoFar : 182001\n",
      "Train_AverageReturn : 42.77904941097452\n",
      "Train_BestReturn : 42.77904941097452\n",
      "TimeSinceStart : 280.24127101898193\n",
      "Training Loss : 0.11577986180782318\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 183001\n",
      "mean reward (100 episodes) 39.954956\n",
      "best mean reward 42.779049\n",
      "running time 282.488315\n",
      "Train_EnvstepsSoFar : 183001\n",
      "Train_AverageReturn : 39.954956408070046\n",
      "Train_BestReturn : 42.77904941097452\n",
      "TimeSinceStart : 282.48831510543823\n",
      "Training Loss : 0.12318742275238037\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 184001\n",
      "mean reward (100 episodes) 44.482027\n",
      "best mean reward 44.482027\n",
      "running time 284.643874\n",
      "Train_EnvstepsSoFar : 184001\n",
      "Train_AverageReturn : 44.48202701846064\n",
      "Train_BestReturn : 44.48202701846064\n",
      "TimeSinceStart : 284.6438739299774\n",
      "Training Loss : 0.9781712293624878\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 185001\n",
      "mean reward (100 episodes) 47.002011\n",
      "best mean reward 47.002011\n",
      "running time 287.199772\n",
      "Train_EnvstepsSoFar : 185001\n",
      "Train_AverageReturn : 47.00201072299042\n",
      "Train_BestReturn : 47.00201072299042\n",
      "TimeSinceStart : 287.1997721195221\n",
      "Training Loss : 0.10455722361803055\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 186001\n",
      "mean reward (100 episodes) 45.615573\n",
      "best mean reward 47.002011\n",
      "running time 289.626400\n",
      "Train_EnvstepsSoFar : 186001\n",
      "Train_AverageReturn : 45.61557266338437\n",
      "Train_BestReturn : 47.00201072299042\n",
      "TimeSinceStart : 289.6263999938965\n",
      "Training Loss : 0.12174345552921295\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 187001\n",
      "mean reward (100 episodes) 49.257756\n",
      "best mean reward 49.257756\n",
      "running time 290.674463\n",
      "Train_EnvstepsSoFar : 187001\n",
      "Train_AverageReturn : 49.25775593271944\n",
      "Train_BestReturn : 49.25775593271944\n",
      "TimeSinceStart : 290.67446279525757\n",
      "Training Loss : 1.1303558349609375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 188001\n",
      "mean reward (100 episodes) 49.234296\n",
      "best mean reward 49.257756\n",
      "running time 291.944776\n",
      "Train_EnvstepsSoFar : 188001\n",
      "Train_AverageReturn : 49.23429606213471\n",
      "Train_BestReturn : 49.25775593271944\n",
      "TimeSinceStart : 291.94477581977844\n",
      "Training Loss : 0.10789676010608673\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 189001\n",
      "mean reward (100 episodes) 47.696692\n",
      "best mean reward 49.257756\n",
      "running time 293.312530\n",
      "Train_EnvstepsSoFar : 189001\n",
      "Train_AverageReturn : 47.696691712183956\n",
      "Train_BestReturn : 49.25775593271944\n",
      "TimeSinceStart : 293.3125298023224\n",
      "Training Loss : 0.15952499210834503\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 49.370930\n",
      "best mean reward 49.370930\n",
      "running time 294.992953\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 49.370929588876116\n",
      "Train_BestReturn : 49.370929588876116\n",
      "TimeSinceStart : 294.9929530620575\n",
      "Training Loss : 0.14453883469104767\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 191001\n",
      "mean reward (100 episodes) 50.089508\n",
      "best mean reward 50.089508\n",
      "running time 296.390168\n",
      "Train_EnvstepsSoFar : 191001\n",
      "Train_AverageReturn : 50.08950791019681\n",
      "Train_BestReturn : 50.08950791019681\n",
      "TimeSinceStart : 296.39016795158386\n",
      "Training Loss : 1.2178553342819214\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 192001\n",
      "mean reward (100 episodes) 52.319806\n",
      "best mean reward 52.319806\n",
      "running time 297.457119\n",
      "Train_EnvstepsSoFar : 192001\n",
      "Train_AverageReturn : 52.319805519933425\n",
      "Train_BestReturn : 52.319805519933425\n",
      "TimeSinceStart : 297.4571189880371\n",
      "Training Loss : 0.12086065858602524\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 193001\n",
      "mean reward (100 episodes) 55.257655\n",
      "best mean reward 55.257655\n",
      "running time 299.109332\n",
      "Train_EnvstepsSoFar : 193001\n",
      "Train_AverageReturn : 55.25765521974081\n",
      "Train_BestReturn : 55.25765521974081\n",
      "TimeSinceStart : 299.1093318462372\n",
      "Training Loss : 0.06698362529277802\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 194001\n",
      "mean reward (100 episodes) 56.896834\n",
      "best mean reward 56.896834\n",
      "running time 300.371342\n",
      "Train_EnvstepsSoFar : 194001\n",
      "Train_AverageReturn : 56.89683440968067\n",
      "Train_BestReturn : 56.89683440968067\n",
      "TimeSinceStart : 300.37134194374084\n",
      "Training Loss : 0.0729583129286766\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 195001\n",
      "mean reward (100 episodes) 55.711063\n",
      "best mean reward 56.896834\n",
      "running time 301.559765\n",
      "Train_EnvstepsSoFar : 195001\n",
      "Train_AverageReturn : 55.71106309776942\n",
      "Train_BestReturn : 56.89683440968067\n",
      "TimeSinceStart : 301.55976486206055\n",
      "Training Loss : 0.4679454267024994\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 196001\n",
      "mean reward (100 episodes) 53.279679\n",
      "best mean reward 56.896834\n",
      "running time 303.561001\n",
      "Train_EnvstepsSoFar : 196001\n",
      "Train_AverageReturn : 53.27967922444887\n",
      "Train_BestReturn : 56.89683440968067\n",
      "TimeSinceStart : 303.5610008239746\n",
      "Training Loss : 0.07523884624242783\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 197001\n",
      "mean reward (100 episodes) 54.440452\n",
      "best mean reward 56.896834\n",
      "running time 304.591801\n",
      "Train_EnvstepsSoFar : 197001\n",
      "Train_AverageReturn : 54.44045202593351\n",
      "Train_BestReturn : 56.89683440968067\n",
      "TimeSinceStart : 304.59180092811584\n",
      "Training Loss : 0.1502901017665863\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 198001\n",
      "mean reward (100 episodes) 58.844165\n",
      "best mean reward 58.844165\n",
      "running time 305.524418\n",
      "Train_EnvstepsSoFar : 198001\n",
      "Train_AverageReturn : 58.84416548471575\n",
      "Train_BestReturn : 58.84416548471575\n",
      "TimeSinceStart : 305.52441787719727\n",
      "Training Loss : 0.06531716883182526\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 199001\n",
      "mean reward (100 episodes) 57.732641\n",
      "best mean reward 58.844165\n",
      "running time 306.546073\n",
      "Train_EnvstepsSoFar : 199001\n",
      "Train_AverageReturn : 57.7326414294376\n",
      "Train_BestReturn : 58.84416548471575\n",
      "TimeSinceStart : 306.5460729598999\n",
      "Training Loss : 0.6912947297096252\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 60.414385\n",
      "best mean reward 60.414385\n",
      "running time 307.487529\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 60.4143853382316\n",
      "Train_BestReturn : 60.4143853382316\n",
      "TimeSinceStart : 307.48752880096436\n",
      "Training Loss : 0.14872387051582336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 201001\n",
      "mean reward (100 episodes) 60.581640\n",
      "best mean reward 60.581640\n",
      "running time 309.052697\n",
      "Train_EnvstepsSoFar : 201001\n",
      "Train_AverageReturn : 60.58164026168967\n",
      "Train_BestReturn : 60.58164026168967\n",
      "TimeSinceStart : 309.0526969432831\n",
      "Training Loss : 0.09706389904022217\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 202001\n",
      "mean reward (100 episodes) 58.814705\n",
      "best mean reward 60.581640\n",
      "running time 309.869190\n",
      "Train_EnvstepsSoFar : 202001\n",
      "Train_AverageReturn : 58.81470509388705\n",
      "Train_BestReturn : 60.58164026168967\n",
      "TimeSinceStart : 309.8691899776459\n",
      "Training Loss : 0.1848442703485489\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 203001\n",
      "mean reward (100 episodes) 59.451437\n",
      "best mean reward 60.581640\n",
      "running time 311.226039\n",
      "Train_EnvstepsSoFar : 203001\n",
      "Train_AverageReturn : 59.451436848164604\n",
      "Train_BestReturn : 60.58164026168967\n",
      "TimeSinceStart : 311.2260389328003\n",
      "Training Loss : 0.26896196603775024\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 204001\n",
      "mean reward (100 episodes) 66.326271\n",
      "best mean reward 66.326271\n",
      "running time 312.017470\n",
      "Train_EnvstepsSoFar : 204001\n",
      "Train_AverageReturn : 66.32627071998795\n",
      "Train_BestReturn : 66.32627071998795\n",
      "TimeSinceStart : 312.0174698829651\n",
      "Training Loss : 0.08173460513353348\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 205001\n",
      "mean reward (100 episodes) 66.719451\n",
      "best mean reward 66.719451\n",
      "running time 313.006316\n",
      "Train_EnvstepsSoFar : 205001\n",
      "Train_AverageReturn : 66.71945131971964\n",
      "Train_BestReturn : 66.71945131971964\n",
      "TimeSinceStart : 313.006315946579\n",
      "Training Loss : 1.3786990642547607\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 206001\n",
      "mean reward (100 episodes) 71.323679\n",
      "best mean reward 71.323679\n",
      "running time 314.076572\n",
      "Train_EnvstepsSoFar : 206001\n",
      "Train_AverageReturn : 71.3236787119035\n",
      "Train_BestReturn : 71.3236787119035\n",
      "TimeSinceStart : 314.0765721797943\n",
      "Training Loss : 0.14628760516643524\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 207001\n",
      "mean reward (100 episodes) 75.889152\n",
      "best mean reward 75.889152\n",
      "running time 314.965543\n",
      "Train_EnvstepsSoFar : 207001\n",
      "Train_AverageReturn : 75.88915216685137\n",
      "Train_BestReturn : 75.88915216685137\n",
      "TimeSinceStart : 314.9655427932739\n",
      "Training Loss : 0.11080310493707657\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 208001\n",
      "mean reward (100 episodes) 72.632846\n",
      "best mean reward 75.889152\n",
      "running time 316.433662\n",
      "Train_EnvstepsSoFar : 208001\n",
      "Train_AverageReturn : 72.63284564460444\n",
      "Train_BestReturn : 75.88915216685137\n",
      "TimeSinceStart : 316.4336619377136\n",
      "Training Loss : 0.22962671518325806\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 209001\n",
      "mean reward (100 episodes) 76.908233\n",
      "best mean reward 76.908233\n",
      "running time 317.518853\n",
      "Train_EnvstepsSoFar : 209001\n",
      "Train_AverageReturn : 76.90823277792761\n",
      "Train_BestReturn : 76.90823277792761\n",
      "TimeSinceStart : 317.51885294914246\n",
      "Training Loss : 0.244569331407547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 77.417912\n",
      "best mean reward 77.417912\n",
      "running time 318.852277\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 77.41791213003486\n",
      "Train_BestReturn : 77.41791213003486\n",
      "TimeSinceStart : 318.852276802063\n",
      "Training Loss : 1.008420467376709\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 211001\n",
      "mean reward (100 episodes) 85.620304\n",
      "best mean reward 85.620304\n",
      "running time 319.752273\n",
      "Train_EnvstepsSoFar : 211001\n",
      "Train_AverageReturn : 85.62030392703758\n",
      "Train_BestReturn : 85.62030392703758\n",
      "TimeSinceStart : 319.75227308273315\n",
      "Training Loss : 0.11408573389053345\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 212001\n",
      "mean reward (100 episodes) 86.185081\n",
      "best mean reward 86.185081\n",
      "running time 321.042455\n",
      "Train_EnvstepsSoFar : 212001\n",
      "Train_AverageReturn : 86.18508119664817\n",
      "Train_BestReturn : 86.18508119664817\n",
      "TimeSinceStart : 321.04245495796204\n",
      "Training Loss : 0.10168532282114029\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 213001\n",
      "mean reward (100 episodes) 91.454408\n",
      "best mean reward 91.454408\n",
      "running time 321.819801\n",
      "Train_EnvstepsSoFar : 213001\n",
      "Train_AverageReturn : 91.4544082615604\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 321.8198010921478\n",
      "Training Loss : 0.1441272348165512\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 214001\n",
      "mean reward (100 episodes) 88.397841\n",
      "best mean reward 91.454408\n",
      "running time 323.427636\n",
      "Train_EnvstepsSoFar : 214001\n",
      "Train_AverageReturn : 88.39784142637643\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 323.42763590812683\n",
      "Training Loss : 0.7653184533119202\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 215001\n",
      "mean reward (100 episodes) 86.504140\n",
      "best mean reward 91.454408\n",
      "running time 325.012284\n",
      "Train_EnvstepsSoFar : 215001\n",
      "Train_AverageReturn : 86.5041399076641\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 325.01228380203247\n",
      "Training Loss : 0.40483975410461426\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 216001\n",
      "mean reward (100 episodes) 82.361750\n",
      "best mean reward 91.454408\n",
      "running time 326.237570\n",
      "Train_EnvstepsSoFar : 216001\n",
      "Train_AverageReturn : 82.36174958176484\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 326.23757004737854\n",
      "Training Loss : 0.09952462464570999\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 217001\n",
      "mean reward (100 episodes) 85.618630\n",
      "best mean reward 91.454408\n",
      "running time 327.092287\n",
      "Train_EnvstepsSoFar : 217001\n",
      "Train_AverageReturn : 85.61862963382573\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 327.09228682518005\n",
      "Training Loss : 0.4203139543533325\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 218001\n",
      "mean reward (100 episodes) 80.918575\n",
      "best mean reward 91.454408\n",
      "running time 328.346804\n",
      "Train_EnvstepsSoFar : 218001\n",
      "Train_AverageReturn : 80.91857496559395\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 328.3468039035797\n",
      "Training Loss : 1.2911956310272217\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 219001\n",
      "mean reward (100 episodes) 77.621431\n",
      "best mean reward 91.454408\n",
      "running time 329.378119\n",
      "Train_EnvstepsSoFar : 219001\n",
      "Train_AverageReturn : 77.62143087063625\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 329.3781187534332\n",
      "Training Loss : 0.0644141286611557\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 79.407850\n",
      "best mean reward 91.454408\n",
      "running time 330.418575\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 79.40784957744097\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 330.41857504844666\n",
      "Training Loss : 0.11494510620832443\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 221001\n",
      "mean reward (100 episodes) 78.887834\n",
      "best mean reward 91.454408\n",
      "running time 332.225011\n",
      "Train_EnvstepsSoFar : 221001\n",
      "Train_AverageReturn : 78.88783362979665\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 332.2250108718872\n",
      "Training Loss : 0.14762115478515625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 222001\n",
      "mean reward (100 episodes) 80.345110\n",
      "best mean reward 91.454408\n",
      "running time 334.495125\n",
      "Train_EnvstepsSoFar : 222001\n",
      "Train_AverageReturn : 80.34511026769442\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 334.49512481689453\n",
      "Training Loss : 3.315845489501953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 223001\n",
      "mean reward (100 episodes) 84.653596\n",
      "best mean reward 91.454408\n",
      "running time 335.384179\n",
      "Train_EnvstepsSoFar : 223001\n",
      "Train_AverageReturn : 84.65359602649369\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 335.38417887687683\n",
      "Training Loss : 1.142096757888794\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 224001\n",
      "mean reward (100 episodes) 86.643746\n",
      "best mean reward 91.454408\n",
      "running time 336.699931\n",
      "Train_EnvstepsSoFar : 224001\n",
      "Train_AverageReturn : 86.64374647825062\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 336.69993114471436\n",
      "Training Loss : 1.8320863246917725\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 225001\n",
      "mean reward (100 episodes) 86.120858\n",
      "best mean reward 91.454408\n",
      "running time 338.676638\n",
      "Train_EnvstepsSoFar : 225001\n",
      "Train_AverageReturn : 86.12085788418544\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 338.6766378879547\n",
      "Training Loss : 0.4435303211212158\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 226001\n",
      "mean reward (100 episodes) 87.010897\n",
      "best mean reward 91.454408\n",
      "running time 339.527544\n",
      "Train_EnvstepsSoFar : 226001\n",
      "Train_AverageReturn : 87.0108974933958\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 339.52754402160645\n",
      "Training Loss : 0.12835684418678284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 227001\n",
      "mean reward (100 episodes) 83.982932\n",
      "best mean reward 91.454408\n",
      "running time 340.518516\n",
      "Train_EnvstepsSoFar : 227001\n",
      "Train_AverageReturn : 83.98293199232907\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 340.5185158252716\n",
      "Training Loss : 1.6233614683151245\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 228001\n",
      "mean reward (100 episodes) 85.029929\n",
      "best mean reward 91.454408\n",
      "running time 341.646373\n",
      "Train_EnvstepsSoFar : 228001\n",
      "Train_AverageReturn : 85.02992945948588\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 341.64637303352356\n",
      "Training Loss : 0.10266523063182831\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 229001\n",
      "mean reward (100 episodes) 91.233010\n",
      "best mean reward 91.454408\n",
      "running time 342.982193\n",
      "Train_EnvstepsSoFar : 229001\n",
      "Train_AverageReturn : 91.23301044414029\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 342.98219299316406\n",
      "Training Loss : 1.1056981086730957\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 90.517438\n",
      "best mean reward 91.454408\n",
      "running time 343.943549\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 90.517438128706\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 343.9435489177704\n",
      "Training Loss : 0.24530093371868134\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 231001\n",
      "mean reward (100 episodes) 83.839178\n",
      "best mean reward 91.454408\n",
      "running time 345.034375\n",
      "Train_EnvstepsSoFar : 231001\n",
      "Train_AverageReturn : 83.83917779894242\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 345.0343749523163\n",
      "Training Loss : 0.20248520374298096\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 232001\n",
      "mean reward (100 episodes) 82.764424\n",
      "best mean reward 91.454408\n",
      "running time 346.146314\n",
      "Train_EnvstepsSoFar : 232001\n",
      "Train_AverageReturn : 82.76442448952199\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 346.14631390571594\n",
      "Training Loss : 4.809179782867432\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 233001\n",
      "mean reward (100 episodes) 76.795548\n",
      "best mean reward 91.454408\n",
      "running time 347.152469\n",
      "Train_EnvstepsSoFar : 233001\n",
      "Train_AverageReturn : 76.79554757560196\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 347.152468919754\n",
      "Training Loss : 0.39730799198150635\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 234001\n",
      "mean reward (100 episodes) 81.194793\n",
      "best mean reward 91.454408\n",
      "running time 348.194030\n",
      "Train_EnvstepsSoFar : 234001\n",
      "Train_AverageReturn : 81.19479266801318\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 348.194030046463\n",
      "Training Loss : 0.287461519241333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 235001\n",
      "mean reward (100 episodes) 79.087702\n",
      "best mean reward 91.454408\n",
      "running time 349.665268\n",
      "Train_EnvstepsSoFar : 235001\n",
      "Train_AverageReturn : 79.08770209991319\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 349.66526794433594\n",
      "Training Loss : 0.5386308431625366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 236001\n",
      "mean reward (100 episodes) 84.289341\n",
      "best mean reward 91.454408\n",
      "running time 350.446325\n",
      "Train_EnvstepsSoFar : 236001\n",
      "Train_AverageReturn : 84.28934057394989\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 350.44632482528687\n",
      "Training Loss : 0.4723398983478546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 237001\n",
      "mean reward (100 episodes) 85.618119\n",
      "best mean reward 91.454408\n",
      "running time 351.318175\n",
      "Train_EnvstepsSoFar : 237001\n",
      "Train_AverageReturn : 85.61811864197753\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 351.3181748390198\n",
      "Training Loss : 0.07412400841712952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 238001\n",
      "mean reward (100 episodes) 88.070966\n",
      "best mean reward 91.454408\n",
      "running time 352.333577\n",
      "Train_EnvstepsSoFar : 238001\n",
      "Train_AverageReturn : 88.07096583591107\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 352.3335769176483\n",
      "Training Loss : 0.09879082441329956\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 239001\n",
      "mean reward (100 episodes) 90.932054\n",
      "best mean reward 91.454408\n",
      "running time 353.318018\n",
      "Train_EnvstepsSoFar : 239001\n",
      "Train_AverageReturn : 90.93205437667748\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 353.3180179595947\n",
      "Training Loss : 6.272676467895508\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 87.131749\n",
      "best mean reward 91.454408\n",
      "running time 354.428717\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 87.13174859114207\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 354.4287168979645\n",
      "Training Loss : 0.3903731405735016\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 241001\n",
      "mean reward (100 episodes) 86.509031\n",
      "best mean reward 91.454408\n",
      "running time 355.368951\n",
      "Train_EnvstepsSoFar : 241001\n",
      "Train_AverageReturn : 86.50903110935505\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 355.36895084381104\n",
      "Training Loss : 0.6232486367225647\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 242001\n",
      "mean reward (100 episodes) 89.178159\n",
      "best mean reward 91.454408\n",
      "running time 356.299763\n",
      "Train_EnvstepsSoFar : 242001\n",
      "Train_AverageReturn : 89.17815941998887\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 356.29976296424866\n",
      "Training Loss : 0.4043824374675751\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 243001\n",
      "mean reward (100 episodes) 87.644099\n",
      "best mean reward 91.454408\n",
      "running time 357.211253\n",
      "Train_EnvstepsSoFar : 243001\n",
      "Train_AverageReturn : 87.6440986326663\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 357.21125292778015\n",
      "Training Loss : 1.100917935371399\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 244001\n",
      "mean reward (100 episodes) 88.896416\n",
      "best mean reward 91.454408\n",
      "running time 358.142984\n",
      "Train_EnvstepsSoFar : 244001\n",
      "Train_AverageReturn : 88.8964155217998\n",
      "Train_BestReturn : 91.4544082615604\n",
      "TimeSinceStart : 358.14298391342163\n",
      "Training Loss : 0.47944149374961853\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 245001\n",
      "mean reward (100 episodes) 91.618139\n",
      "best mean reward 91.618139\n",
      "running time 358.920735\n",
      "Train_EnvstepsSoFar : 245001\n",
      "Train_AverageReturn : 91.61813922184659\n",
      "Train_BestReturn : 91.61813922184659\n",
      "TimeSinceStart : 358.92073488235474\n",
      "Training Loss : 0.6787377595901489\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 246001\n",
      "mean reward (100 episodes) 91.261299\n",
      "best mean reward 91.618139\n",
      "running time 359.723580\n",
      "Train_EnvstepsSoFar : 246001\n",
      "Train_AverageReturn : 91.26129913853312\n",
      "Train_BestReturn : 91.61813922184659\n",
      "TimeSinceStart : 359.72357988357544\n",
      "Training Loss : 0.13472943007946014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 247001\n",
      "mean reward (100 episodes) 100.633856\n",
      "best mean reward 100.633856\n",
      "running time 360.503923\n",
      "Train_EnvstepsSoFar : 247001\n",
      "Train_AverageReturn : 100.63385645717311\n",
      "Train_BestReturn : 100.63385645717311\n",
      "TimeSinceStart : 360.50392293930054\n",
      "Training Loss : 0.12376295030117035\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 248001\n",
      "mean reward (100 episodes) 100.961195\n",
      "best mean reward 100.961195\n",
      "running time 361.469368\n",
      "Train_EnvstepsSoFar : 248001\n",
      "Train_AverageReturn : 100.96119474401584\n",
      "Train_BestReturn : 100.96119474401584\n",
      "TimeSinceStart : 361.46936798095703\n",
      "Training Loss : 0.12350241839885712\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 249001\n",
      "mean reward (100 episodes) 98.069086\n",
      "best mean reward 100.961195\n",
      "running time 363.145028\n",
      "Train_EnvstepsSoFar : 249001\n",
      "Train_AverageReturn : 98.06908643051338\n",
      "Train_BestReturn : 100.96119474401584\n",
      "TimeSinceStart : 363.14502811431885\n",
      "Training Loss : 0.18911725282669067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 103.465498\n",
      "best mean reward 103.465498\n",
      "running time 364.225469\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 103.46549833229288\n",
      "Train_BestReturn : 103.46549833229288\n",
      "TimeSinceStart : 364.22546887397766\n",
      "Training Loss : 0.7737953066825867\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 251001\n",
      "mean reward (100 episodes) 103.385917\n",
      "best mean reward 103.465498\n",
      "running time 365.318533\n",
      "Train_EnvstepsSoFar : 251001\n",
      "Train_AverageReturn : 103.38591723882915\n",
      "Train_BestReturn : 103.46549833229288\n",
      "TimeSinceStart : 365.3185329437256\n",
      "Training Loss : 0.4140259623527527\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 252001\n",
      "mean reward (100 episodes) 105.646826\n",
      "best mean reward 105.646826\n",
      "running time 366.630073\n",
      "Train_EnvstepsSoFar : 252001\n",
      "Train_AverageReturn : 105.64682565837047\n",
      "Train_BestReturn : 105.64682565837047\n",
      "TimeSinceStart : 366.6300730705261\n",
      "Training Loss : 1.69291353225708\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 253001\n",
      "mean reward (100 episodes) 103.116007\n",
      "best mean reward 105.646826\n",
      "running time 368.419016\n",
      "Train_EnvstepsSoFar : 253001\n",
      "Train_AverageReturn : 103.11600694930296\n",
      "Train_BestReturn : 105.64682565837047\n",
      "TimeSinceStart : 368.4190158843994\n",
      "Training Loss : 0.9797742366790771\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 254001\n",
      "mean reward (100 episodes) 109.018783\n",
      "best mean reward 109.018783\n",
      "running time 369.333325\n",
      "Train_EnvstepsSoFar : 254001\n",
      "Train_AverageReturn : 109.01878331938092\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 369.3333249092102\n",
      "Training Loss : 3.220900297164917\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 255001\n",
      "mean reward (100 episodes) 107.628767\n",
      "best mean reward 109.018783\n",
      "running time 370.539510\n",
      "Train_EnvstepsSoFar : 255001\n",
      "Train_AverageReturn : 107.62876683123473\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 370.539510011673\n",
      "Training Loss : 0.11961089819669724\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 256001\n",
      "mean reward (100 episodes) 103.295569\n",
      "best mean reward 109.018783\n",
      "running time 372.066747\n",
      "Train_EnvstepsSoFar : 256001\n",
      "Train_AverageReturn : 103.29556899012564\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 372.06674695014954\n",
      "Training Loss : 0.2145916372537613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 257001\n",
      "mean reward (100 episodes) 100.203992\n",
      "best mean reward 109.018783\n",
      "running time 373.626423\n",
      "Train_EnvstepsSoFar : 257001\n",
      "Train_AverageReturn : 100.20399191078006\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 373.6264228820801\n",
      "Training Loss : 0.9319051504135132\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 258001\n",
      "mean reward (100 episodes) 98.654750\n",
      "best mean reward 109.018783\n",
      "running time 376.013250\n",
      "Train_EnvstepsSoFar : 258001\n",
      "Train_AverageReturn : 98.65474961295443\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 376.013249874115\n",
      "Training Loss : 0.5588404536247253\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 259001\n",
      "mean reward (100 episodes) 96.060851\n",
      "best mean reward 109.018783\n",
      "running time 377.350007\n",
      "Train_EnvstepsSoFar : 259001\n",
      "Train_AverageReturn : 96.06085128863411\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 377.35000705718994\n",
      "Training Loss : 0.747431755065918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 99.786948\n",
      "best mean reward 109.018783\n",
      "running time 378.296355\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 99.78694811684468\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 378.296355009079\n",
      "Training Loss : 0.27970799803733826\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 261001\n",
      "mean reward (100 episodes) 98.286366\n",
      "best mean reward 109.018783\n",
      "running time 379.219812\n",
      "Train_EnvstepsSoFar : 261001\n",
      "Train_AverageReturn : 98.28636591637333\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 379.2198119163513\n",
      "Training Loss : 0.6446684002876282\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 262001\n",
      "mean reward (100 episodes) 96.931665\n",
      "best mean reward 109.018783\n",
      "running time 380.508241\n",
      "Train_EnvstepsSoFar : 262001\n",
      "Train_AverageReturn : 96.93166504038668\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 380.5082411766052\n",
      "Training Loss : 1.0983548164367676\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 263001\n",
      "mean reward (100 episodes) 98.862222\n",
      "best mean reward 109.018783\n",
      "running time 381.438322\n",
      "Train_EnvstepsSoFar : 263001\n",
      "Train_AverageReturn : 98.86222226987088\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 381.43832182884216\n",
      "Training Loss : 3.4026129245758057\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 264001\n",
      "mean reward (100 episodes) 102.443159\n",
      "best mean reward 109.018783\n",
      "running time 382.250893\n",
      "Train_EnvstepsSoFar : 264001\n",
      "Train_AverageReturn : 102.44315868172201\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 382.25089287757874\n",
      "Training Loss : 0.35614311695098877\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 265001\n",
      "mean reward (100 episodes) 95.583766\n",
      "best mean reward 109.018783\n",
      "running time 383.782526\n",
      "Train_EnvstepsSoFar : 265001\n",
      "Train_AverageReturn : 95.58376590912134\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 383.7825257778168\n",
      "Training Loss : 0.10478116571903229\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 266001\n",
      "mean reward (100 episodes) 93.796068\n",
      "best mean reward 109.018783\n",
      "running time 385.678045\n",
      "Train_EnvstepsSoFar : 266001\n",
      "Train_AverageReturn : 93.79606787951272\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 385.67804479599\n",
      "Training Loss : 0.8108552694320679\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 267001\n",
      "mean reward (100 episodes) 87.577662\n",
      "best mean reward 109.018783\n",
      "running time 386.529677\n",
      "Train_EnvstepsSoFar : 267001\n",
      "Train_AverageReturn : 87.57766238370051\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 386.5296769142151\n",
      "Training Loss : 0.7278845906257629\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 268001\n",
      "mean reward (100 episodes) 84.776537\n",
      "best mean reward 109.018783\n",
      "running time 387.575661\n",
      "Train_EnvstepsSoFar : 268001\n",
      "Train_AverageReturn : 84.77653747449455\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 387.575660943985\n",
      "Training Loss : 0.6859464645385742\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 269001\n",
      "mean reward (100 episodes) 85.414551\n",
      "best mean reward 109.018783\n",
      "running time 389.308795\n",
      "Train_EnvstepsSoFar : 269001\n",
      "Train_AverageReturn : 85.41455119703465\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 389.30879497528076\n",
      "Training Loss : 1.2904797792434692\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 82.580029\n",
      "best mean reward 109.018783\n",
      "running time 390.280508\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 82.5800292246783\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 390.28050780296326\n",
      "Training Loss : 0.13859644532203674\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 271001\n",
      "mean reward (100 episodes) 82.359140\n",
      "best mean reward 109.018783\n",
      "running time 391.765006\n",
      "Train_EnvstepsSoFar : 271001\n",
      "Train_AverageReturn : 82.35914049216255\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 391.76500606536865\n",
      "Training Loss : 2.8618948459625244\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 272001\n",
      "mean reward (100 episodes) 82.376774\n",
      "best mean reward 109.018783\n",
      "running time 392.969249\n",
      "Train_EnvstepsSoFar : 272001\n",
      "Train_AverageReturn : 82.3767739794244\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 392.96924901008606\n",
      "Training Loss : 2.113731861114502\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 273001\n",
      "mean reward (100 episodes) 82.618598\n",
      "best mean reward 109.018783\n",
      "running time 393.822158\n",
      "Train_EnvstepsSoFar : 273001\n",
      "Train_AverageReturn : 82.61859809332192\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 393.8221580982208\n",
      "Training Loss : 0.28502994775772095\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 274001\n",
      "mean reward (100 episodes) 80.992021\n",
      "best mean reward 109.018783\n",
      "running time 394.844257\n",
      "Train_EnvstepsSoFar : 274001\n",
      "Train_AverageReturn : 80.99202128271934\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 394.84425687789917\n",
      "Training Loss : 0.12037961184978485\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 275001\n",
      "mean reward (100 episodes) 76.384096\n",
      "best mean reward 109.018783\n",
      "running time 395.627578\n",
      "Train_EnvstepsSoFar : 275001\n",
      "Train_AverageReturn : 76.38409587464248\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 395.6275780200958\n",
      "Training Loss : 0.07567357271909714\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 276001\n",
      "mean reward (100 episodes) 70.286854\n",
      "best mean reward 109.018783\n",
      "running time 396.435546\n",
      "Train_EnvstepsSoFar : 276001\n",
      "Train_AverageReturn : 70.28685372455837\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 396.4355459213257\n",
      "Training Loss : 0.22014102339744568\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 277001\n",
      "mean reward (100 episodes) 72.525885\n",
      "best mean reward 109.018783\n",
      "running time 397.775645\n",
      "Train_EnvstepsSoFar : 277001\n",
      "Train_AverageReturn : 72.5258847154517\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 397.7756447792053\n",
      "Training Loss : 0.5387941002845764\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 278001\n",
      "mean reward (100 episodes) 71.754953\n",
      "best mean reward 109.018783\n",
      "running time 399.643881\n",
      "Train_EnvstepsSoFar : 278001\n",
      "Train_AverageReturn : 71.75495254350044\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 399.6438808441162\n",
      "Training Loss : 0.4646560549736023\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 279001\n",
      "mean reward (100 episodes) 68.287723\n",
      "best mean reward 109.018783\n",
      "running time 400.386806\n",
      "Train_EnvstepsSoFar : 279001\n",
      "Train_AverageReturn : 68.28772345422722\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 400.38680601119995\n",
      "Training Loss : 0.08456534892320633\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 71.313675\n",
      "best mean reward 109.018783\n",
      "running time 401.191195\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 71.31367473990436\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 401.1911950111389\n",
      "Training Loss : 0.6801431179046631\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 281001\n",
      "mean reward (100 episodes) 74.429235\n",
      "best mean reward 109.018783\n",
      "running time 401.991370\n",
      "Train_EnvstepsSoFar : 281001\n",
      "Train_AverageReturn : 74.42923501571134\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 401.99136996269226\n",
      "Training Loss : 0.21618518233299255\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 282001\n",
      "mean reward (100 episodes) 74.859649\n",
      "best mean reward 109.018783\n",
      "running time 402.791330\n",
      "Train_EnvstepsSoFar : 282001\n",
      "Train_AverageReturn : 74.8596493941266\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 402.79132986068726\n",
      "Training Loss : 3.354663372039795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 283001\n",
      "mean reward (100 episodes) 81.586348\n",
      "best mean reward 109.018783\n",
      "running time 403.604938\n",
      "Train_EnvstepsSoFar : 283001\n",
      "Train_AverageReturn : 81.58634809374134\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 403.60493779182434\n",
      "Training Loss : 0.1768721044063568\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 284001\n",
      "mean reward (100 episodes) 81.630955\n",
      "best mean reward 109.018783\n",
      "running time 404.464802\n",
      "Train_EnvstepsSoFar : 284001\n",
      "Train_AverageReturn : 81.63095500592001\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 404.46480202674866\n",
      "Training Loss : 1.4488272666931152\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 285001\n",
      "mean reward (100 episodes) 90.378836\n",
      "best mean reward 109.018783\n",
      "running time 405.238499\n",
      "Train_EnvstepsSoFar : 285001\n",
      "Train_AverageReturn : 90.37883591644433\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 405.2384989261627\n",
      "Training Loss : 0.7920814752578735\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 286001\n",
      "mean reward (100 episodes) 89.123813\n",
      "best mean reward 109.018783\n",
      "running time 406.215609\n",
      "Train_EnvstepsSoFar : 286001\n",
      "Train_AverageReturn : 89.12381337634547\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 406.2156090736389\n",
      "Training Loss : 0.1544821858406067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 287001\n",
      "mean reward (100 episodes) 96.531451\n",
      "best mean reward 109.018783\n",
      "running time 407.067126\n",
      "Train_EnvstepsSoFar : 287001\n",
      "Train_AverageReturn : 96.53145092184455\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 407.0671260356903\n",
      "Training Loss : 0.10872981697320938\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 288001\n",
      "mean reward (100 episodes) 99.360434\n",
      "best mean reward 109.018783\n",
      "running time 408.170010\n",
      "Train_EnvstepsSoFar : 288001\n",
      "Train_AverageReturn : 99.36043440496972\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 408.17001008987427\n",
      "Training Loss : 0.4534789025783539\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 289001\n",
      "mean reward (100 episodes) 107.875055\n",
      "best mean reward 109.018783\n",
      "running time 409.066557\n",
      "Train_EnvstepsSoFar : 289001\n",
      "Train_AverageReturn : 107.8750547086724\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 409.066556930542\n",
      "Training Loss : 0.503465473651886\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 106.797089\n",
      "best mean reward 109.018783\n",
      "running time 409.977436\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 106.79708852927195\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 409.9774360656738\n",
      "Training Loss : 0.21598830819129944\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 291001\n",
      "mean reward (100 episodes) 107.279119\n",
      "best mean reward 109.018783\n",
      "running time 411.100193\n",
      "Train_EnvstepsSoFar : 291001\n",
      "Train_AverageReturn : 107.27911873386614\n",
      "Train_BestReturn : 109.01878331938092\n",
      "TimeSinceStart : 411.10019302368164\n",
      "Training Loss : 0.6054992079734802\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 292001\n",
      "mean reward (100 episodes) 110.585593\n",
      "best mean reward 110.585593\n",
      "running time 411.982218\n",
      "Train_EnvstepsSoFar : 292001\n",
      "Train_AverageReturn : 110.58559293504749\n",
      "Train_BestReturn : 110.58559293504749\n",
      "TimeSinceStart : 411.9822177886963\n",
      "Training Loss : 2.346296787261963\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 293001\n",
      "mean reward (100 episodes) 111.939393\n",
      "best mean reward 111.939393\n",
      "running time 412.923225\n",
      "Train_EnvstepsSoFar : 293001\n",
      "Train_AverageReturn : 111.93939263229157\n",
      "Train_BestReturn : 111.93939263229157\n",
      "TimeSinceStart : 412.9232249259949\n",
      "Training Loss : 0.2586383819580078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 294001\n",
      "mean reward (100 episodes) 113.816884\n",
      "best mean reward 113.816884\n",
      "running time 413.881398\n",
      "Train_EnvstepsSoFar : 294001\n",
      "Train_AverageReturn : 113.81688431801226\n",
      "Train_BestReturn : 113.81688431801226\n",
      "TimeSinceStart : 413.8813979625702\n",
      "Training Loss : 0.4624762237071991\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 295001\n",
      "mean reward (100 episodes) 113.938804\n",
      "best mean reward 113.938804\n",
      "running time 414.887602\n",
      "Train_EnvstepsSoFar : 295001\n",
      "Train_AverageReturn : 113.93880441770794\n",
      "Train_BestReturn : 113.93880441770794\n",
      "TimeSinceStart : 414.887601852417\n",
      "Training Loss : 0.3783397674560547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 296001\n",
      "mean reward (100 episodes) 116.973360\n",
      "best mean reward 116.973360\n",
      "running time 416.301603\n",
      "Train_EnvstepsSoFar : 296001\n",
      "Train_AverageReturn : 116.973359752044\n",
      "Train_BestReturn : 116.973359752044\n",
      "TimeSinceStart : 416.30160307884216\n",
      "Training Loss : 0.1447559893131256\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 297001\n",
      "mean reward (100 episodes) 117.596691\n",
      "best mean reward 117.596691\n",
      "running time 417.176826\n",
      "Train_EnvstepsSoFar : 297001\n",
      "Train_AverageReturn : 117.59669147120246\n",
      "Train_BestReturn : 117.59669147120246\n",
      "TimeSinceStart : 417.1768260002136\n",
      "Training Loss : 0.1355668306350708\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 298001\n",
      "mean reward (100 episodes) 120.710167\n",
      "best mean reward 120.710167\n",
      "running time 418.120970\n",
      "Train_EnvstepsSoFar : 298001\n",
      "Train_AverageReturn : 120.71016686179694\n",
      "Train_BestReturn : 120.71016686179694\n",
      "TimeSinceStart : 418.12097001075745\n",
      "Training Loss : 0.34348833560943604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 299001\n",
      "mean reward (100 episodes) 125.826489\n",
      "best mean reward 125.826489\n",
      "running time 419.086683\n",
      "Train_EnvstepsSoFar : 299001\n",
      "Train_AverageReturn : 125.82648872497164\n",
      "Train_BestReturn : 125.82648872497164\n",
      "TimeSinceStart : 419.08668303489685\n",
      "Training Loss : 0.25817906856536865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 127.272926\n",
      "best mean reward 127.272926\n",
      "running time 419.879886\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 127.27292556262545\n",
      "Train_BestReturn : 127.27292556262545\n",
      "TimeSinceStart : 419.8798859119415\n",
      "Training Loss : 0.27582305669784546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 301001\n",
      "mean reward (100 episodes) 127.714517\n",
      "best mean reward 127.714517\n",
      "running time 420.764901\n",
      "Train_EnvstepsSoFar : 301001\n",
      "Train_AverageReturn : 127.7145165573465\n",
      "Train_BestReturn : 127.7145165573465\n",
      "TimeSinceStart : 420.76490092277527\n",
      "Training Loss : 0.39768821001052856\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 302001\n",
      "mean reward (100 episodes) 121.259458\n",
      "best mean reward 127.714517\n",
      "running time 421.714750\n",
      "Train_EnvstepsSoFar : 302001\n",
      "Train_AverageReturn : 121.2594580098927\n",
      "Train_BestReturn : 127.7145165573465\n",
      "TimeSinceStart : 421.71474981307983\n",
      "Training Loss : 0.11118083447217941\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 303001\n",
      "mean reward (100 episodes) 120.828382\n",
      "best mean reward 127.714517\n",
      "running time 422.589437\n",
      "Train_EnvstepsSoFar : 303001\n",
      "Train_AverageReturn : 120.82838176499598\n",
      "Train_BestReturn : 127.7145165573465\n",
      "TimeSinceStart : 422.5894367694855\n",
      "Training Loss : 0.5195868015289307\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 304001\n",
      "mean reward (100 episodes) 124.037010\n",
      "best mean reward 127.714517\n",
      "running time 423.548292\n",
      "Train_EnvstepsSoFar : 304001\n",
      "Train_AverageReturn : 124.03700953159095\n",
      "Train_BestReturn : 127.7145165573465\n",
      "TimeSinceStart : 423.5482919216156\n",
      "Training Loss : 0.3113824129104614\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 305001\n",
      "mean reward (100 episodes) 126.877365\n",
      "best mean reward 127.714517\n",
      "running time 424.465339\n",
      "Train_EnvstepsSoFar : 305001\n",
      "Train_AverageReturn : 126.87736546811473\n",
      "Train_BestReturn : 127.7145165573465\n",
      "TimeSinceStart : 424.4653389453888\n",
      "Training Loss : 0.6346451640129089\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 306001\n",
      "mean reward (100 episodes) 129.333943\n",
      "best mean reward 129.333943\n",
      "running time 425.200955\n",
      "Train_EnvstepsSoFar : 306001\n",
      "Train_AverageReturn : 129.33394296531972\n",
      "Train_BestReturn : 129.33394296531972\n",
      "TimeSinceStart : 425.2009551525116\n",
      "Training Loss : 0.09166543930768967\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 307001\n",
      "mean reward (100 episodes) 126.567706\n",
      "best mean reward 129.333943\n",
      "running time 425.918328\n",
      "Train_EnvstepsSoFar : 307001\n",
      "Train_AverageReturn : 126.56770597259835\n",
      "Train_BestReturn : 129.33394296531972\n",
      "TimeSinceStart : 425.9183280467987\n",
      "Training Loss : 1.5172009468078613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 308001\n",
      "mean reward (100 episodes) 129.391668\n",
      "best mean reward 129.391668\n",
      "running time 426.675714\n",
      "Train_EnvstepsSoFar : 308001\n",
      "Train_AverageReturn : 129.39166778103817\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 426.6757140159607\n",
      "Training Loss : 1.5771809816360474\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 309001\n",
      "mean reward (100 episodes) 124.100597\n",
      "best mean reward 129.391668\n",
      "running time 427.409700\n",
      "Train_EnvstepsSoFar : 309001\n",
      "Train_AverageReturn : 124.10059741412051\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 427.4096999168396\n",
      "Training Loss : 0.2152833789587021\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 120.294977\n",
      "best mean reward 129.391668\n",
      "running time 428.130053\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 120.29497693956178\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 428.1300530433655\n",
      "Training Loss : 0.5451587438583374\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 311001\n",
      "mean reward (100 episodes) 126.518076\n",
      "best mean reward 129.391668\n",
      "running time 428.933423\n",
      "Train_EnvstepsSoFar : 311001\n",
      "Train_AverageReturn : 126.51807626623741\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 428.93342304229736\n",
      "Training Loss : 0.8958724141120911\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 312001\n",
      "mean reward (100 episodes) 122.383059\n",
      "best mean reward 129.391668\n",
      "running time 429.699239\n",
      "Train_EnvstepsSoFar : 312001\n",
      "Train_AverageReturn : 122.38305876746823\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 429.69923877716064\n",
      "Training Loss : 1.9312654733657837\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 313001\n",
      "mean reward (100 episodes) 118.123866\n",
      "best mean reward 129.391668\n",
      "running time 430.452055\n",
      "Train_EnvstepsSoFar : 313001\n",
      "Train_AverageReturn : 118.12386642463092\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 430.452054977417\n",
      "Training Loss : 0.10870785266160965\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 314001\n",
      "mean reward (100 episodes) 110.441815\n",
      "best mean reward 129.391668\n",
      "running time 431.202548\n",
      "Train_EnvstepsSoFar : 314001\n",
      "Train_AverageReturn : 110.4418147027127\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 431.20254778862\n",
      "Training Loss : 0.08754324913024902\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 315001\n",
      "mean reward (100 episodes) 110.155416\n",
      "best mean reward 129.391668\n",
      "running time 432.036471\n",
      "Train_EnvstepsSoFar : 315001\n",
      "Train_AverageReturn : 110.15541565845433\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 432.03647089004517\n",
      "Training Loss : 0.19762617349624634\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 316001\n",
      "mean reward (100 episodes) 110.533595\n",
      "best mean reward 129.391668\n",
      "running time 432.762188\n",
      "Train_EnvstepsSoFar : 316001\n",
      "Train_AverageReturn : 110.53359539448822\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 432.7621879577637\n",
      "Training Loss : 2.5047831535339355\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 317001\n",
      "mean reward (100 episodes) 109.984069\n",
      "best mean reward 129.391668\n",
      "running time 433.654148\n",
      "Train_EnvstepsSoFar : 317001\n",
      "Train_AverageReturn : 109.98406889284166\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 433.65414810180664\n",
      "Training Loss : 2.802786350250244\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 318001\n",
      "mean reward (100 episodes) 116.815669\n",
      "best mean reward 129.391668\n",
      "running time 434.368771\n",
      "Train_EnvstepsSoFar : 318001\n",
      "Train_AverageReturn : 116.81566922527277\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 434.3687710762024\n",
      "Training Loss : 1.5233256816864014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 319001\n",
      "mean reward (100 episodes) 117.926525\n",
      "best mean reward 129.391668\n",
      "running time 435.105289\n",
      "Train_EnvstepsSoFar : 319001\n",
      "Train_AverageReturn : 117.92652506055407\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 435.10528898239136\n",
      "Training Loss : 1.814652919769287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 116.323508\n",
      "best mean reward 129.391668\n",
      "running time 435.843872\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 116.3235079273454\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 435.8438718318939\n",
      "Training Loss : 2.496819019317627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 321001\n",
      "mean reward (100 episodes) 118.022613\n",
      "best mean reward 129.391668\n",
      "running time 436.580828\n",
      "Train_EnvstepsSoFar : 321001\n",
      "Train_AverageReturn : 118.02261278849299\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 436.58082818984985\n",
      "Training Loss : 0.08433202654123306\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 322001\n",
      "mean reward (100 episodes) 121.770066\n",
      "best mean reward 129.391668\n",
      "running time 437.333557\n",
      "Train_EnvstepsSoFar : 322001\n",
      "Train_AverageReturn : 121.77006603053628\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 437.33355689048767\n",
      "Training Loss : 0.5536627769470215\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 323001\n",
      "mean reward (100 episodes) 120.116410\n",
      "best mean reward 129.391668\n",
      "running time 438.099124\n",
      "Train_EnvstepsSoFar : 323001\n",
      "Train_AverageReturn : 120.11641009866953\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 438.09912395477295\n",
      "Training Loss : 1.1865179538726807\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 324001\n",
      "mean reward (100 episodes) 125.525447\n",
      "best mean reward 129.391668\n",
      "running time 438.881788\n",
      "Train_EnvstepsSoFar : 324001\n",
      "Train_AverageReturn : 125.52544714242205\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 438.881787776947\n",
      "Training Loss : 1.112888216972351\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 325001\n",
      "mean reward (100 episodes) 125.805336\n",
      "best mean reward 129.391668\n",
      "running time 439.786147\n",
      "Train_EnvstepsSoFar : 325001\n",
      "Train_AverageReturn : 125.80533587494575\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 439.78614687919617\n",
      "Training Loss : 0.41255873441696167\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 326001\n",
      "mean reward (100 episodes) 126.916894\n",
      "best mean reward 129.391668\n",
      "running time 440.663075\n",
      "Train_EnvstepsSoFar : 326001\n",
      "Train_AverageReturn : 126.916893693459\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 440.66307497024536\n",
      "Training Loss : 2.16813063621521\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 327001\n",
      "mean reward (100 episodes) 125.328907\n",
      "best mean reward 129.391668\n",
      "running time 441.732408\n",
      "Train_EnvstepsSoFar : 327001\n",
      "Train_AverageReturn : 125.32890679141987\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 441.7324080467224\n",
      "Training Loss : 0.11952359974384308\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 328001\n",
      "mean reward (100 episodes) 118.362517\n",
      "best mean reward 129.391668\n",
      "running time 442.488902\n",
      "Train_EnvstepsSoFar : 328001\n",
      "Train_AverageReturn : 118.36251668192091\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 442.48890209198\n",
      "Training Loss : 1.3279225826263428\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 329001\n",
      "mean reward (100 episodes) 128.748779\n",
      "best mean reward 129.391668\n",
      "running time 443.326154\n",
      "Train_EnvstepsSoFar : 329001\n",
      "Train_AverageReturn : 128.748778543979\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 443.32615399360657\n",
      "Training Loss : 1.4228425025939941\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 127.578152\n",
      "best mean reward 129.391668\n",
      "running time 444.067001\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 127.57815153502959\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 444.06700110435486\n",
      "Training Loss : 5.428314685821533\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 331001\n",
      "mean reward (100 episodes) 126.272002\n",
      "best mean reward 129.391668\n",
      "running time 444.784544\n",
      "Train_EnvstepsSoFar : 331001\n",
      "Train_AverageReturn : 126.27200156319748\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 444.78454399108887\n",
      "Training Loss : 0.365846186876297\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 332001\n",
      "mean reward (100 episodes) 120.901736\n",
      "best mean reward 129.391668\n",
      "running time 445.857218\n",
      "Train_EnvstepsSoFar : 332001\n",
      "Train_AverageReturn : 120.90173606130207\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 445.85721802711487\n",
      "Training Loss : 4.627022743225098\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 333001\n",
      "mean reward (100 episodes) 118.929666\n",
      "best mean reward 129.391668\n",
      "running time 446.847328\n",
      "Train_EnvstepsSoFar : 333001\n",
      "Train_AverageReturn : 118.9296656859522\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 446.8473279476166\n",
      "Training Loss : 1.5665571689605713\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 334001\n",
      "mean reward (100 episodes) 117.610684\n",
      "best mean reward 129.391668\n",
      "running time 447.555474\n",
      "Train_EnvstepsSoFar : 334001\n",
      "Train_AverageReturn : 117.61068375046032\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 447.5554738044739\n",
      "Training Loss : 2.5000319480895996\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 335001\n",
      "mean reward (100 episodes) 114.714522\n",
      "best mean reward 129.391668\n",
      "running time 448.265473\n",
      "Train_EnvstepsSoFar : 335001\n",
      "Train_AverageReturn : 114.7145217287308\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 448.26547288894653\n",
      "Training Loss : 0.07497890293598175\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 336001\n",
      "mean reward (100 episodes) 118.322149\n",
      "best mean reward 129.391668\n",
      "running time 449.024284\n",
      "Train_EnvstepsSoFar : 336001\n",
      "Train_AverageReturn : 118.32214879344511\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 449.0242841243744\n",
      "Training Loss : 2.82463002204895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 337001\n",
      "mean reward (100 episodes) 114.413597\n",
      "best mean reward 129.391668\n",
      "running time 449.875872\n",
      "Train_EnvstepsSoFar : 337001\n",
      "Train_AverageReturn : 114.41359732145297\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 449.8758718967438\n",
      "Training Loss : 0.19098618626594543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 338001\n",
      "mean reward (100 episodes) 114.384135\n",
      "best mean reward 129.391668\n",
      "running time 450.627709\n",
      "Train_EnvstepsSoFar : 338001\n",
      "Train_AverageReturn : 114.38413517984793\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 450.62770891189575\n",
      "Training Loss : 0.21530023217201233\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 339001\n",
      "mean reward (100 episodes) 115.388933\n",
      "best mean reward 129.391668\n",
      "running time 451.842945\n",
      "Train_EnvstepsSoFar : 339001\n",
      "Train_AverageReturn : 115.3889325493807\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 451.8429448604584\n",
      "Training Loss : 0.3400403559207916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 118.528146\n",
      "best mean reward 129.391668\n",
      "running time 452.588112\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 118.52814642026364\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 452.58811211586\n",
      "Training Loss : 0.23695722222328186\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 341001\n",
      "mean reward (100 episodes) 125.316567\n",
      "best mean reward 129.391668\n",
      "running time 454.155921\n",
      "Train_EnvstepsSoFar : 341001\n",
      "Train_AverageReturn : 125.31656699997296\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 454.15592098236084\n",
      "Training Loss : 0.11505386978387833\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 342001\n",
      "mean reward (100 episodes) 119.811371\n",
      "best mean reward 129.391668\n",
      "running time 455.545479\n",
      "Train_EnvstepsSoFar : 342001\n",
      "Train_AverageReturn : 119.81137100009923\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 455.5454788208008\n",
      "Training Loss : 0.19212409853935242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 343001\n",
      "mean reward (100 episodes) 120.151745\n",
      "best mean reward 129.391668\n",
      "running time 456.352210\n",
      "Train_EnvstepsSoFar : 343001\n",
      "Train_AverageReturn : 120.15174502872215\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 456.35220980644226\n",
      "Training Loss : 0.6355031132698059\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 344001\n",
      "mean reward (100 episodes) 125.293073\n",
      "best mean reward 129.391668\n",
      "running time 457.168499\n",
      "Train_EnvstepsSoFar : 344001\n",
      "Train_AverageReturn : 125.29307333235408\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 457.16849875450134\n",
      "Training Loss : 0.6788531541824341\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 345001\n",
      "mean reward (100 episodes) 127.291347\n",
      "best mean reward 129.391668\n",
      "running time 457.938474\n",
      "Train_EnvstepsSoFar : 345001\n",
      "Train_AverageReturn : 127.29134725182753\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 457.93847393989563\n",
      "Training Loss : 1.1471213102340698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 346001\n",
      "mean reward (100 episodes) 126.954980\n",
      "best mean reward 129.391668\n",
      "running time 459.213612\n",
      "Train_EnvstepsSoFar : 346001\n",
      "Train_AverageReturn : 126.95497985231299\n",
      "Train_BestReturn : 129.39166778103817\n",
      "TimeSinceStart : 459.2136118412018\n",
      "Training Loss : 2.2759664058685303\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 347001\n",
      "mean reward (100 episodes) 139.132310\n",
      "best mean reward 139.132310\n",
      "running time 460.056958\n",
      "Train_EnvstepsSoFar : 347001\n",
      "Train_AverageReturn : 139.13231039040957\n",
      "Train_BestReturn : 139.13231039040957\n",
      "TimeSinceStart : 460.0569579601288\n",
      "Training Loss : 0.46342214941978455\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 348001\n",
      "mean reward (100 episodes) 133.721731\n",
      "best mean reward 139.132310\n",
      "running time 460.776015\n",
      "Train_EnvstepsSoFar : 348001\n",
      "Train_AverageReturn : 133.72173084563315\n",
      "Train_BestReturn : 139.13231039040957\n",
      "TimeSinceStart : 460.77601504325867\n",
      "Training Loss : 0.6269031763076782\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 349001\n",
      "mean reward (100 episodes) 134.845652\n",
      "best mean reward 139.132310\n",
      "running time 461.554402\n",
      "Train_EnvstepsSoFar : 349001\n",
      "Train_AverageReturn : 134.8456522934458\n",
      "Train_BestReturn : 139.13231039040957\n",
      "TimeSinceStart : 461.55440187454224\n",
      "Training Loss : 0.1682123988866806\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 138.597437\n",
      "best mean reward 139.132310\n",
      "running time 462.283811\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 138.59743671407807\n",
      "Train_BestReturn : 139.13231039040957\n",
      "TimeSinceStart : 462.2838110923767\n",
      "Training Loss : 0.15967318415641785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 351001\n",
      "mean reward (100 episodes) 141.052127\n",
      "best mean reward 141.052127\n",
      "running time 463.032366\n",
      "Train_EnvstepsSoFar : 351001\n",
      "Train_AverageReturn : 141.05212749779733\n",
      "Train_BestReturn : 141.05212749779733\n",
      "TimeSinceStart : 463.0323660373688\n",
      "Training Loss : 0.33969205617904663\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 352001\n",
      "mean reward (100 episodes) 137.139932\n",
      "best mean reward 141.052127\n",
      "running time 463.856104\n",
      "Train_EnvstepsSoFar : 352001\n",
      "Train_AverageReturn : 137.13993171221048\n",
      "Train_BestReturn : 141.05212749779733\n",
      "TimeSinceStart : 463.8561038970947\n",
      "Training Loss : 0.21295024454593658\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 353001\n",
      "mean reward (100 episodes) 136.339673\n",
      "best mean reward 141.052127\n",
      "running time 464.592820\n",
      "Train_EnvstepsSoFar : 353001\n",
      "Train_AverageReturn : 136.33967250778113\n",
      "Train_BestReturn : 141.05212749779733\n",
      "TimeSinceStart : 464.5928199291229\n",
      "Training Loss : 1.632224678993225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 354001\n",
      "mean reward (100 episodes) 139.422786\n",
      "best mean reward 141.052127\n",
      "running time 465.343555\n",
      "Train_EnvstepsSoFar : 354001\n",
      "Train_AverageReturn : 139.42278628458917\n",
      "Train_BestReturn : 141.05212749779733\n",
      "TimeSinceStart : 465.3435549736023\n",
      "Training Loss : 0.8962680697441101\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 355001\n",
      "mean reward (100 episodes) 137.575350\n",
      "best mean reward 141.052127\n",
      "running time 466.043941\n",
      "Train_EnvstepsSoFar : 355001\n",
      "Train_AverageReturn : 137.57534971108996\n",
      "Train_BestReturn : 141.05212749779733\n",
      "TimeSinceStart : 466.0439410209656\n",
      "Training Loss : 2.5419936180114746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 356001\n",
      "mean reward (100 episodes) 138.198719\n",
      "best mean reward 141.052127\n",
      "running time 466.788825\n",
      "Train_EnvstepsSoFar : 356001\n",
      "Train_AverageReturn : 138.19871944208327\n",
      "Train_BestReturn : 141.05212749779733\n",
      "TimeSinceStart : 466.7888250350952\n",
      "Training Loss : 0.5502265691757202\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 357001\n",
      "mean reward (100 episodes) 140.426868\n",
      "best mean reward 141.052127\n",
      "running time 467.682941\n",
      "Train_EnvstepsSoFar : 357001\n",
      "Train_AverageReturn : 140.42686821047903\n",
      "Train_BestReturn : 141.05212749779733\n",
      "TimeSinceStart : 467.6829409599304\n",
      "Training Loss : 1.388073205947876\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 358001\n",
      "mean reward (100 episodes) 145.548202\n",
      "best mean reward 145.548202\n",
      "running time 469.021481\n",
      "Train_EnvstepsSoFar : 358001\n",
      "Train_AverageReturn : 145.54820159738733\n",
      "Train_BestReturn : 145.54820159738733\n",
      "TimeSinceStart : 469.0214807987213\n",
      "Training Loss : 0.901220977306366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 359001\n",
      "mean reward (100 episodes) 146.631613\n",
      "best mean reward 146.631613\n",
      "running time 469.754983\n",
      "Train_EnvstepsSoFar : 359001\n",
      "Train_AverageReturn : 146.63161251341648\n",
      "Train_BestReturn : 146.63161251341648\n",
      "TimeSinceStart : 469.7549829483032\n",
      "Training Loss : 0.3459321856498718\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 146.876917\n",
      "best mean reward 146.876917\n",
      "running time 470.594260\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 146.87691748575136\n",
      "Train_BestReturn : 146.87691748575136\n",
      "TimeSinceStart : 470.5942599773407\n",
      "Training Loss : 0.2509920299053192\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 361001\n",
      "mean reward (100 episodes) 143.175657\n",
      "best mean reward 146.876917\n",
      "running time 471.390787\n",
      "Train_EnvstepsSoFar : 361001\n",
      "Train_AverageReturn : 143.17565730155968\n",
      "Train_BestReturn : 146.87691748575136\n",
      "TimeSinceStart : 471.3907868862152\n",
      "Training Loss : 0.5744308233261108\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 362001\n",
      "mean reward (100 episodes) 141.321396\n",
      "best mean reward 146.876917\n",
      "running time 472.376934\n",
      "Train_EnvstepsSoFar : 362001\n",
      "Train_AverageReturn : 141.32139593663035\n",
      "Train_BestReturn : 146.87691748575136\n",
      "TimeSinceStart : 472.3769340515137\n",
      "Training Loss : 1.0744441747665405\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 363001\n",
      "mean reward (100 episodes) 151.266189\n",
      "best mean reward 151.266189\n",
      "running time 473.457603\n",
      "Train_EnvstepsSoFar : 363001\n",
      "Train_AverageReturn : 151.26618897197125\n",
      "Train_BestReturn : 151.26618897197125\n",
      "TimeSinceStart : 473.4576029777527\n",
      "Training Loss : 0.170798659324646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 364001\n",
      "mean reward (100 episodes) 153.187434\n",
      "best mean reward 153.187434\n",
      "running time 474.435285\n",
      "Train_EnvstepsSoFar : 364001\n",
      "Train_AverageReturn : 153.18743397866862\n",
      "Train_BestReturn : 153.18743397866862\n",
      "TimeSinceStart : 474.43528485298157\n",
      "Training Loss : 7.417057991027832\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 365001\n",
      "mean reward (100 episodes) 157.621495\n",
      "best mean reward 157.621495\n",
      "running time 475.311539\n",
      "Train_EnvstepsSoFar : 365001\n",
      "Train_AverageReturn : 157.62149484174063\n",
      "Train_BestReturn : 157.62149484174063\n",
      "TimeSinceStart : 475.31153893470764\n",
      "Training Loss : 0.25245150923728943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 366001\n",
      "mean reward (100 episodes) 157.305481\n",
      "best mean reward 157.621495\n",
      "running time 476.270248\n",
      "Train_EnvstepsSoFar : 366001\n",
      "Train_AverageReturn : 157.30548085535426\n",
      "Train_BestReturn : 157.62149484174063\n",
      "TimeSinceStart : 476.2702479362488\n",
      "Training Loss : 3.1598587036132812\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 367001\n",
      "mean reward (100 episodes) 155.037485\n",
      "best mean reward 157.621495\n",
      "running time 477.190625\n",
      "Train_EnvstepsSoFar : 367001\n",
      "Train_AverageReturn : 155.03748506050914\n",
      "Train_BestReturn : 157.62149484174063\n",
      "TimeSinceStart : 477.1906249523163\n",
      "Training Loss : 1.4016169309616089\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 368001\n",
      "mean reward (100 episodes) 158.049323\n",
      "best mean reward 158.049323\n",
      "running time 477.998232\n",
      "Train_EnvstepsSoFar : 368001\n",
      "Train_AverageReturn : 158.04932272060503\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 477.9982318878174\n",
      "Training Loss : 1.2371963262557983\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 369001\n",
      "mean reward (100 episodes) 152.928918\n",
      "best mean reward 158.049323\n",
      "running time 479.259195\n",
      "Train_EnvstepsSoFar : 369001\n",
      "Train_AverageReturn : 152.92891844539798\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 479.2591950893402\n",
      "Training Loss : 0.2753894031047821\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 151.560368\n",
      "best mean reward 158.049323\n",
      "running time 480.239765\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 151.5603676001264\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 480.23976492881775\n",
      "Training Loss : 0.11964049935340881\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 371001\n",
      "mean reward (100 episodes) 152.684699\n",
      "best mean reward 158.049323\n",
      "running time 480.986614\n",
      "Train_EnvstepsSoFar : 371001\n",
      "Train_AverageReturn : 152.6846988026138\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 480.98661398887634\n",
      "Training Loss : 0.19006334245204926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 372001\n",
      "mean reward (100 episodes) 155.212511\n",
      "best mean reward 158.049323\n",
      "running time 481.809208\n",
      "Train_EnvstepsSoFar : 372001\n",
      "Train_AverageReturn : 155.21251097539846\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 481.80920791625977\n",
      "Training Loss : 2.6759393215179443\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 373001\n",
      "mean reward (100 episodes) 152.709084\n",
      "best mean reward 158.049323\n",
      "running time 482.609512\n",
      "Train_EnvstepsSoFar : 373001\n",
      "Train_AverageReturn : 152.7090842709292\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 482.6095118522644\n",
      "Training Loss : 0.4057331383228302\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 374001\n",
      "mean reward (100 episodes) 154.398194\n",
      "best mean reward 158.049323\n",
      "running time 483.377502\n",
      "Train_EnvstepsSoFar : 374001\n",
      "Train_AverageReturn : 154.39819372605342\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 483.3775019645691\n",
      "Training Loss : 1.1941510438919067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 375001\n",
      "mean reward (100 episodes) 151.723633\n",
      "best mean reward 158.049323\n",
      "running time 484.257918\n",
      "Train_EnvstepsSoFar : 375001\n",
      "Train_AverageReturn : 151.72363295970985\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 484.25791788101196\n",
      "Training Loss : 0.3762977123260498\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 376001\n",
      "mean reward (100 episodes) 152.337394\n",
      "best mean reward 158.049323\n",
      "running time 485.154471\n",
      "Train_EnvstepsSoFar : 376001\n",
      "Train_AverageReturn : 152.33739362521976\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 485.15447092056274\n",
      "Training Loss : 0.6922998428344727\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 377001\n",
      "mean reward (100 episodes) 141.314513\n",
      "best mean reward 158.049323\n",
      "running time 486.000639\n",
      "Train_EnvstepsSoFar : 377001\n",
      "Train_AverageReturn : 141.31451322760265\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 486.000638961792\n",
      "Training Loss : 1.7110786437988281\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 378001\n",
      "mean reward (100 episodes) 145.647688\n",
      "best mean reward 158.049323\n",
      "running time 486.752423\n",
      "Train_EnvstepsSoFar : 378001\n",
      "Train_AverageReturn : 145.6476879269483\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 486.75242280960083\n",
      "Training Loss : 1.3877925872802734\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 379001\n",
      "mean reward (100 episodes) 143.182991\n",
      "best mean reward 158.049323\n",
      "running time 487.526829\n",
      "Train_EnvstepsSoFar : 379001\n",
      "Train_AverageReturn : 143.18299127615057\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 487.5268290042877\n",
      "Training Loss : 1.9513787031173706\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 138.153872\n",
      "best mean reward 158.049323\n",
      "running time 488.346305\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 138.15387216311336\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 488.34630489349365\n",
      "Training Loss : 1.3455846309661865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 381001\n",
      "mean reward (100 episodes) 140.672082\n",
      "best mean reward 158.049323\n",
      "running time 489.250606\n",
      "Train_EnvstepsSoFar : 381001\n",
      "Train_AverageReturn : 140.67208173449094\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 489.2506060600281\n",
      "Training Loss : 3.1658406257629395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 382001\n",
      "mean reward (100 episodes) 141.262821\n",
      "best mean reward 158.049323\n",
      "running time 490.194222\n",
      "Train_EnvstepsSoFar : 382001\n",
      "Train_AverageReturn : 141.26282082775722\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 490.1942219734192\n",
      "Training Loss : 0.17657041549682617\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 383001\n",
      "mean reward (100 episodes) 141.355833\n",
      "best mean reward 158.049323\n",
      "running time 490.941427\n",
      "Train_EnvstepsSoFar : 383001\n",
      "Train_AverageReturn : 141.3558329926362\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 490.9414267539978\n",
      "Training Loss : 0.6348947286605835\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 384001\n",
      "mean reward (100 episodes) 143.039354\n",
      "best mean reward 158.049323\n",
      "running time 491.730195\n",
      "Train_EnvstepsSoFar : 384001\n",
      "Train_AverageReturn : 143.03935364896213\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 491.7301948070526\n",
      "Training Loss : 2.0651791095733643\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 385001\n",
      "mean reward (100 episodes) 146.838887\n",
      "best mean reward 158.049323\n",
      "running time 492.459424\n",
      "Train_EnvstepsSoFar : 385001\n",
      "Train_AverageReturn : 146.83888733895787\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 492.45942401885986\n",
      "Training Loss : 1.2435369491577148\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 386001\n",
      "mean reward (100 episodes) 146.378674\n",
      "best mean reward 158.049323\n",
      "running time 493.325581\n",
      "Train_EnvstepsSoFar : 386001\n",
      "Train_AverageReturn : 146.37867402495073\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 493.325581073761\n",
      "Training Loss : 1.8962457180023193\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 387001\n",
      "mean reward (100 episodes) 147.985562\n",
      "best mean reward 158.049323\n",
      "running time 494.201712\n",
      "Train_EnvstepsSoFar : 387001\n",
      "Train_AverageReturn : 147.98556174577254\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 494.20171213150024\n",
      "Training Loss : 4.296258926391602\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 388001\n",
      "mean reward (100 episodes) 142.940060\n",
      "best mean reward 158.049323\n",
      "running time 494.982835\n",
      "Train_EnvstepsSoFar : 388001\n",
      "Train_AverageReturn : 142.94006037286792\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 494.982834815979\n",
      "Training Loss : 1.1761066913604736\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 389001\n",
      "mean reward (100 episodes) 144.943207\n",
      "best mean reward 158.049323\n",
      "running time 495.766054\n",
      "Train_EnvstepsSoFar : 389001\n",
      "Train_AverageReturn : 144.94320734926814\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 495.7660541534424\n",
      "Training Loss : 1.0942773818969727\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 145.985859\n",
      "best mean reward 158.049323\n",
      "running time 496.575592\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 145.98585936734477\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 496.57559180259705\n",
      "Training Loss : 0.6460267901420593\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 391001\n",
      "mean reward (100 episodes) 144.512572\n",
      "best mean reward 158.049323\n",
      "running time 497.354065\n",
      "Train_EnvstepsSoFar : 391001\n",
      "Train_AverageReturn : 144.5125722045868\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 497.35406517982483\n",
      "Training Loss : 0.6443471908569336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 392001\n",
      "mean reward (100 episodes) 156.324146\n",
      "best mean reward 158.049323\n",
      "running time 498.110562\n",
      "Train_EnvstepsSoFar : 392001\n",
      "Train_AverageReturn : 156.32414609790735\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 498.11056208610535\n",
      "Training Loss : 0.2978110611438751\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 393001\n",
      "mean reward (100 episodes) 151.375713\n",
      "best mean reward 158.049323\n",
      "running time 498.853902\n",
      "Train_EnvstepsSoFar : 393001\n",
      "Train_AverageReturn : 151.37571294897268\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 498.8539021015167\n",
      "Training Loss : 0.31860727071762085\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 394001\n",
      "mean reward (100 episodes) 151.819512\n",
      "best mean reward 158.049323\n",
      "running time 499.630319\n",
      "Train_EnvstepsSoFar : 394001\n",
      "Train_AverageReturn : 151.819511757236\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 499.63031911849976\n",
      "Training Loss : 0.2256803810596466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 395001\n",
      "mean reward (100 episodes) 150.205335\n",
      "best mean reward 158.049323\n",
      "running time 500.422156\n",
      "Train_EnvstepsSoFar : 395001\n",
      "Train_AverageReturn : 150.20533503701833\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 500.4221558570862\n",
      "Training Loss : 0.2636113166809082\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 396001\n",
      "mean reward (100 episodes) 149.906368\n",
      "best mean reward 158.049323\n",
      "running time 501.261056\n",
      "Train_EnvstepsSoFar : 396001\n",
      "Train_AverageReturn : 149.90636755384824\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 501.2610559463501\n",
      "Training Loss : 0.43147507309913635\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 397001\n",
      "mean reward (100 episodes) 147.005031\n",
      "best mean reward 158.049323\n",
      "running time 502.039683\n",
      "Train_EnvstepsSoFar : 397001\n",
      "Train_AverageReturn : 147.00503122142266\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 502.0396831035614\n",
      "Training Loss : 0.3735875189304352\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 398001\n",
      "mean reward (100 episodes) 149.416744\n",
      "best mean reward 158.049323\n",
      "running time 502.824590\n",
      "Train_EnvstepsSoFar : 398001\n",
      "Train_AverageReturn : 149.41674365147017\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 502.82458996772766\n",
      "Training Loss : 0.43078550696372986\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 399001\n",
      "mean reward (100 episodes) 153.493448\n",
      "best mean reward 158.049323\n",
      "running time 503.657923\n",
      "Train_EnvstepsSoFar : 399001\n",
      "Train_AverageReturn : 153.49344789049178\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 503.65792298316956\n",
      "Training Loss : 0.28447386622428894\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 149.954190\n",
      "best mean reward 158.049323\n",
      "running time 504.477096\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 149.95418962599794\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 504.47709584236145\n",
      "Training Loss : 1.679511547088623\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 401001\n",
      "mean reward (100 episodes) 148.267992\n",
      "best mean reward 158.049323\n",
      "running time 505.335909\n",
      "Train_EnvstepsSoFar : 401001\n",
      "Train_AverageReturn : 148.26799158938044\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 505.3359091281891\n",
      "Training Loss : 4.869449615478516\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 402001\n",
      "mean reward (100 episodes) 151.279527\n",
      "best mean reward 158.049323\n",
      "running time 506.150712\n",
      "Train_EnvstepsSoFar : 402001\n",
      "Train_AverageReturn : 151.27952686204486\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 506.15071201324463\n",
      "Training Loss : 1.153193473815918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 403001\n",
      "mean reward (100 episodes) 156.503642\n",
      "best mean reward 158.049323\n",
      "running time 506.986953\n",
      "Train_EnvstepsSoFar : 403001\n",
      "Train_AverageReturn : 156.50364196368778\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 506.98695278167725\n",
      "Training Loss : 0.13310059905052185\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 404001\n",
      "mean reward (100 episodes) 153.970584\n",
      "best mean reward 158.049323\n",
      "running time 507.769010\n",
      "Train_EnvstepsSoFar : 404001\n",
      "Train_AverageReturn : 153.97058384276193\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 507.7690098285675\n",
      "Training Loss : 1.2205123901367188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 405001\n",
      "mean reward (100 episodes) 154.684896\n",
      "best mean reward 158.049323\n",
      "running time 508.610339\n",
      "Train_EnvstepsSoFar : 405001\n",
      "Train_AverageReturn : 154.68489587869087\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 508.6103389263153\n",
      "Training Loss : 4.915700435638428\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 406001\n",
      "mean reward (100 episodes) 149.604179\n",
      "best mean reward 158.049323\n",
      "running time 509.802078\n",
      "Train_EnvstepsSoFar : 406001\n",
      "Train_AverageReturn : 149.60417889531766\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 509.80207777023315\n",
      "Training Loss : 7.4626545906066895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 407001\n",
      "mean reward (100 episodes) 142.629119\n",
      "best mean reward 158.049323\n",
      "running time 510.655998\n",
      "Train_EnvstepsSoFar : 407001\n",
      "Train_AverageReturn : 142.6291191704225\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 510.6559979915619\n",
      "Training Loss : 1.605708122253418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 408001\n",
      "mean reward (100 episodes) 145.750584\n",
      "best mean reward 158.049323\n",
      "running time 511.465392\n",
      "Train_EnvstepsSoFar : 408001\n",
      "Train_AverageReturn : 145.75058415225587\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 511.46539211273193\n",
      "Training Loss : 1.4559996128082275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 409001\n",
      "mean reward (100 episodes) 141.641475\n",
      "best mean reward 158.049323\n",
      "running time 512.544656\n",
      "Train_EnvstepsSoFar : 409001\n",
      "Train_AverageReturn : 141.6414747645807\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 512.5446560382843\n",
      "Training Loss : 1.2261308431625366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 141.560450\n",
      "best mean reward 158.049323\n",
      "running time 513.412892\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 141.5604502706268\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 513.4128921031952\n",
      "Training Loss : 1.7124980688095093\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 411001\n",
      "mean reward (100 episodes) 138.208624\n",
      "best mean reward 158.049323\n",
      "running time 514.151055\n",
      "Train_EnvstepsSoFar : 411001\n",
      "Train_AverageReturn : 138.20862371662793\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 514.15105509758\n",
      "Training Loss : 1.0460063219070435\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 412001\n",
      "mean reward (100 episodes) 131.472884\n",
      "best mean reward 158.049323\n",
      "running time 514.946307\n",
      "Train_EnvstepsSoFar : 412001\n",
      "Train_AverageReturn : 131.47288363032465\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 514.9463069438934\n",
      "Training Loss : 0.10599660128355026\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 413001\n",
      "mean reward (100 episodes) 123.811894\n",
      "best mean reward 158.049323\n",
      "running time 515.710541\n",
      "Train_EnvstepsSoFar : 413001\n",
      "Train_AverageReturn : 123.81189418904509\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 515.710541009903\n",
      "Training Loss : 0.1599818468093872\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 414001\n",
      "mean reward (100 episodes) 121.353133\n",
      "best mean reward 158.049323\n",
      "running time 516.452268\n",
      "Train_EnvstepsSoFar : 414001\n",
      "Train_AverageReturn : 121.35313320448346\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 516.4522678852081\n",
      "Training Loss : 0.1469782292842865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 415001\n",
      "mean reward (100 episodes) 114.453788\n",
      "best mean reward 158.049323\n",
      "running time 517.183513\n",
      "Train_EnvstepsSoFar : 415001\n",
      "Train_AverageReturn : 114.45378785779272\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 517.1835129261017\n",
      "Training Loss : 5.495162010192871\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 416001\n",
      "mean reward (100 episodes) 114.183241\n",
      "best mean reward 158.049323\n",
      "running time 518.029036\n",
      "Train_EnvstepsSoFar : 416001\n",
      "Train_AverageReturn : 114.18324115760106\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 518.0290360450745\n",
      "Training Loss : 0.2790660858154297\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 417001\n",
      "mean reward (100 episodes) 110.005125\n",
      "best mean reward 158.049323\n",
      "running time 518.895285\n",
      "Train_EnvstepsSoFar : 417001\n",
      "Train_AverageReturn : 110.00512492699299\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 518.8952848911285\n",
      "Training Loss : 0.23514485359191895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 418001\n",
      "mean reward (100 episodes) 113.188070\n",
      "best mean reward 158.049323\n",
      "running time 519.859293\n",
      "Train_EnvstepsSoFar : 418001\n",
      "Train_AverageReturn : 113.1880700317844\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 519.8592927455902\n",
      "Training Loss : 0.27887386083602905\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 419001\n",
      "mean reward (100 episodes) 114.004291\n",
      "best mean reward 158.049323\n",
      "running time 520.687687\n",
      "Train_EnvstepsSoFar : 419001\n",
      "Train_AverageReturn : 114.00429108430164\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 520.687686920166\n",
      "Training Loss : 1.6925952434539795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 113.297253\n",
      "best mean reward 158.049323\n",
      "running time 521.492120\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 113.29725286746907\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 521.4921200275421\n",
      "Training Loss : 0.20406965911388397\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 421001\n",
      "mean reward (100 episodes) 113.452762\n",
      "best mean reward 158.049323\n",
      "running time 522.255629\n",
      "Train_EnvstepsSoFar : 421001\n",
      "Train_AverageReturn : 113.45276174315384\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 522.2556290626526\n",
      "Training Loss : 3.328223705291748\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 422001\n",
      "mean reward (100 episodes) 109.289544\n",
      "best mean reward 158.049323\n",
      "running time 523.032685\n",
      "Train_EnvstepsSoFar : 422001\n",
      "Train_AverageReturn : 109.28954388074848\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 523.0326850414276\n",
      "Training Loss : 0.1610632985830307\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 423001\n",
      "mean reward (100 episodes) 113.103119\n",
      "best mean reward 158.049323\n",
      "running time 523.920788\n",
      "Train_EnvstepsSoFar : 423001\n",
      "Train_AverageReturn : 113.10311937010621\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 523.9207878112793\n",
      "Training Loss : 2.2613685131073\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 424001\n",
      "mean reward (100 episodes) 107.787475\n",
      "best mean reward 158.049323\n",
      "running time 524.697239\n",
      "Train_EnvstepsSoFar : 424001\n",
      "Train_AverageReturn : 107.78747460593053\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 524.6972389221191\n",
      "Training Loss : 2.501478672027588\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 425001\n",
      "mean reward (100 episodes) 108.268091\n",
      "best mean reward 158.049323\n",
      "running time 525.460181\n",
      "Train_EnvstepsSoFar : 425001\n",
      "Train_AverageReturn : 108.26809119827456\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 525.4601809978485\n",
      "Training Loss : 0.20650601387023926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 426001\n",
      "mean reward (100 episodes) 109.885460\n",
      "best mean reward 158.049323\n",
      "running time 526.184240\n",
      "Train_EnvstepsSoFar : 426001\n",
      "Train_AverageReturn : 109.88546006780587\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 526.184240102768\n",
      "Training Loss : 1.541426181793213\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 427001\n",
      "mean reward (100 episodes) 116.696919\n",
      "best mean reward 158.049323\n",
      "running time 526.978503\n",
      "Train_EnvstepsSoFar : 427001\n",
      "Train_AverageReturn : 116.6969193309404\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 526.9785027503967\n",
      "Training Loss : 0.18625502288341522\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 428001\n",
      "mean reward (100 episodes) 125.894369\n",
      "best mean reward 158.049323\n",
      "running time 527.717032\n",
      "Train_EnvstepsSoFar : 428001\n",
      "Train_AverageReturn : 125.89436874609645\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 527.717031955719\n",
      "Training Loss : 0.18500423431396484\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 429001\n",
      "mean reward (100 episodes) 126.292352\n",
      "best mean reward 158.049323\n",
      "running time 528.458524\n",
      "Train_EnvstepsSoFar : 429001\n",
      "Train_AverageReturn : 126.29235248434762\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 528.4585239887238\n",
      "Training Loss : 0.5394008755683899\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 122.729654\n",
      "best mean reward 158.049323\n",
      "running time 529.224631\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 122.72965446836534\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 529.2246310710907\n",
      "Training Loss : 0.6228386759757996\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 431001\n",
      "mean reward (100 episodes) 124.984637\n",
      "best mean reward 158.049323\n",
      "running time 529.963808\n",
      "Train_EnvstepsSoFar : 431001\n",
      "Train_AverageReturn : 124.98463721315602\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 529.9638078212738\n",
      "Training Loss : 0.17749415338039398\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 432001\n",
      "mean reward (100 episodes) 125.697130\n",
      "best mean reward 158.049323\n",
      "running time 530.700667\n",
      "Train_EnvstepsSoFar : 432001\n",
      "Train_AverageReturn : 125.69713035243339\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 530.700667142868\n",
      "Training Loss : 0.38919907808303833\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 433001\n",
      "mean reward (100 episodes) 123.351333\n",
      "best mean reward 158.049323\n",
      "running time 531.486871\n",
      "Train_EnvstepsSoFar : 433001\n",
      "Train_AverageReturn : 123.35133334369684\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 531.4868710041046\n",
      "Training Loss : 0.2570524215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 434001\n",
      "mean reward (100 episodes) 117.082942\n",
      "best mean reward 158.049323\n",
      "running time 532.200165\n",
      "Train_EnvstepsSoFar : 434001\n",
      "Train_AverageReturn : 117.08294158508431\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 532.2001650333405\n",
      "Training Loss : 5.081097602844238\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 435001\n",
      "mean reward (100 episodes) 110.312058\n",
      "best mean reward 158.049323\n",
      "running time 532.926822\n",
      "Train_EnvstepsSoFar : 435001\n",
      "Train_AverageReturn : 110.31205832258345\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 532.9268219470978\n",
      "Training Loss : 1.0639082193374634\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 436001\n",
      "mean reward (100 episodes) 113.334394\n",
      "best mean reward 158.049323\n",
      "running time 534.668059\n",
      "Train_EnvstepsSoFar : 436001\n",
      "Train_AverageReturn : 113.33439382832232\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 534.6680591106415\n",
      "Training Loss : 0.11970467865467072\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 437001\n",
      "mean reward (100 episodes) 125.150987\n",
      "best mean reward 158.049323\n",
      "running time 535.527000\n",
      "Train_EnvstepsSoFar : 437001\n",
      "Train_AverageReturn : 125.15098654840857\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 535.5269999504089\n",
      "Training Loss : 0.9438473582267761\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 438001\n",
      "mean reward (100 episodes) 122.428874\n",
      "best mean reward 158.049323\n",
      "running time 536.368703\n",
      "Train_EnvstepsSoFar : 438001\n",
      "Train_AverageReturn : 122.42887354627614\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 536.3687028884888\n",
      "Training Loss : 0.2960681915283203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 439001\n",
      "mean reward (100 episodes) 125.251374\n",
      "best mean reward 158.049323\n",
      "running time 537.191309\n",
      "Train_EnvstepsSoFar : 439001\n",
      "Train_AverageReturn : 125.25137439509419\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 537.1913089752197\n",
      "Training Loss : 2.449282646179199\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 120.010854\n",
      "best mean reward 158.049323\n",
      "running time 538.323532\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 120.01085392549129\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 538.3235318660736\n",
      "Training Loss : 0.862661600112915\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 441001\n",
      "mean reward (100 episodes) 120.789403\n",
      "best mean reward 158.049323\n",
      "running time 539.112096\n",
      "Train_EnvstepsSoFar : 441001\n",
      "Train_AverageReturn : 120.78940324569854\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 539.1120958328247\n",
      "Training Loss : 0.6786375641822815\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 442001\n",
      "mean reward (100 episodes) 117.109527\n",
      "best mean reward 158.049323\n",
      "running time 539.879208\n",
      "Train_EnvstepsSoFar : 442001\n",
      "Train_AverageReturn : 117.10952730242006\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 539.8792078495026\n",
      "Training Loss : 0.19576114416122437\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 443001\n",
      "mean reward (100 episodes) 113.357854\n",
      "best mean reward 158.049323\n",
      "running time 540.659397\n",
      "Train_EnvstepsSoFar : 443001\n",
      "Train_AverageReturn : 113.3578537975811\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 540.6593971252441\n",
      "Training Loss : 0.19314776360988617\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 444001\n",
      "mean reward (100 episodes) 113.149543\n",
      "best mean reward 158.049323\n",
      "running time 541.445919\n",
      "Train_EnvstepsSoFar : 444001\n",
      "Train_AverageReturn : 113.14954314449196\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 541.4459187984467\n",
      "Training Loss : 0.5935670137405396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 445001\n",
      "mean reward (100 episodes) 112.974102\n",
      "best mean reward 158.049323\n",
      "running time 542.203964\n",
      "Train_EnvstepsSoFar : 445001\n",
      "Train_AverageReturn : 112.97410207017396\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 542.2039637565613\n",
      "Training Loss : 0.7020387649536133\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 446001\n",
      "mean reward (100 episodes) 112.211048\n",
      "best mean reward 158.049323\n",
      "running time 542.966921\n",
      "Train_EnvstepsSoFar : 446001\n",
      "Train_AverageReturn : 112.2110479337576\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 542.9669208526611\n",
      "Training Loss : 0.8166576027870178\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 447001\n",
      "mean reward (100 episodes) 101.621443\n",
      "best mean reward 158.049323\n",
      "running time 543.799497\n",
      "Train_EnvstepsSoFar : 447001\n",
      "Train_AverageReturn : 101.62144302492472\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 543.7994968891144\n",
      "Training Loss : 1.2795976400375366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 448001\n",
      "mean reward (100 episodes) 98.505947\n",
      "best mean reward 158.049323\n",
      "running time 544.642093\n",
      "Train_EnvstepsSoFar : 448001\n",
      "Train_AverageReturn : 98.5059468937004\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 544.6420929431915\n",
      "Training Loss : 0.15770435333251953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 449001\n",
      "mean reward (100 episodes) 94.316602\n",
      "best mean reward 158.049323\n",
      "running time 545.381326\n",
      "Train_EnvstepsSoFar : 449001\n",
      "Train_AverageReturn : 94.31660197288103\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 545.3813259601593\n",
      "Training Loss : 0.3898940086364746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 80.592666\n",
      "best mean reward 158.049323\n",
      "running time 546.091444\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 80.59266586408299\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 546.0914437770844\n",
      "Training Loss : 0.5067010521888733\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 451001\n",
      "mean reward (100 episodes) 70.437001\n",
      "best mean reward 158.049323\n",
      "running time 546.818444\n",
      "Train_EnvstepsSoFar : 451001\n",
      "Train_AverageReturn : 70.4370014098898\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 546.8184440135956\n",
      "Training Loss : 0.6050846576690674\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 452001\n",
      "mean reward (100 episodes) 68.516874\n",
      "best mean reward 158.049323\n",
      "running time 547.536119\n",
      "Train_EnvstepsSoFar : 452001\n",
      "Train_AverageReturn : 68.51687378343082\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 547.5361189842224\n",
      "Training Loss : 5.522423267364502\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 453001\n",
      "mean reward (100 episodes) 60.772632\n",
      "best mean reward 158.049323\n",
      "running time 548.264392\n",
      "Train_EnvstepsSoFar : 453001\n",
      "Train_AverageReturn : 60.772631564169444\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 548.2643918991089\n",
      "Training Loss : 0.26401105523109436\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 454001\n",
      "mean reward (100 episodes) 51.040120\n",
      "best mean reward 158.049323\n",
      "running time 548.991929\n",
      "Train_EnvstepsSoFar : 454001\n",
      "Train_AverageReturn : 51.0401198194026\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 548.9919290542603\n",
      "Training Loss : 1.5690370798110962\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 455001\n",
      "mean reward (100 episodes) 43.026543\n",
      "best mean reward 158.049323\n",
      "running time 549.766327\n",
      "Train_EnvstepsSoFar : 455001\n",
      "Train_AverageReturn : 43.02654251473186\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 549.7663269042969\n",
      "Training Loss : 0.26076969504356384\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 456001\n",
      "mean reward (100 episodes) 44.074668\n",
      "best mean reward 158.049323\n",
      "running time 550.585040\n",
      "Train_EnvstepsSoFar : 456001\n",
      "Train_AverageReturn : 44.07466768080244\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 550.5850400924683\n",
      "Training Loss : 3.733264923095703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 457001\n",
      "mean reward (100 episodes) 50.016868\n",
      "best mean reward 158.049323\n",
      "running time 551.345323\n",
      "Train_EnvstepsSoFar : 457001\n",
      "Train_AverageReturn : 50.01686814316586\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 551.3453228473663\n",
      "Training Loss : 0.8118863105773926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 458001\n",
      "mean reward (100 episodes) 41.048462\n",
      "best mean reward 158.049323\n",
      "running time 552.109471\n",
      "Train_EnvstepsSoFar : 458001\n",
      "Train_AverageReturn : 41.04846194370628\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 552.1094708442688\n",
      "Training Loss : 1.928227424621582\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 459001\n",
      "mean reward (100 episodes) 44.320966\n",
      "best mean reward 158.049323\n",
      "running time 552.829905\n",
      "Train_EnvstepsSoFar : 459001\n",
      "Train_AverageReturn : 44.320966257547035\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 552.8299050331116\n",
      "Training Loss : 0.1459687054157257\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 45.205543\n",
      "best mean reward 158.049323\n",
      "running time 553.608912\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 45.205543342862384\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 553.6089119911194\n",
      "Training Loss : 0.16132189333438873\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 461001\n",
      "mean reward (100 episodes) 40.998683\n",
      "best mean reward 158.049323\n",
      "running time 554.594373\n",
      "Train_EnvstepsSoFar : 461001\n",
      "Train_AverageReturn : 40.99868340488351\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 554.5943729877472\n",
      "Training Loss : 0.2076391577720642\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 462001\n",
      "mean reward (100 episodes) 40.607265\n",
      "best mean reward 158.049323\n",
      "running time 555.386250\n",
      "Train_EnvstepsSoFar : 462001\n",
      "Train_AverageReturn : 40.607264786153316\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 555.3862500190735\n",
      "Training Loss : 0.840536892414093\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 463001\n",
      "mean reward (100 episodes) 45.348837\n",
      "best mean reward 158.049323\n",
      "running time 556.128335\n",
      "Train_EnvstepsSoFar : 463001\n",
      "Train_AverageReturn : 45.34883718481555\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 556.1283349990845\n",
      "Training Loss : 0.28950008749961853\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 464001\n",
      "mean reward (100 episodes) 39.394407\n",
      "best mean reward 158.049323\n",
      "running time 556.882529\n",
      "Train_EnvstepsSoFar : 464001\n",
      "Train_AverageReturn : 39.3944069222519\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 556.8825290203094\n",
      "Training Loss : 0.25538671016693115\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 465001\n",
      "mean reward (100 episodes) 45.308776\n",
      "best mean reward 158.049323\n",
      "running time 557.659789\n",
      "Train_EnvstepsSoFar : 465001\n",
      "Train_AverageReturn : 45.30877604814217\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 557.6597890853882\n",
      "Training Loss : 0.21394041180610657\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 466001\n",
      "mean reward (100 episodes) 43.028234\n",
      "best mean reward 158.049323\n",
      "running time 558.391281\n",
      "Train_EnvstepsSoFar : 466001\n",
      "Train_AverageReturn : 43.02823438389162\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 558.3912811279297\n",
      "Training Loss : 0.2109968364238739\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 467001\n",
      "mean reward (100 episodes) 48.558215\n",
      "best mean reward 158.049323\n",
      "running time 559.275532\n",
      "Train_EnvstepsSoFar : 467001\n",
      "Train_AverageReturn : 48.55821453661782\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 559.2755320072174\n",
      "Training Loss : 1.408576250076294\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 468001\n",
      "mean reward (100 episodes) 43.754740\n",
      "best mean reward 158.049323\n",
      "running time 560.198734\n",
      "Train_EnvstepsSoFar : 468001\n",
      "Train_AverageReturn : 43.75474049526314\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 560.1987340450287\n",
      "Training Loss : 0.4391469955444336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 469001\n",
      "mean reward (100 episodes) 41.993285\n",
      "best mean reward 158.049323\n",
      "running time 561.037502\n",
      "Train_EnvstepsSoFar : 469001\n",
      "Train_AverageReturn : 41.9932850167132\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 561.0375018119812\n",
      "Training Loss : 1.6074187755584717\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 29.328739\n",
      "best mean reward 158.049323\n",
      "running time 561.802659\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 29.328739233080295\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 561.802659034729\n",
      "Training Loss : 0.9667035341262817\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 471001\n",
      "mean reward (100 episodes) 34.209393\n",
      "best mean reward 158.049323\n",
      "running time 562.548686\n",
      "Train_EnvstepsSoFar : 471001\n",
      "Train_AverageReturn : 34.209392888789864\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 562.5486860275269\n",
      "Training Loss : 8.516818046569824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 472001\n",
      "mean reward (100 episodes) 21.268233\n",
      "best mean reward 158.049323\n",
      "running time 563.334252\n",
      "Train_EnvstepsSoFar : 472001\n",
      "Train_AverageReturn : 21.268233264185668\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 563.3342518806458\n",
      "Training Loss : 0.13836534321308136\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 473001\n",
      "mean reward (100 episodes) 32.175508\n",
      "best mean reward 158.049323\n",
      "running time 564.213666\n",
      "Train_EnvstepsSoFar : 473001\n",
      "Train_AverageReturn : 32.17550813057938\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 564.2136659622192\n",
      "Training Loss : 0.20366500318050385\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 474001\n",
      "mean reward (100 episodes) 42.705061\n",
      "best mean reward 158.049323\n",
      "running time 565.048488\n",
      "Train_EnvstepsSoFar : 474001\n",
      "Train_AverageReturn : 42.70506146186328\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 565.0484879016876\n",
      "Training Loss : 0.10525862127542496\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 475001\n",
      "mean reward (100 episodes) 51.578036\n",
      "best mean reward 158.049323\n",
      "running time 565.837587\n",
      "Train_EnvstepsSoFar : 475001\n",
      "Train_AverageReturn : 51.57803563574004\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 565.8375868797302\n",
      "Training Loss : 0.3983834981918335\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 476001\n",
      "mean reward (100 episodes) 60.537817\n",
      "best mean reward 158.049323\n",
      "running time 566.619814\n",
      "Train_EnvstepsSoFar : 476001\n",
      "Train_AverageReturn : 60.53781726501361\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 566.619814157486\n",
      "Training Loss : 0.12827622890472412\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 477001\n",
      "mean reward (100 episodes) 65.781920\n",
      "best mean reward 158.049323\n",
      "running time 567.341259\n",
      "Train_EnvstepsSoFar : 477001\n",
      "Train_AverageReturn : 65.78192036472306\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 567.3412590026855\n",
      "Training Loss : 0.21723993122577667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 478001\n",
      "mean reward (100 episodes) 64.466428\n",
      "best mean reward 158.049323\n",
      "running time 568.081026\n",
      "Train_EnvstepsSoFar : 478001\n",
      "Train_AverageReturn : 64.4664276007876\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 568.0810260772705\n",
      "Training Loss : 2.8160884380340576\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 479001\n",
      "mean reward (100 episodes) 59.329660\n",
      "best mean reward 158.049323\n",
      "running time 568.793914\n",
      "Train_EnvstepsSoFar : 479001\n",
      "Train_AverageReturn : 59.32966035684996\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 568.7939140796661\n",
      "Training Loss : 9.372964859008789\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 65.124459\n",
      "best mean reward 158.049323\n",
      "running time 569.640547\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 65.1244588151095\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 569.6405470371246\n",
      "Training Loss : 0.4327288568019867\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 481001\n",
      "mean reward (100 episodes) 72.446383\n",
      "best mean reward 158.049323\n",
      "running time 570.400228\n",
      "Train_EnvstepsSoFar : 481001\n",
      "Train_AverageReturn : 72.44638308886468\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 570.400228023529\n",
      "Training Loss : 0.42059850692749023\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 482001\n",
      "mean reward (100 episodes) 66.881233\n",
      "best mean reward 158.049323\n",
      "running time 571.141819\n",
      "Train_EnvstepsSoFar : 482001\n",
      "Train_AverageReturn : 66.88123338129378\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 571.1418190002441\n",
      "Training Loss : 0.41236135363578796\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 483001\n",
      "mean reward (100 episodes) 81.142393\n",
      "best mean reward 158.049323\n",
      "running time 571.881499\n",
      "Train_EnvstepsSoFar : 483001\n",
      "Train_AverageReturn : 81.14239292481305\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 571.8814990520477\n",
      "Training Loss : 0.07567985355854034\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 484001\n",
      "mean reward (100 episodes) 83.463871\n",
      "best mean reward 158.049323\n",
      "running time 572.668497\n",
      "Train_EnvstepsSoFar : 484001\n",
      "Train_AverageReturn : 83.46387053635284\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 572.6684970855713\n",
      "Training Loss : 0.4219503700733185\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 485001\n",
      "mean reward (100 episodes) 90.921756\n",
      "best mean reward 158.049323\n",
      "running time 573.393893\n",
      "Train_EnvstepsSoFar : 485001\n",
      "Train_AverageReturn : 90.92175613369461\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 573.3938930034637\n",
      "Training Loss : 0.23021596670150757\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 486001\n",
      "mean reward (100 episodes) 90.272086\n",
      "best mean reward 158.049323\n",
      "running time 574.178952\n",
      "Train_EnvstepsSoFar : 486001\n",
      "Train_AverageReturn : 90.27208619562302\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 574.1789517402649\n",
      "Training Loss : 0.280285507440567\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 487001\n",
      "mean reward (100 episodes) 88.089484\n",
      "best mean reward 158.049323\n",
      "running time 574.951850\n",
      "Train_EnvstepsSoFar : 487001\n",
      "Train_AverageReturn : 88.08948419108647\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 574.951849937439\n",
      "Training Loss : 1.0314302444458008\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 488001\n",
      "mean reward (100 episodes) 82.042910\n",
      "best mean reward 158.049323\n",
      "running time 575.774944\n",
      "Train_EnvstepsSoFar : 488001\n",
      "Train_AverageReturn : 82.04290961479691\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 575.7749438285828\n",
      "Training Loss : 0.18318739533424377\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 489001\n",
      "mean reward (100 episodes) 75.758146\n",
      "best mean reward 158.049323\n",
      "running time 576.633819\n",
      "Train_EnvstepsSoFar : 489001\n",
      "Train_AverageReturn : 75.75814641646129\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 576.633819103241\n",
      "Training Loss : 0.32005950808525085\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 74.090700\n",
      "best mean reward 158.049323\n",
      "running time 577.352916\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 74.0906997832373\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 577.3529160022736\n",
      "Training Loss : 0.2653903663158417\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 491001\n",
      "mean reward (100 episodes) 69.100615\n",
      "best mean reward 158.049323\n",
      "running time 578.123164\n",
      "Train_EnvstepsSoFar : 491001\n",
      "Train_AverageReturn : 69.10061510837338\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 578.1231641769409\n",
      "Training Loss : 0.5729003548622131\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 492001\n",
      "mean reward (100 episodes) 70.324552\n",
      "best mean reward 158.049323\n",
      "running time 578.894887\n",
      "Train_EnvstepsSoFar : 492001\n",
      "Train_AverageReturn : 70.32455218577405\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 578.8948867321014\n",
      "Training Loss : 0.6315779685974121\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 493001\n",
      "mean reward (100 episodes) 72.654533\n",
      "best mean reward 158.049323\n",
      "running time 579.703145\n",
      "Train_EnvstepsSoFar : 493001\n",
      "Train_AverageReturn : 72.65453295026347\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 579.7031447887421\n",
      "Training Loss : 0.24836812913417816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 494001\n",
      "mean reward (100 episodes) 73.469970\n",
      "best mean reward 158.049323\n",
      "running time 580.601861\n",
      "Train_EnvstepsSoFar : 494001\n",
      "Train_AverageReturn : 73.46997034590422\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 580.601861000061\n",
      "Training Loss : 0.509554386138916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 495001\n",
      "mean reward (100 episodes) 74.670168\n",
      "best mean reward 158.049323\n",
      "running time 581.445245\n",
      "Train_EnvstepsSoFar : 495001\n",
      "Train_AverageReturn : 74.67016758698178\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 581.4452447891235\n",
      "Training Loss : 0.20541411638259888\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 496001\n",
      "mean reward (100 episodes) 77.469994\n",
      "best mean reward 158.049323\n",
      "running time 582.170405\n",
      "Train_EnvstepsSoFar : 496001\n",
      "Train_AverageReturn : 77.46999432109956\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 582.1704051494598\n",
      "Training Loss : 0.8369584679603577\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 497001\n",
      "mean reward (100 episodes) 84.059442\n",
      "best mean reward 158.049323\n",
      "running time 582.879598\n",
      "Train_EnvstepsSoFar : 497001\n",
      "Train_AverageReturn : 84.05944152114783\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 582.879597902298\n",
      "Training Loss : 1.6357131004333496\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 498001\n",
      "mean reward (100 episodes) 78.357629\n",
      "best mean reward 158.049323\n",
      "running time 583.644124\n",
      "Train_EnvstepsSoFar : 498001\n",
      "Train_AverageReturn : 78.35762861874474\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 583.6441240310669\n",
      "Training Loss : 2.6084959506988525\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 499001\n",
      "mean reward (100 episodes) 75.414504\n",
      "best mean reward 158.049323\n",
      "running time 584.416958\n",
      "Train_EnvstepsSoFar : 499001\n",
      "Train_AverageReturn : 75.41450434726089\n",
      "Train_BestReturn : 158.04932272060503\n",
      "TimeSinceStart : 584.4169578552246\n",
      "Training Loss : 0.389513224363327\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running DQN experiment with seed 1\n",
      "########################\n",
      "logging outputs to  logs/dqn/LunarLander/vanilla_dqn/seed1\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.000183\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.00018310546875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1001\n",
      "mean reward (100 episodes) -262.515721\n",
      "best mean reward -inf\n",
      "running time 0.257183\n",
      "Train_EnvstepsSoFar : 1001\n",
      "Train_AverageReturn : -262.5157208012048\n",
      "TimeSinceStart : 0.2571830749511719\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2001\n",
      "mean reward (100 episodes) -278.745888\n",
      "best mean reward -inf\n",
      "running time 0.911561\n",
      "Train_EnvstepsSoFar : 2001\n",
      "Train_AverageReturn : -278.7458875944531\n",
      "TimeSinceStart : 0.9115610122680664\n",
      "Training Loss : 0.4433700144290924\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3001\n",
      "mean reward (100 episodes) -265.301446\n",
      "best mean reward -inf\n",
      "running time 1.616040\n",
      "Train_EnvstepsSoFar : 3001\n",
      "Train_AverageReturn : -265.30144591250365\n",
      "TimeSinceStart : 1.6160399913787842\n",
      "Training Loss : 0.36529025435447693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4001\n",
      "mean reward (100 episodes) -284.868181\n",
      "best mean reward -inf\n",
      "running time 2.312578\n",
      "Train_EnvstepsSoFar : 4001\n",
      "Train_AverageReturn : -284.8681806912153\n",
      "TimeSinceStart : 2.3125782012939453\n",
      "Training Loss : 0.6106700301170349\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5001\n",
      "mean reward (100 episodes) -276.357826\n",
      "best mean reward -inf\n",
      "running time 3.002708\n",
      "Train_EnvstepsSoFar : 5001\n",
      "Train_AverageReturn : -276.3578261850576\n",
      "TimeSinceStart : 3.0027079582214355\n",
      "Training Loss : 0.39822039008140564\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6001\n",
      "mean reward (100 episodes) -259.929380\n",
      "best mean reward -inf\n",
      "running time 3.679165\n",
      "Train_EnvstepsSoFar : 6001\n",
      "Train_AverageReturn : -259.92937965286404\n",
      "TimeSinceStart : 3.6791648864746094\n",
      "Training Loss : 1.7702614068984985\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7001\n",
      "mean reward (100 episodes) -249.955409\n",
      "best mean reward -inf\n",
      "running time 4.415052\n",
      "Train_EnvstepsSoFar : 7001\n",
      "Train_AverageReturn : -249.955408744468\n",
      "TimeSinceStart : 4.415052175521851\n",
      "Training Loss : 3.665846347808838\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8001\n",
      "mean reward (100 episodes) -239.502504\n",
      "best mean reward -inf\n",
      "running time 5.119763\n",
      "Train_EnvstepsSoFar : 8001\n",
      "Train_AverageReturn : -239.50250407002457\n",
      "TimeSinceStart : 5.119762897491455\n",
      "Training Loss : 0.3715558350086212\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9001\n",
      "mean reward (100 episodes) -221.673854\n",
      "best mean reward -inf\n",
      "running time 5.817001\n",
      "Train_EnvstepsSoFar : 9001\n",
      "Train_AverageReturn : -221.67385378685674\n",
      "TimeSinceStart : 5.817001104354858\n",
      "Training Loss : 0.2690153121948242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -213.687137\n",
      "best mean reward -inf\n",
      "running time 6.525128\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -213.68713673738233\n",
      "TimeSinceStart : 6.52512788772583\n",
      "Training Loss : 0.664692759513855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 11001\n",
      "mean reward (100 episodes) -214.302593\n",
      "best mean reward -inf\n",
      "running time 7.267818\n",
      "Train_EnvstepsSoFar : 11001\n",
      "Train_AverageReturn : -214.3025934839438\n",
      "TimeSinceStart : 7.267817974090576\n",
      "Training Loss : 0.5315226912498474\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 12001\n",
      "mean reward (100 episodes) -204.899125\n",
      "best mean reward -inf\n",
      "running time 8.048121\n",
      "Train_EnvstepsSoFar : 12001\n",
      "Train_AverageReturn : -204.89912498416967\n",
      "TimeSinceStart : 8.048121213912964\n",
      "Training Loss : 0.3475174307823181\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 13001\n",
      "mean reward (100 episodes) -201.947345\n",
      "best mean reward -inf\n",
      "running time 8.919383\n",
      "Train_EnvstepsSoFar : 13001\n",
      "Train_AverageReturn : -201.94734513029314\n",
      "TimeSinceStart : 8.91938304901123\n",
      "Training Loss : 0.08330406248569489\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 14001\n",
      "mean reward (100 episodes) -206.647883\n",
      "best mean reward -206.647883\n",
      "running time 9.987929\n",
      "Train_EnvstepsSoFar : 14001\n",
      "Train_AverageReturn : -206.64788345024127\n",
      "Train_BestReturn : -206.64788345024127\n",
      "TimeSinceStart : 9.987929105758667\n",
      "Training Loss : 0.47549504041671753\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 15001\n",
      "mean reward (100 episodes) -196.958313\n",
      "best mean reward -196.958313\n",
      "running time 10.893110\n",
      "Train_EnvstepsSoFar : 15001\n",
      "Train_AverageReturn : -196.9583130749498\n",
      "Train_BestReturn : -196.9583130749498\n",
      "TimeSinceStart : 10.893110036849976\n",
      "Training Loss : 0.34932631254196167\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 16001\n",
      "mean reward (100 episodes) -193.378850\n",
      "best mean reward -193.378850\n",
      "running time 11.837391\n",
      "Train_EnvstepsSoFar : 16001\n",
      "Train_AverageReturn : -193.37885013995376\n",
      "Train_BestReturn : -193.37885013995376\n",
      "TimeSinceStart : 11.837391138076782\n",
      "Training Loss : 0.5962414145469666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 17001\n",
      "mean reward (100 episodes) -185.743594\n",
      "best mean reward -185.743594\n",
      "running time 13.010768\n",
      "Train_EnvstepsSoFar : 17001\n",
      "Train_AverageReturn : -185.74359408107014\n",
      "Train_BestReturn : -185.74359408107014\n",
      "TimeSinceStart : 13.010767936706543\n",
      "Training Loss : 1.129838228225708\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 18001\n",
      "mean reward (100 episodes) -179.128791\n",
      "best mean reward -179.128791\n",
      "running time 13.850952\n",
      "Train_EnvstepsSoFar : 18001\n",
      "Train_AverageReturn : -179.12879066384912\n",
      "Train_BestReturn : -179.12879066384912\n",
      "TimeSinceStart : 13.850951910018921\n",
      "Training Loss : 0.5690927505493164\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 19001\n",
      "mean reward (100 episodes) -178.217948\n",
      "best mean reward -178.217948\n",
      "running time 15.329314\n",
      "Train_EnvstepsSoFar : 19001\n",
      "Train_AverageReturn : -178.21794841382703\n",
      "Train_BestReturn : -178.21794841382703\n",
      "TimeSinceStart : 15.329314231872559\n",
      "Training Loss : 1.3775525093078613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -177.891981\n",
      "best mean reward -177.891981\n",
      "running time 17.514955\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -177.89198068713756\n",
      "Train_BestReturn : -177.89198068713756\n",
      "TimeSinceStart : 17.514955043792725\n",
      "Training Loss : 3.807833194732666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 21001\n",
      "mean reward (100 episodes) -177.697809\n",
      "best mean reward -177.697809\n",
      "running time 18.770620\n",
      "Train_EnvstepsSoFar : 21001\n",
      "Train_AverageReturn : -177.69780870235368\n",
      "Train_BestReturn : -177.69780870235368\n",
      "TimeSinceStart : 18.770619869232178\n",
      "Training Loss : 0.4512069821357727\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 22001\n",
      "mean reward (100 episodes) -174.546107\n",
      "best mean reward -174.546107\n",
      "running time 19.716983\n",
      "Train_EnvstepsSoFar : 22001\n",
      "Train_AverageReturn : -174.54610651058695\n",
      "Train_BestReturn : -174.54610651058695\n",
      "TimeSinceStart : 19.71698307991028\n",
      "Training Loss : 2.422429084777832\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 23001\n",
      "mean reward (100 episodes) -171.924320\n",
      "best mean reward -171.924320\n",
      "running time 21.237848\n",
      "Train_EnvstepsSoFar : 23001\n",
      "Train_AverageReturn : -171.92432028773553\n",
      "Train_BestReturn : -171.92432028773553\n",
      "TimeSinceStart : 21.237848043441772\n",
      "Training Loss : 1.9114047288894653\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 24001\n",
      "mean reward (100 episodes) -168.808757\n",
      "best mean reward -168.808757\n",
      "running time 22.776257\n",
      "Train_EnvstepsSoFar : 24001\n",
      "Train_AverageReturn : -168.80875722147178\n",
      "Train_BestReturn : -168.80875722147178\n",
      "TimeSinceStart : 22.776257038116455\n",
      "Training Loss : 0.5913366675376892\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 25001\n",
      "mean reward (100 episodes) -165.663198\n",
      "best mean reward -165.663198\n",
      "running time 24.473852\n",
      "Train_EnvstepsSoFar : 25001\n",
      "Train_AverageReturn : -165.66319782313474\n",
      "Train_BestReturn : -165.66319782313474\n",
      "TimeSinceStart : 24.473851919174194\n",
      "Training Loss : 0.7759026288986206\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 26001\n",
      "mean reward (100 episodes) -161.821487\n",
      "best mean reward -161.821487\n",
      "running time 26.157347\n",
      "Train_EnvstepsSoFar : 26001\n",
      "Train_AverageReturn : -161.82148683784723\n",
      "Train_BestReturn : -161.82148683784723\n",
      "TimeSinceStart : 26.157347202301025\n",
      "Training Loss : 2.9543473720550537\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 27001\n",
      "mean reward (100 episodes) -159.481710\n",
      "best mean reward -159.481710\n",
      "running time 27.939086\n",
      "Train_EnvstepsSoFar : 27001\n",
      "Train_AverageReturn : -159.4817096295137\n",
      "Train_BestReturn : -159.4817096295137\n",
      "TimeSinceStart : 27.939086198806763\n",
      "Training Loss : 0.4115966856479645\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 28001\n",
      "mean reward (100 episodes) -158.561771\n",
      "best mean reward -158.561771\n",
      "running time 29.480047\n",
      "Train_EnvstepsSoFar : 28001\n",
      "Train_AverageReturn : -158.56177109369716\n",
      "Train_BestReturn : -158.56177109369716\n",
      "TimeSinceStart : 29.48004722595215\n",
      "Training Loss : 1.1816134452819824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 29001\n",
      "mean reward (100 episodes) -157.930047\n",
      "best mean reward -157.930047\n",
      "running time 31.554333\n",
      "Train_EnvstepsSoFar : 29001\n",
      "Train_AverageReturn : -157.9300465752398\n",
      "Train_BestReturn : -157.9300465752398\n",
      "TimeSinceStart : 31.554332971572876\n",
      "Training Loss : 0.5776616334915161\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -160.490306\n",
      "best mean reward -157.930047\n",
      "running time 33.996420\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -160.49030610686495\n",
      "Train_BestReturn : -157.9300465752398\n",
      "TimeSinceStart : 33.99642014503479\n",
      "Training Loss : 0.4421994090080261\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 31001\n",
      "mean reward (100 episodes) -157.702853\n",
      "best mean reward -157.702853\n",
      "running time 36.267181\n",
      "Train_EnvstepsSoFar : 31001\n",
      "Train_AverageReturn : -157.70285324553896\n",
      "Train_BestReturn : -157.70285324553896\n",
      "TimeSinceStart : 36.267181158065796\n",
      "Training Loss : 3.7509725093841553\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 32001\n",
      "mean reward (100 episodes) -154.512284\n",
      "best mean reward -154.512284\n",
      "running time 38.207941\n",
      "Train_EnvstepsSoFar : 32001\n",
      "Train_AverageReturn : -154.5122836984931\n",
      "Train_BestReturn : -154.5122836984931\n",
      "TimeSinceStart : 38.20794129371643\n",
      "Training Loss : 1.1387993097305298\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 33001\n",
      "mean reward (100 episodes) -152.864832\n",
      "best mean reward -152.864832\n",
      "running time 39.809003\n",
      "Train_EnvstepsSoFar : 33001\n",
      "Train_AverageReturn : -152.86483214893036\n",
      "Train_BestReturn : -152.86483214893036\n",
      "TimeSinceStart : 39.80900311470032\n",
      "Training Loss : 0.3757930099964142\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 34001\n",
      "mean reward (100 episodes) -153.162477\n",
      "best mean reward -152.864832\n",
      "running time 41.380167\n",
      "Train_EnvstepsSoFar : 34001\n",
      "Train_AverageReturn : -153.16247744602205\n",
      "Train_BestReturn : -152.86483214893036\n",
      "TimeSinceStart : 41.38016700744629\n",
      "Training Loss : 0.19667638838291168\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 35001\n",
      "mean reward (100 episodes) -153.612846\n",
      "best mean reward -152.864832\n",
      "running time 43.320836\n",
      "Train_EnvstepsSoFar : 35001\n",
      "Train_AverageReturn : -153.61284631216284\n",
      "Train_BestReturn : -152.86483214893036\n",
      "TimeSinceStart : 43.32083606719971\n",
      "Training Loss : 2.1310317516326904\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 36001\n",
      "mean reward (100 episodes) -153.858529\n",
      "best mean reward -152.864832\n",
      "running time 45.100382\n",
      "Train_EnvstepsSoFar : 36001\n",
      "Train_AverageReturn : -153.85852924366074\n",
      "Train_BestReturn : -152.86483214893036\n",
      "TimeSinceStart : 45.10038208961487\n",
      "Training Loss : 0.32852500677108765\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 37001\n",
      "mean reward (100 episodes) -153.763156\n",
      "best mean reward -152.864832\n",
      "running time 47.000424\n",
      "Train_EnvstepsSoFar : 37001\n",
      "Train_AverageReturn : -153.76315615707438\n",
      "Train_BestReturn : -152.86483214893036\n",
      "TimeSinceStart : 47.00042390823364\n",
      "Training Loss : 0.6521152853965759\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 38001\n",
      "mean reward (100 episodes) -153.218109\n",
      "best mean reward -152.864832\n",
      "running time 49.679517\n",
      "Train_EnvstepsSoFar : 38001\n",
      "Train_AverageReturn : -153.2181088863904\n",
      "Train_BestReturn : -152.86483214893036\n",
      "TimeSinceStart : 49.67951703071594\n",
      "Training Loss : 1.1944659948349\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 39001\n",
      "mean reward (100 episodes) -151.832039\n",
      "best mean reward -151.832039\n",
      "running time 51.398489\n",
      "Train_EnvstepsSoFar : 39001\n",
      "Train_AverageReturn : -151.8320392346173\n",
      "Train_BestReturn : -151.8320392346173\n",
      "TimeSinceStart : 51.398489236831665\n",
      "Training Loss : 0.3956001102924347\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -150.351827\n",
      "best mean reward -150.351827\n",
      "running time 53.575625\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -150.35182696887375\n",
      "Train_BestReturn : -150.35182696887375\n",
      "TimeSinceStart : 53.57562494277954\n",
      "Training Loss : 0.3929162323474884\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 41001\n",
      "mean reward (100 episodes) -148.078877\n",
      "best mean reward -148.078877\n",
      "running time 55.346921\n",
      "Train_EnvstepsSoFar : 41001\n",
      "Train_AverageReturn : -148.0788771406534\n",
      "Train_BestReturn : -148.0788771406534\n",
      "TimeSinceStart : 55.34692096710205\n",
      "Training Loss : 4.051167011260986\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 42001\n",
      "mean reward (100 episodes) -148.319364\n",
      "best mean reward -148.078877\n",
      "running time 57.837300\n",
      "Train_EnvstepsSoFar : 42001\n",
      "Train_AverageReturn : -148.3193638930058\n",
      "Train_BestReturn : -148.0788771406534\n",
      "TimeSinceStart : 57.837300062179565\n",
      "Training Loss : 0.8390329480171204\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 43001\n",
      "mean reward (100 episodes) -147.285564\n",
      "best mean reward -147.285564\n",
      "running time 59.499143\n",
      "Train_EnvstepsSoFar : 43001\n",
      "Train_AverageReturn : -147.2855636752183\n",
      "Train_BestReturn : -147.2855636752183\n",
      "TimeSinceStart : 59.49914288520813\n",
      "Training Loss : 0.3086743950843811\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 44001\n",
      "mean reward (100 episodes) -147.935622\n",
      "best mean reward -147.285564\n",
      "running time 61.213250\n",
      "Train_EnvstepsSoFar : 44001\n",
      "Train_AverageReturn : -147.93562236736926\n",
      "Train_BestReturn : -147.2855636752183\n",
      "TimeSinceStart : 61.213250160217285\n",
      "Training Loss : 0.2022729367017746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 45001\n",
      "mean reward (100 episodes) -148.250904\n",
      "best mean reward -147.285564\n",
      "running time 63.160615\n",
      "Train_EnvstepsSoFar : 45001\n",
      "Train_AverageReturn : -148.2509044651904\n",
      "Train_BestReturn : -147.2855636752183\n",
      "TimeSinceStart : 63.16061496734619\n",
      "Training Loss : 0.3422369956970215\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 46001\n",
      "mean reward (100 episodes) -148.218861\n",
      "best mean reward -147.285564\n",
      "running time 64.749157\n",
      "Train_EnvstepsSoFar : 46001\n",
      "Train_AverageReturn : -148.21886121977926\n",
      "Train_BestReturn : -147.2855636752183\n",
      "TimeSinceStart : 64.7491569519043\n",
      "Training Loss : 0.3142227232456207\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 47001\n",
      "mean reward (100 episodes) -144.715514\n",
      "best mean reward -144.715514\n",
      "running time 66.330872\n",
      "Train_EnvstepsSoFar : 47001\n",
      "Train_AverageReturn : -144.71551364936786\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 66.33087205886841\n",
      "Training Loss : 0.34976011514663696\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 48001\n",
      "mean reward (100 episodes) -144.899086\n",
      "best mean reward -144.715514\n",
      "running time 67.807778\n",
      "Train_EnvstepsSoFar : 48001\n",
      "Train_AverageReturn : -144.89908599170514\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 67.8077781200409\n",
      "Training Loss : 0.43946564197540283\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 49001\n",
      "mean reward (100 episodes) -145.147289\n",
      "best mean reward -144.715514\n",
      "running time 69.376320\n",
      "Train_EnvstepsSoFar : 49001\n",
      "Train_AverageReturn : -145.14728888373486\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 69.37632012367249\n",
      "Training Loss : 4.7114715576171875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -145.201819\n",
      "best mean reward -144.715514\n",
      "running time 71.230777\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -145.2018188450506\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 71.23077726364136\n",
      "Training Loss : 0.42461204528808594\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 51001\n",
      "mean reward (100 episodes) -145.593043\n",
      "best mean reward -144.715514\n",
      "running time 72.655895\n",
      "Train_EnvstepsSoFar : 51001\n",
      "Train_AverageReturn : -145.59304329254007\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 72.65589499473572\n",
      "Training Loss : 0.38989585638046265\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 52001\n",
      "mean reward (100 episodes) -146.428637\n",
      "best mean reward -144.715514\n",
      "running time 73.864206\n",
      "Train_EnvstepsSoFar : 52001\n",
      "Train_AverageReturn : -146.42863700253764\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 73.86420607566833\n",
      "Training Loss : 0.23150064051151276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 53001\n",
      "mean reward (100 episodes) -144.937148\n",
      "best mean reward -144.715514\n",
      "running time 75.804798\n",
      "Train_EnvstepsSoFar : 53001\n",
      "Train_AverageReturn : -144.9371483568124\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 75.8047981262207\n",
      "Training Loss : 3.4378488063812256\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 54001\n",
      "mean reward (100 episodes) -146.573240\n",
      "best mean reward -144.715514\n",
      "running time 77.726669\n",
      "Train_EnvstepsSoFar : 54001\n",
      "Train_AverageReturn : -146.5732399186244\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 77.72666907310486\n",
      "Training Loss : 0.2796616852283478\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 55001\n",
      "mean reward (100 episodes) -149.198301\n",
      "best mean reward -144.715514\n",
      "running time 79.683716\n",
      "Train_EnvstepsSoFar : 55001\n",
      "Train_AverageReturn : -149.1983013809937\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 79.68371605873108\n",
      "Training Loss : 7.09867525100708\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 56001\n",
      "mean reward (100 episodes) -148.070601\n",
      "best mean reward -144.715514\n",
      "running time 81.579414\n",
      "Train_EnvstepsSoFar : 56001\n",
      "Train_AverageReturn : -148.07060086912142\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 81.5794141292572\n",
      "Training Loss : 0.33715367317199707\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 57001\n",
      "mean reward (100 episodes) -149.790376\n",
      "best mean reward -144.715514\n",
      "running time 82.699414\n",
      "Train_EnvstepsSoFar : 57001\n",
      "Train_AverageReturn : -149.79037550620833\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 82.69941401481628\n",
      "Training Loss : 0.32407277822494507\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 58001\n",
      "mean reward (100 episodes) -150.226207\n",
      "best mean reward -144.715514\n",
      "running time 84.323574\n",
      "Train_EnvstepsSoFar : 58001\n",
      "Train_AverageReturn : -150.22620692937787\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 84.32357406616211\n",
      "Training Loss : 0.4563418924808502\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 59001\n",
      "mean reward (100 episodes) -149.000879\n",
      "best mean reward -144.715514\n",
      "running time 86.320542\n",
      "Train_EnvstepsSoFar : 59001\n",
      "Train_AverageReturn : -149.0008787580979\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 86.32054209709167\n",
      "Training Loss : 0.588062047958374\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -148.098433\n",
      "best mean reward -144.715514\n",
      "running time 88.147481\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -148.09843283758244\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 88.14748120307922\n",
      "Training Loss : 0.7866525650024414\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 61001\n",
      "mean reward (100 episodes) -152.745536\n",
      "best mean reward -144.715514\n",
      "running time 89.565631\n",
      "Train_EnvstepsSoFar : 61001\n",
      "Train_AverageReturn : -152.7455361741366\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 89.56563115119934\n",
      "Training Loss : 0.4080473482608795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 62001\n",
      "mean reward (100 episodes) -153.245163\n",
      "best mean reward -144.715514\n",
      "running time 90.743818\n",
      "Train_EnvstepsSoFar : 62001\n",
      "Train_AverageReturn : -153.24516328281723\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 90.74381804466248\n",
      "Training Loss : 0.2708360552787781\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 63001\n",
      "mean reward (100 episodes) -150.627866\n",
      "best mean reward -144.715514\n",
      "running time 92.261276\n",
      "Train_EnvstepsSoFar : 63001\n",
      "Train_AverageReturn : -150.6278661863115\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 92.26127600669861\n",
      "Training Loss : 0.34128639101982117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 64001\n",
      "mean reward (100 episodes) -149.756679\n",
      "best mean reward -144.715514\n",
      "running time 94.007742\n",
      "Train_EnvstepsSoFar : 64001\n",
      "Train_AverageReturn : -149.75667942198945\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 94.00774192810059\n",
      "Training Loss : 0.2926168441772461\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 65001\n",
      "mean reward (100 episodes) -147.406716\n",
      "best mean reward -144.715514\n",
      "running time 95.667281\n",
      "Train_EnvstepsSoFar : 65001\n",
      "Train_AverageReturn : -147.40671567790935\n",
      "Train_BestReturn : -144.71551364936786\n",
      "TimeSinceStart : 95.66728091239929\n",
      "Training Loss : 4.1291704177856445\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 66001\n",
      "mean reward (100 episodes) -144.455339\n",
      "best mean reward -144.455339\n",
      "running time 97.605726\n",
      "Train_EnvstepsSoFar : 66001\n",
      "Train_AverageReturn : -144.45533879175295\n",
      "Train_BestReturn : -144.45533879175295\n",
      "TimeSinceStart : 97.60572600364685\n",
      "Training Loss : 0.2562964856624603\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 67001\n",
      "mean reward (100 episodes) -144.533289\n",
      "best mean reward -144.455339\n",
      "running time 99.572536\n",
      "Train_EnvstepsSoFar : 67001\n",
      "Train_AverageReturn : -144.53328860207\n",
      "Train_BestReturn : -144.45533879175295\n",
      "TimeSinceStart : 99.57253623008728\n",
      "Training Loss : 0.3575655221939087\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 68001\n",
      "mean reward (100 episodes) -139.529696\n",
      "best mean reward -139.529696\n",
      "running time 101.406799\n",
      "Train_EnvstepsSoFar : 68001\n",
      "Train_AverageReturn : -139.52969557557043\n",
      "Train_BestReturn : -139.52969557557043\n",
      "TimeSinceStart : 101.40679907798767\n",
      "Training Loss : 0.28968697786331177\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 69001\n",
      "mean reward (100 episodes) -136.116471\n",
      "best mean reward -136.116471\n",
      "running time 102.620652\n",
      "Train_EnvstepsSoFar : 69001\n",
      "Train_AverageReturn : -136.11647097885373\n",
      "Train_BestReturn : -136.11647097885373\n",
      "TimeSinceStart : 102.6206521987915\n",
      "Training Loss : 1.2336117029190063\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -136.729951\n",
      "best mean reward -136.116471\n",
      "running time 104.199052\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -136.72995063208936\n",
      "Train_BestReturn : -136.11647097885373\n",
      "TimeSinceStart : 104.19905209541321\n",
      "Training Loss : 0.39708173274993896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 71001\n",
      "mean reward (100 episodes) -135.829263\n",
      "best mean reward -135.829263\n",
      "running time 106.301341\n",
      "Train_EnvstepsSoFar : 71001\n",
      "Train_AverageReturn : -135.82926295867014\n",
      "Train_BestReturn : -135.82926295867014\n",
      "TimeSinceStart : 106.30134105682373\n",
      "Training Loss : 0.21648697555065155\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 72001\n",
      "mean reward (100 episodes) -131.618981\n",
      "best mean reward -131.618981\n",
      "running time 107.477393\n",
      "Train_EnvstepsSoFar : 72001\n",
      "Train_AverageReturn : -131.61898051635\n",
      "Train_BestReturn : -131.61898051635\n",
      "TimeSinceStart : 107.47739291191101\n",
      "Training Loss : 0.3557126224040985\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 73001\n",
      "mean reward (100 episodes) -131.979599\n",
      "best mean reward -131.618981\n",
      "running time 108.573638\n",
      "Train_EnvstepsSoFar : 73001\n",
      "Train_AverageReturn : -131.97959850493677\n",
      "Train_BestReturn : -131.61898051635\n",
      "TimeSinceStart : 108.57363796234131\n",
      "Training Loss : 0.2374207079410553\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 74001\n",
      "mean reward (100 episodes) -129.627968\n",
      "best mean reward -129.627968\n",
      "running time 109.856064\n",
      "Train_EnvstepsSoFar : 74001\n",
      "Train_AverageReturn : -129.627968494644\n",
      "Train_BestReturn : -129.627968494644\n",
      "TimeSinceStart : 109.85606408119202\n",
      "Training Loss : 0.2957656979560852\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 75001\n",
      "mean reward (100 episodes) -127.809475\n",
      "best mean reward -127.809475\n",
      "running time 111.657228\n",
      "Train_EnvstepsSoFar : 75001\n",
      "Train_AverageReturn : -127.80947472721962\n",
      "Train_BestReturn : -127.80947472721962\n",
      "TimeSinceStart : 111.65722799301147\n",
      "Training Loss : 0.3291519582271576\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 76001\n",
      "mean reward (100 episodes) -123.214751\n",
      "best mean reward -123.214751\n",
      "running time 113.001576\n",
      "Train_EnvstepsSoFar : 76001\n",
      "Train_AverageReturn : -123.21475088950558\n",
      "Train_BestReturn : -123.21475088950558\n",
      "TimeSinceStart : 113.00157594680786\n",
      "Training Loss : 0.20043519139289856\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 77001\n",
      "mean reward (100 episodes) -117.734812\n",
      "best mean reward -117.734812\n",
      "running time 114.230648\n",
      "Train_EnvstepsSoFar : 77001\n",
      "Train_AverageReturn : -117.73481156510643\n",
      "Train_BestReturn : -117.73481156510643\n",
      "TimeSinceStart : 114.23064804077148\n",
      "Training Loss : 0.2577659487724304\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 78001\n",
      "mean reward (100 episodes) -113.859708\n",
      "best mean reward -113.859708\n",
      "running time 115.697769\n",
      "Train_EnvstepsSoFar : 78001\n",
      "Train_AverageReturn : -113.8597078510334\n",
      "Train_BestReturn : -113.8597078510334\n",
      "TimeSinceStart : 115.69776892662048\n",
      "Training Loss : 0.2693532109260559\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 79001\n",
      "mean reward (100 episodes) -109.915995\n",
      "best mean reward -109.915995\n",
      "running time 116.714250\n",
      "Train_EnvstepsSoFar : 79001\n",
      "Train_AverageReturn : -109.91599513038739\n",
      "Train_BestReturn : -109.91599513038739\n",
      "TimeSinceStart : 116.71425008773804\n",
      "Training Loss : 0.21877996623516083\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -107.836100\n",
      "best mean reward -107.836100\n",
      "running time 118.455701\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -107.8360998580144\n",
      "Train_BestReturn : -107.8360998580144\n",
      "TimeSinceStart : 118.45570111274719\n",
      "Training Loss : 0.2981618642807007\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 81001\n",
      "mean reward (100 episodes) -106.936623\n",
      "best mean reward -106.936623\n",
      "running time 120.623671\n",
      "Train_EnvstepsSoFar : 81001\n",
      "Train_AverageReturn : -106.93662311158327\n",
      "Train_BestReturn : -106.93662311158327\n",
      "TimeSinceStart : 120.62367105484009\n",
      "Training Loss : 0.4152052402496338\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 82001\n",
      "mean reward (100 episodes) -105.625517\n",
      "best mean reward -105.625517\n",
      "running time 122.524905\n",
      "Train_EnvstepsSoFar : 82001\n",
      "Train_AverageReturn : -105.62551728569059\n",
      "Train_BestReturn : -105.62551728569059\n",
      "TimeSinceStart : 122.52490496635437\n",
      "Training Loss : 0.29067105054855347\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 83001\n",
      "mean reward (100 episodes) -98.961115\n",
      "best mean reward -98.961115\n",
      "running time 124.265819\n",
      "Train_EnvstepsSoFar : 83001\n",
      "Train_AverageReturn : -98.96111531858182\n",
      "Train_BestReturn : -98.96111531858182\n",
      "TimeSinceStart : 124.26581883430481\n",
      "Training Loss : 0.3368874490261078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 84001\n",
      "mean reward (100 episodes) -98.650301\n",
      "best mean reward -98.650301\n",
      "running time 125.893268\n",
      "Train_EnvstepsSoFar : 84001\n",
      "Train_AverageReturn : -98.6503009075404\n",
      "Train_BestReturn : -98.6503009075404\n",
      "TimeSinceStart : 125.89326810836792\n",
      "Training Loss : 0.15646414458751678\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 85001\n",
      "mean reward (100 episodes) -101.095989\n",
      "best mean reward -98.650301\n",
      "running time 127.800609\n",
      "Train_EnvstepsSoFar : 85001\n",
      "Train_AverageReturn : -101.09598944496534\n",
      "Train_BestReturn : -98.6503009075404\n",
      "TimeSinceStart : 127.80060911178589\n",
      "Training Loss : 0.19083169102668762\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 86001\n",
      "mean reward (100 episodes) -98.736448\n",
      "best mean reward -98.650301\n",
      "running time 129.400879\n",
      "Train_EnvstepsSoFar : 86001\n",
      "Train_AverageReturn : -98.73644830548446\n",
      "Train_BestReturn : -98.6503009075404\n",
      "TimeSinceStart : 129.40087914466858\n",
      "Training Loss : 0.26638346910476685\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 87001\n",
      "mean reward (100 episodes) -97.573497\n",
      "best mean reward -97.573497\n",
      "running time 131.126083\n",
      "Train_EnvstepsSoFar : 87001\n",
      "Train_AverageReturn : -97.57349655419532\n",
      "Train_BestReturn : -97.57349655419532\n",
      "TimeSinceStart : 131.12608313560486\n",
      "Training Loss : 0.23080454766750336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 88001\n",
      "mean reward (100 episodes) -95.642291\n",
      "best mean reward -95.642291\n",
      "running time 132.484526\n",
      "Train_EnvstepsSoFar : 88001\n",
      "Train_AverageReturn : -95.64229108097683\n",
      "Train_BestReturn : -95.64229108097683\n",
      "TimeSinceStart : 132.48452615737915\n",
      "Training Loss : 0.17814888060092926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 89001\n",
      "mean reward (100 episodes) -97.535010\n",
      "best mean reward -95.642291\n",
      "running time 133.478355\n",
      "Train_EnvstepsSoFar : 89001\n",
      "Train_AverageReturn : -97.53500983366612\n",
      "Train_BestReturn : -95.64229108097683\n",
      "TimeSinceStart : 133.47835493087769\n",
      "Training Loss : 0.28393328189849854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -95.982067\n",
      "best mean reward -95.642291\n",
      "running time 134.800758\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -95.9820665511722\n",
      "Train_BestReturn : -95.64229108097683\n",
      "TimeSinceStart : 134.80075812339783\n",
      "Training Loss : 0.2231462150812149\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 91001\n",
      "mean reward (100 episodes) -87.071748\n",
      "best mean reward -87.071748\n",
      "running time 135.947606\n",
      "Train_EnvstepsSoFar : 91001\n",
      "Train_AverageReturn : -87.07174836746499\n",
      "Train_BestReturn : -87.07174836746499\n",
      "TimeSinceStart : 135.94760608673096\n",
      "Training Loss : 0.26178547739982605\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 92001\n",
      "mean reward (100 episodes) -82.590134\n",
      "best mean reward -82.590134\n",
      "running time 137.440146\n",
      "Train_EnvstepsSoFar : 92001\n",
      "Train_AverageReturn : -82.59013367257884\n",
      "Train_BestReturn : -82.59013367257884\n",
      "TimeSinceStart : 137.44014596939087\n",
      "Training Loss : 0.25155720114707947\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 93001\n",
      "mean reward (100 episodes) -78.655415\n",
      "best mean reward -78.655415\n",
      "running time 139.012694\n",
      "Train_EnvstepsSoFar : 93001\n",
      "Train_AverageReturn : -78.65541482385636\n",
      "Train_BestReturn : -78.65541482385636\n",
      "TimeSinceStart : 139.0126941204071\n",
      "Training Loss : 0.16387926042079926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 94001\n",
      "mean reward (100 episodes) -73.241218\n",
      "best mean reward -73.241218\n",
      "running time 140.633497\n",
      "Train_EnvstepsSoFar : 94001\n",
      "Train_AverageReturn : -73.24121758406801\n",
      "Train_BestReturn : -73.24121758406801\n",
      "TimeSinceStart : 140.63349723815918\n",
      "Training Loss : 0.4244408905506134\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 95001\n",
      "mean reward (100 episodes) -67.738143\n",
      "best mean reward -67.738143\n",
      "running time 141.935373\n",
      "Train_EnvstepsSoFar : 95001\n",
      "Train_AverageReturn : -67.73814324536261\n",
      "Train_BestReturn : -67.73814324536261\n",
      "TimeSinceStart : 141.93537306785583\n",
      "Training Loss : 0.20888535678386688\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 96001\n",
      "mean reward (100 episodes) -65.912672\n",
      "best mean reward -65.912672\n",
      "running time 143.407031\n",
      "Train_EnvstepsSoFar : 96001\n",
      "Train_AverageReturn : -65.91267229943078\n",
      "Train_BestReturn : -65.91267229943078\n",
      "TimeSinceStart : 143.40703105926514\n",
      "Training Loss : 0.16072407364845276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 97001\n",
      "mean reward (100 episodes) -65.778162\n",
      "best mean reward -65.778162\n",
      "running time 145.201877\n",
      "Train_EnvstepsSoFar : 97001\n",
      "Train_AverageReturn : -65.77816154366286\n",
      "Train_BestReturn : -65.77816154366286\n",
      "TimeSinceStart : 145.20187711715698\n",
      "Training Loss : 0.19498318433761597\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 98001\n",
      "mean reward (100 episodes) -63.209653\n",
      "best mean reward -63.209653\n",
      "running time 146.895292\n",
      "Train_EnvstepsSoFar : 98001\n",
      "Train_AverageReturn : -63.20965293189011\n",
      "Train_BestReturn : -63.20965293189011\n",
      "TimeSinceStart : 146.8952920436859\n",
      "Training Loss : 0.2734636962413788\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 99001\n",
      "mean reward (100 episodes) -62.832500\n",
      "best mean reward -62.832500\n",
      "running time 148.342558\n",
      "Train_EnvstepsSoFar : 99001\n",
      "Train_AverageReturn : -62.832499760772386\n",
      "Train_BestReturn : -62.832499760772386\n",
      "TimeSinceStart : 148.3425579071045\n",
      "Training Loss : 2.600076198577881\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -61.493472\n",
      "best mean reward -61.493472\n",
      "running time 150.161451\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -61.49347189328525\n",
      "Train_BestReturn : -61.49347189328525\n",
      "TimeSinceStart : 150.1614511013031\n",
      "Training Loss : 0.1558745950460434\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 101001\n",
      "mean reward (100 episodes) -60.572238\n",
      "best mean reward -60.572238\n",
      "running time 151.606802\n",
      "Train_EnvstepsSoFar : 101001\n",
      "Train_AverageReturn : -60.5722383346056\n",
      "Train_BestReturn : -60.5722383346056\n",
      "TimeSinceStart : 151.60680198669434\n",
      "Training Loss : 1.2621345520019531\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 102001\n",
      "mean reward (100 episodes) -55.631921\n",
      "best mean reward -55.631921\n",
      "running time 152.983734\n",
      "Train_EnvstepsSoFar : 102001\n",
      "Train_AverageReturn : -55.631921435169886\n",
      "Train_BestReturn : -55.631921435169886\n",
      "TimeSinceStart : 152.98373413085938\n",
      "Training Loss : 0.5537899136543274\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 103001\n",
      "mean reward (100 episodes) -54.660765\n",
      "best mean reward -54.660765\n",
      "running time 154.552697\n",
      "Train_EnvstepsSoFar : 103001\n",
      "Train_AverageReturn : -54.66076530215566\n",
      "Train_BestReturn : -54.66076530215566\n",
      "TimeSinceStart : 154.55269718170166\n",
      "Training Loss : 0.16557389497756958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 104001\n",
      "mean reward (100 episodes) -51.717458\n",
      "best mean reward -51.717458\n",
      "running time 155.820309\n",
      "Train_EnvstepsSoFar : 104001\n",
      "Train_AverageReturn : -51.717458204645794\n",
      "Train_BestReturn : -51.717458204645794\n",
      "TimeSinceStart : 155.8203091621399\n",
      "Training Loss : 0.17343983054161072\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 105001\n",
      "mean reward (100 episodes) -51.752809\n",
      "best mean reward -51.717458\n",
      "running time 157.675907\n",
      "Train_EnvstepsSoFar : 105001\n",
      "Train_AverageReturn : -51.7528089453732\n",
      "Train_BestReturn : -51.717458204645794\n",
      "TimeSinceStart : 157.67590713500977\n",
      "Training Loss : 0.6430802345275879\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 106001\n",
      "mean reward (100 episodes) -50.816436\n",
      "best mean reward -50.816436\n",
      "running time 159.149472\n",
      "Train_EnvstepsSoFar : 106001\n",
      "Train_AverageReturn : -50.816436182262244\n",
      "Train_BestReturn : -50.816436182262244\n",
      "TimeSinceStart : 159.14947199821472\n",
      "Training Loss : 0.3331761658191681\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 107001\n",
      "mean reward (100 episodes) -50.368320\n",
      "best mean reward -50.368320\n",
      "running time 160.512023\n",
      "Train_EnvstepsSoFar : 107001\n",
      "Train_AverageReturn : -50.36832032444323\n",
      "Train_BestReturn : -50.36832032444323\n",
      "TimeSinceStart : 160.5120232105255\n",
      "Training Loss : 0.24021710455417633\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 108001\n",
      "mean reward (100 episodes) -49.959503\n",
      "best mean reward -49.959503\n",
      "running time 162.450064\n",
      "Train_EnvstepsSoFar : 108001\n",
      "Train_AverageReturn : -49.95950303949138\n",
      "Train_BestReturn : -49.95950303949138\n",
      "TimeSinceStart : 162.45006394386292\n",
      "Training Loss : 2.288548469543457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 109001\n",
      "mean reward (100 episodes) -48.143822\n",
      "best mean reward -48.143822\n",
      "running time 164.521783\n",
      "Train_EnvstepsSoFar : 109001\n",
      "Train_AverageReturn : -48.14382163110729\n",
      "Train_BestReturn : -48.14382163110729\n",
      "TimeSinceStart : 164.52178311347961\n",
      "Training Loss : 0.0996391549706459\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -46.919799\n",
      "best mean reward -46.919799\n",
      "running time 166.745599\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -46.91979850721541\n",
      "Train_BestReturn : -46.91979850721541\n",
      "TimeSinceStart : 166.74559903144836\n",
      "Training Loss : 0.08367995917797089\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 111001\n",
      "mean reward (100 episodes) -43.803604\n",
      "best mean reward -43.803604\n",
      "running time 168.779580\n",
      "Train_EnvstepsSoFar : 111001\n",
      "Train_AverageReturn : -43.80360433631913\n",
      "Train_BestReturn : -43.80360433631913\n",
      "TimeSinceStart : 168.7795798778534\n",
      "Training Loss : 0.18725484609603882\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 112001\n",
      "mean reward (100 episodes) -41.089380\n",
      "best mean reward -41.089380\n",
      "running time 170.224326\n",
      "Train_EnvstepsSoFar : 112001\n",
      "Train_AverageReturn : -41.089380330272505\n",
      "Train_BestReturn : -41.089380330272505\n",
      "TimeSinceStart : 170.22432589530945\n",
      "Training Loss : 0.20412153005599976\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 113001\n",
      "mean reward (100 episodes) -39.528391\n",
      "best mean reward -39.528391\n",
      "running time 171.950242\n",
      "Train_EnvstepsSoFar : 113001\n",
      "Train_AverageReturn : -39.52839084158094\n",
      "Train_BestReturn : -39.52839084158094\n",
      "TimeSinceStart : 171.9502420425415\n",
      "Training Loss : 0.15532338619232178\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 114001\n",
      "mean reward (100 episodes) -33.665997\n",
      "best mean reward -33.665997\n",
      "running time 173.670380\n",
      "Train_EnvstepsSoFar : 114001\n",
      "Train_AverageReturn : -33.66599685898075\n",
      "Train_BestReturn : -33.66599685898075\n",
      "TimeSinceStart : 173.67038011550903\n",
      "Training Loss : 0.7191148996353149\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 115001\n",
      "mean reward (100 episodes) -33.355811\n",
      "best mean reward -33.355811\n",
      "running time 175.977792\n",
      "Train_EnvstepsSoFar : 115001\n",
      "Train_AverageReturn : -33.355810619604995\n",
      "Train_BestReturn : -33.355810619604995\n",
      "TimeSinceStart : 175.97779202461243\n",
      "Training Loss : 0.5330800414085388\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 116001\n",
      "mean reward (100 episodes) -31.278421\n",
      "best mean reward -31.278421\n",
      "running time 177.404410\n",
      "Train_EnvstepsSoFar : 116001\n",
      "Train_AverageReturn : -31.278421429672907\n",
      "Train_BestReturn : -31.278421429672907\n",
      "TimeSinceStart : 177.40441012382507\n",
      "Training Loss : 0.27765798568725586\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 117001\n",
      "mean reward (100 episodes) -28.259769\n",
      "best mean reward -28.259769\n",
      "running time 178.570035\n",
      "Train_EnvstepsSoFar : 117001\n",
      "Train_AverageReturn : -28.25976940135755\n",
      "Train_BestReturn : -28.25976940135755\n",
      "TimeSinceStart : 178.57003498077393\n",
      "Training Loss : 0.0658901035785675\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 118001\n",
      "mean reward (100 episodes) -28.591124\n",
      "best mean reward -28.259769\n",
      "running time 180.009216\n",
      "Train_EnvstepsSoFar : 118001\n",
      "Train_AverageReturn : -28.591124102056174\n",
      "Train_BestReturn : -28.25976940135755\n",
      "TimeSinceStart : 180.00921607017517\n",
      "Training Loss : 0.21948660910129547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 119001\n",
      "mean reward (100 episodes) -26.547830\n",
      "best mean reward -26.547830\n",
      "running time 181.499619\n",
      "Train_EnvstepsSoFar : 119001\n",
      "Train_AverageReturn : -26.547829984430173\n",
      "Train_BestReturn : -26.547829984430173\n",
      "TimeSinceStart : 181.49961924552917\n",
      "Training Loss : 0.14844243228435516\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -22.390892\n",
      "best mean reward -22.390892\n",
      "running time 182.721860\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -22.39089235068592\n",
      "Train_BestReturn : -22.39089235068592\n",
      "TimeSinceStart : 182.72186017036438\n",
      "Training Loss : 0.1946788728237152\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 121001\n",
      "mean reward (100 episodes) -18.486821\n",
      "best mean reward -18.486821\n",
      "running time 183.848415\n",
      "Train_EnvstepsSoFar : 121001\n",
      "Train_AverageReturn : -18.486820972387257\n",
      "Train_BestReturn : -18.486820972387257\n",
      "TimeSinceStart : 183.84841513633728\n",
      "Training Loss : 0.14170366525650024\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 122001\n",
      "mean reward (100 episodes) -17.039893\n",
      "best mean reward -17.039893\n",
      "running time 185.267236\n",
      "Train_EnvstepsSoFar : 122001\n",
      "Train_AverageReturn : -17.039893338843946\n",
      "Train_BestReturn : -17.039893338843946\n",
      "TimeSinceStart : 185.26723623275757\n",
      "Training Loss : 0.19692444801330566\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 123001\n",
      "mean reward (100 episodes) -16.570106\n",
      "best mean reward -16.570106\n",
      "running time 186.803836\n",
      "Train_EnvstepsSoFar : 123001\n",
      "Train_AverageReturn : -16.570106157856944\n",
      "Train_BestReturn : -16.570106157856944\n",
      "TimeSinceStart : 186.80383586883545\n",
      "Training Loss : 0.17514865100383759\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 124001\n",
      "mean reward (100 episodes) -15.851919\n",
      "best mean reward -15.851919\n",
      "running time 188.774185\n",
      "Train_EnvstepsSoFar : 124001\n",
      "Train_AverageReturn : -15.851919273066697\n",
      "Train_BestReturn : -15.851919273066697\n",
      "TimeSinceStart : 188.77418494224548\n",
      "Training Loss : 0.12699010968208313\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 125001\n",
      "mean reward (100 episodes) -13.026878\n",
      "best mean reward -13.026878\n",
      "running time 190.205618\n",
      "Train_EnvstepsSoFar : 125001\n",
      "Train_AverageReturn : -13.026877938265514\n",
      "Train_BestReturn : -13.026877938265514\n",
      "TimeSinceStart : 190.2056179046631\n",
      "Training Loss : 0.046937230974435806\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 126001\n",
      "mean reward (100 episodes) -11.411457\n",
      "best mean reward -11.411457\n",
      "running time 192.108995\n",
      "Train_EnvstepsSoFar : 126001\n",
      "Train_AverageReturn : -11.411457173986655\n",
      "Train_BestReturn : -11.411457173986655\n",
      "TimeSinceStart : 192.1089951992035\n",
      "Training Loss : 0.13603433966636658\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 127001\n",
      "mean reward (100 episodes) -6.828678\n",
      "best mean reward -6.828678\n",
      "running time 193.552207\n",
      "Train_EnvstepsSoFar : 127001\n",
      "Train_AverageReturn : -6.828677901671852\n",
      "Train_BestReturn : -6.828677901671852\n",
      "TimeSinceStart : 193.5522072315216\n",
      "Training Loss : 0.0986640527844429\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 128001\n",
      "mean reward (100 episodes) -4.101661\n",
      "best mean reward -4.101661\n",
      "running time 195.564832\n",
      "Train_EnvstepsSoFar : 128001\n",
      "Train_AverageReturn : -4.10166092792767\n",
      "Train_BestReturn : -4.10166092792767\n",
      "TimeSinceStart : 195.5648319721222\n",
      "Training Loss : 0.20547789335250854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 129001\n",
      "mean reward (100 episodes) -6.540553\n",
      "best mean reward -4.101661\n",
      "running time 196.876177\n",
      "Train_EnvstepsSoFar : 129001\n",
      "Train_AverageReturn : -6.540553066249027\n",
      "Train_BestReturn : -4.10166092792767\n",
      "TimeSinceStart : 196.87617707252502\n",
      "Training Loss : 0.23758141696453094\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -4.371817\n",
      "best mean reward -4.101661\n",
      "running time 198.446303\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -4.371816964295403\n",
      "Train_BestReturn : -4.10166092792767\n",
      "TimeSinceStart : 198.44630312919617\n",
      "Training Loss : 0.19216139614582062\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 131001\n",
      "mean reward (100 episodes) -0.922771\n",
      "best mean reward -0.922771\n",
      "running time 199.912955\n",
      "Train_EnvstepsSoFar : 131001\n",
      "Train_AverageReturn : -0.9227711200560162\n",
      "Train_BestReturn : -0.9227711200560162\n",
      "TimeSinceStart : 199.91295504570007\n",
      "Training Loss : 0.09819234907627106\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 132001\n",
      "mean reward (100 episodes) 3.876358\n",
      "best mean reward 3.876358\n",
      "running time 201.333323\n",
      "Train_EnvstepsSoFar : 132001\n",
      "Train_AverageReturn : 3.8763578655639934\n",
      "Train_BestReturn : 3.8763578655639934\n",
      "TimeSinceStart : 201.33332300186157\n",
      "Training Loss : 0.19001273810863495\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 133001\n",
      "mean reward (100 episodes) 11.085600\n",
      "best mean reward 11.085600\n",
      "running time 202.400927\n",
      "Train_EnvstepsSoFar : 133001\n",
      "Train_AverageReturn : 11.085599991043816\n",
      "Train_BestReturn : 11.085599991043816\n",
      "TimeSinceStart : 202.40092706680298\n",
      "Training Loss : 1.2351069450378418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 134001\n",
      "mean reward (100 episodes) 16.517442\n",
      "best mean reward 16.517442\n",
      "running time 203.520840\n",
      "Train_EnvstepsSoFar : 134001\n",
      "Train_AverageReturn : 16.517441614008913\n",
      "Train_BestReturn : 16.517441614008913\n",
      "TimeSinceStart : 203.5208399295807\n",
      "Training Loss : 0.1255994737148285\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 135001\n",
      "mean reward (100 episodes) 18.817235\n",
      "best mean reward 18.817235\n",
      "running time 204.756726\n",
      "Train_EnvstepsSoFar : 135001\n",
      "Train_AverageReturn : 18.81723549306499\n",
      "Train_BestReturn : 18.81723549306499\n",
      "TimeSinceStart : 204.75672602653503\n",
      "Training Loss : 0.2867494821548462\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 136001\n",
      "mean reward (100 episodes) 17.863463\n",
      "best mean reward 18.817235\n",
      "running time 205.938230\n",
      "Train_EnvstepsSoFar : 136001\n",
      "Train_AverageReturn : 17.86346306805271\n",
      "Train_BestReturn : 18.81723549306499\n",
      "TimeSinceStart : 205.9382300376892\n",
      "Training Loss : 0.6119043827056885\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 137001\n",
      "mean reward (100 episodes) 20.402434\n",
      "best mean reward 20.402434\n",
      "running time 207.308038\n",
      "Train_EnvstepsSoFar : 137001\n",
      "Train_AverageReturn : 20.402434442908152\n",
      "Train_BestReturn : 20.402434442908152\n",
      "TimeSinceStart : 207.30803799629211\n",
      "Training Loss : 0.36086905002593994\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 138001\n",
      "mean reward (100 episodes) 24.831912\n",
      "best mean reward 24.831912\n",
      "running time 208.196563\n",
      "Train_EnvstepsSoFar : 138001\n",
      "Train_AverageReturn : 24.831912467750854\n",
      "Train_BestReturn : 24.831912467750854\n",
      "TimeSinceStart : 208.1965630054474\n",
      "Training Loss : 3.296973943710327\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 139001\n",
      "mean reward (100 episodes) 27.513241\n",
      "best mean reward 27.513241\n",
      "running time 209.565414\n",
      "Train_EnvstepsSoFar : 139001\n",
      "Train_AverageReturn : 27.513241423016407\n",
      "Train_BestReturn : 27.513241423016407\n",
      "TimeSinceStart : 209.56541395187378\n",
      "Training Loss : 0.039297983050346375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 27.261824\n",
      "best mean reward 27.513241\n",
      "running time 211.536084\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 27.261823778171724\n",
      "Train_BestReturn : 27.513241423016407\n",
      "TimeSinceStart : 211.53608393669128\n",
      "Training Loss : 0.16282859444618225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 141001\n",
      "mean reward (100 episodes) 28.216105\n",
      "best mean reward 28.216105\n",
      "running time 213.475438\n",
      "Train_EnvstepsSoFar : 141001\n",
      "Train_AverageReturn : 28.216104962563737\n",
      "Train_BestReturn : 28.216104962563737\n",
      "TimeSinceStart : 213.47543811798096\n",
      "Training Loss : 0.3332241177558899\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 142001\n",
      "mean reward (100 episodes) 31.154726\n",
      "best mean reward 31.154726\n",
      "running time 214.958096\n",
      "Train_EnvstepsSoFar : 142001\n",
      "Train_AverageReturn : 31.15472638961798\n",
      "Train_BestReturn : 31.15472638961798\n",
      "TimeSinceStart : 214.95809602737427\n",
      "Training Loss : 0.19561028480529785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 143001\n",
      "mean reward (100 episodes) 35.084878\n",
      "best mean reward 35.084878\n",
      "running time 216.039043\n",
      "Train_EnvstepsSoFar : 143001\n",
      "Train_AverageReturn : 35.08487779976247\n",
      "Train_BestReturn : 35.08487779976247\n",
      "TimeSinceStart : 216.0390431880951\n",
      "Training Loss : 0.15465092658996582\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 144001\n",
      "mean reward (100 episodes) 32.857917\n",
      "best mean reward 35.084878\n",
      "running time 217.626162\n",
      "Train_EnvstepsSoFar : 144001\n",
      "Train_AverageReturn : 32.85791670489837\n",
      "Train_BestReturn : 35.08487779976247\n",
      "TimeSinceStart : 217.62616205215454\n",
      "Training Loss : 0.2789013087749481\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 145001\n",
      "mean reward (100 episodes) 34.343549\n",
      "best mean reward 35.084878\n",
      "running time 218.655915\n",
      "Train_EnvstepsSoFar : 145001\n",
      "Train_AverageReturn : 34.34354913792924\n",
      "Train_BestReturn : 35.08487779976247\n",
      "TimeSinceStart : 218.65591502189636\n",
      "Training Loss : 0.24317103624343872\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 146001\n",
      "mean reward (100 episodes) 33.103854\n",
      "best mean reward 35.084878\n",
      "running time 220.067038\n",
      "Train_EnvstepsSoFar : 146001\n",
      "Train_AverageReturn : 33.10385449964045\n",
      "Train_BestReturn : 35.08487779976247\n",
      "TimeSinceStart : 220.06703805923462\n",
      "Training Loss : 0.8016019463539124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 147001\n",
      "mean reward (100 episodes) 36.398371\n",
      "best mean reward 36.398371\n",
      "running time 221.519578\n",
      "Train_EnvstepsSoFar : 147001\n",
      "Train_AverageReturn : 36.398370602321876\n",
      "Train_BestReturn : 36.398370602321876\n",
      "TimeSinceStart : 221.51957821846008\n",
      "Training Loss : 0.10004650056362152\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 148001\n",
      "mean reward (100 episodes) 41.071849\n",
      "best mean reward 41.071849\n",
      "running time 222.646066\n",
      "Train_EnvstepsSoFar : 148001\n",
      "Train_AverageReturn : 41.071849303122626\n",
      "Train_BestReturn : 41.071849303122626\n",
      "TimeSinceStart : 222.64606595039368\n",
      "Training Loss : 0.11565229296684265\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 149001\n",
      "mean reward (100 episodes) 41.659961\n",
      "best mean reward 41.659961\n",
      "running time 223.960708\n",
      "Train_EnvstepsSoFar : 149001\n",
      "Train_AverageReturn : 41.659961177294115\n",
      "Train_BestReturn : 41.659961177294115\n",
      "TimeSinceStart : 223.9607081413269\n",
      "Training Loss : 0.11316356807947159\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 42.748871\n",
      "best mean reward 42.748871\n",
      "running time 225.430785\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 42.74887080821831\n",
      "Train_BestReturn : 42.74887080821831\n",
      "TimeSinceStart : 225.43078517913818\n",
      "Training Loss : 0.7452794313430786\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 151001\n",
      "mean reward (100 episodes) 43.270650\n",
      "best mean reward 43.270650\n",
      "running time 226.490622\n",
      "Train_EnvstepsSoFar : 151001\n",
      "Train_AverageReturn : 43.27065024701862\n",
      "Train_BestReturn : 43.27065024701862\n",
      "TimeSinceStart : 226.49062204360962\n",
      "Training Loss : 0.21034060418605804\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 152001\n",
      "mean reward (100 episodes) 49.456333\n",
      "best mean reward 49.456333\n",
      "running time 227.406259\n",
      "Train_EnvstepsSoFar : 152001\n",
      "Train_AverageReturn : 49.45633345227137\n",
      "Train_BestReturn : 49.45633345227137\n",
      "TimeSinceStart : 227.406259059906\n",
      "Training Loss : 0.09417911618947983\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 153001\n",
      "mean reward (100 episodes) 50.956506\n",
      "best mean reward 50.956506\n",
      "running time 228.423680\n",
      "Train_EnvstepsSoFar : 153001\n",
      "Train_AverageReturn : 50.95650585497495\n",
      "Train_BestReturn : 50.95650585497495\n",
      "TimeSinceStart : 228.42368006706238\n",
      "Training Loss : 0.13300885260105133\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 154001\n",
      "mean reward (100 episodes) 51.401211\n",
      "best mean reward 51.401211\n",
      "running time 229.405081\n",
      "Train_EnvstepsSoFar : 154001\n",
      "Train_AverageReturn : 51.401211270842886\n",
      "Train_BestReturn : 51.401211270842886\n",
      "TimeSinceStart : 229.40508127212524\n",
      "Training Loss : 0.20472031831741333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 155001\n",
      "mean reward (100 episodes) 48.560556\n",
      "best mean reward 51.401211\n",
      "running time 230.597712\n",
      "Train_EnvstepsSoFar : 155001\n",
      "Train_AverageReturn : 48.56055639210539\n",
      "Train_BestReturn : 51.401211270842886\n",
      "TimeSinceStart : 230.5977120399475\n",
      "Training Loss : 0.222135528922081\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 156001\n",
      "mean reward (100 episodes) 50.846885\n",
      "best mean reward 51.401211\n",
      "running time 231.638049\n",
      "Train_EnvstepsSoFar : 156001\n",
      "Train_AverageReturn : 50.84688513417859\n",
      "Train_BestReturn : 51.401211270842886\n",
      "TimeSinceStart : 231.6380491256714\n",
      "Training Loss : 0.0703740119934082\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 157001\n",
      "mean reward (100 episodes) 47.687976\n",
      "best mean reward 51.401211\n",
      "running time 232.931517\n",
      "Train_EnvstepsSoFar : 157001\n",
      "Train_AverageReturn : 47.68797578330756\n",
      "Train_BestReturn : 51.401211270842886\n",
      "TimeSinceStart : 232.93151688575745\n",
      "Training Loss : 0.10539712011814117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 158001\n",
      "mean reward (100 episodes) 49.850668\n",
      "best mean reward 51.401211\n",
      "running time 234.456383\n",
      "Train_EnvstepsSoFar : 158001\n",
      "Train_AverageReturn : 49.85066817707204\n",
      "Train_BestReturn : 51.401211270842886\n",
      "TimeSinceStart : 234.45638298988342\n",
      "Training Loss : 1.684402346611023\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 159001\n",
      "mean reward (100 episodes) 57.863424\n",
      "best mean reward 57.863424\n",
      "running time 235.409957\n",
      "Train_EnvstepsSoFar : 159001\n",
      "Train_AverageReturn : 57.863424477560386\n",
      "Train_BestReturn : 57.863424477560386\n",
      "TimeSinceStart : 235.40995693206787\n",
      "Training Loss : 0.06014222279191017\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 61.929560\n",
      "best mean reward 61.929560\n",
      "running time 236.374938\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 61.9295603927877\n",
      "Train_BestReturn : 61.9295603927877\n",
      "TimeSinceStart : 236.374938249588\n",
      "Training Loss : 2.2752037048339844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 161001\n",
      "mean reward (100 episodes) 60.804104\n",
      "best mean reward 61.929560\n",
      "running time 237.735050\n",
      "Train_EnvstepsSoFar : 161001\n",
      "Train_AverageReturn : 60.80410389925097\n",
      "Train_BestReturn : 61.9295603927877\n",
      "TimeSinceStart : 237.73504996299744\n",
      "Training Loss : 0.127881720662117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 162001\n",
      "mean reward (100 episodes) 61.320201\n",
      "best mean reward 61.929560\n",
      "running time 238.712523\n",
      "Train_EnvstepsSoFar : 162001\n",
      "Train_AverageReturn : 61.32020057385034\n",
      "Train_BestReturn : 61.9295603927877\n",
      "TimeSinceStart : 238.71252298355103\n",
      "Training Loss : 0.4235977828502655\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 163001\n",
      "mean reward (100 episodes) 63.933926\n",
      "best mean reward 63.933926\n",
      "running time 240.286236\n",
      "Train_EnvstepsSoFar : 163001\n",
      "Train_AverageReturn : 63.933926367146896\n",
      "Train_BestReturn : 63.933926367146896\n",
      "TimeSinceStart : 240.28623604774475\n",
      "Training Loss : 1.5713765621185303\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 164001\n",
      "mean reward (100 episodes) 66.198217\n",
      "best mean reward 66.198217\n",
      "running time 241.574991\n",
      "Train_EnvstepsSoFar : 164001\n",
      "Train_AverageReturn : 66.1982167795356\n",
      "Train_BestReturn : 66.1982167795356\n",
      "TimeSinceStart : 241.5749912261963\n",
      "Training Loss : 0.08894559741020203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 165001\n",
      "mean reward (100 episodes) 65.246102\n",
      "best mean reward 66.198217\n",
      "running time 244.745915\n",
      "Train_EnvstepsSoFar : 165001\n",
      "Train_AverageReturn : 65.24610221760821\n",
      "Train_BestReturn : 66.1982167795356\n",
      "TimeSinceStart : 244.74591493606567\n",
      "Training Loss : 0.15290246903896332\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 166001\n",
      "mean reward (100 episodes) 65.808145\n",
      "best mean reward 66.198217\n",
      "running time 246.694880\n",
      "Train_EnvstepsSoFar : 166001\n",
      "Train_AverageReturn : 65.80814504917922\n",
      "Train_BestReturn : 66.1982167795356\n",
      "TimeSinceStart : 246.6948800086975\n",
      "Training Loss : 0.719758927822113\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 167001\n",
      "mean reward (100 episodes) 65.382387\n",
      "best mean reward 66.198217\n",
      "running time 248.461598\n",
      "Train_EnvstepsSoFar : 167001\n",
      "Train_AverageReturn : 65.38238721818259\n",
      "Train_BestReturn : 66.1982167795356\n",
      "TimeSinceStart : 248.4615979194641\n",
      "Training Loss : 0.4826691746711731\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 168001\n",
      "mean reward (100 episodes) 64.539371\n",
      "best mean reward 66.198217\n",
      "running time 249.950629\n",
      "Train_EnvstepsSoFar : 168001\n",
      "Train_AverageReturn : 64.53937090387221\n",
      "Train_BestReturn : 66.1982167795356\n",
      "TimeSinceStart : 249.95062899589539\n",
      "Training Loss : 0.17411799728870392\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 169001\n",
      "mean reward (100 episodes) 68.742360\n",
      "best mean reward 68.742360\n",
      "running time 250.790300\n",
      "Train_EnvstepsSoFar : 169001\n",
      "Train_AverageReturn : 68.74235964923302\n",
      "Train_BestReturn : 68.74235964923302\n",
      "TimeSinceStart : 250.79029989242554\n",
      "Training Loss : 2.888514757156372\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 69.326493\n",
      "best mean reward 69.326493\n",
      "running time 251.914808\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 69.32649341609398\n",
      "Train_BestReturn : 69.32649341609398\n",
      "TimeSinceStart : 251.91480803489685\n",
      "Training Loss : 0.25273990631103516\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 171001\n",
      "mean reward (100 episodes) 70.897441\n",
      "best mean reward 70.897441\n",
      "running time 253.447395\n",
      "Train_EnvstepsSoFar : 171001\n",
      "Train_AverageReturn : 70.89744136263063\n",
      "Train_BestReturn : 70.89744136263063\n",
      "TimeSinceStart : 253.44739508628845\n",
      "Training Loss : 0.7040413618087769\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 172001\n",
      "mean reward (100 episodes) 70.200236\n",
      "best mean reward 70.897441\n",
      "running time 254.569546\n",
      "Train_EnvstepsSoFar : 172001\n",
      "Train_AverageReturn : 70.20023616609316\n",
      "Train_BestReturn : 70.89744136263063\n",
      "TimeSinceStart : 254.5695459842682\n",
      "Training Loss : 2.0547914505004883\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 173001\n",
      "mean reward (100 episodes) 71.610822\n",
      "best mean reward 71.610822\n",
      "running time 255.679130\n",
      "Train_EnvstepsSoFar : 173001\n",
      "Train_AverageReturn : 71.61082206123295\n",
      "Train_BestReturn : 71.61082206123295\n",
      "TimeSinceStart : 255.67913007736206\n",
      "Training Loss : 0.20885007083415985\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 174001\n",
      "mean reward (100 episodes) 76.448487\n",
      "best mean reward 76.448487\n",
      "running time 257.179388\n",
      "Train_EnvstepsSoFar : 174001\n",
      "Train_AverageReturn : 76.4484868597561\n",
      "Train_BestReturn : 76.4484868597561\n",
      "TimeSinceStart : 257.17938804626465\n",
      "Training Loss : 0.5135133862495422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 175001\n",
      "mean reward (100 episodes) 77.124870\n",
      "best mean reward 77.124870\n",
      "running time 258.729885\n",
      "Train_EnvstepsSoFar : 175001\n",
      "Train_AverageReturn : 77.12487027726884\n",
      "Train_BestReturn : 77.12487027726884\n",
      "TimeSinceStart : 258.72988510131836\n",
      "Training Loss : 2.929476737976074\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 176001\n",
      "mean reward (100 episodes) 79.898125\n",
      "best mean reward 79.898125\n",
      "running time 259.955031\n",
      "Train_EnvstepsSoFar : 176001\n",
      "Train_AverageReturn : 79.89812506696563\n",
      "Train_BestReturn : 79.89812506696563\n",
      "TimeSinceStart : 259.9550311565399\n",
      "Training Loss : 0.15936459600925446\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 177001\n",
      "mean reward (100 episodes) 78.565608\n",
      "best mean reward 79.898125\n",
      "running time 261.192532\n",
      "Train_EnvstepsSoFar : 177001\n",
      "Train_AverageReturn : 78.56560827842773\n",
      "Train_BestReturn : 79.89812506696563\n",
      "TimeSinceStart : 261.1925320625305\n",
      "Training Loss : 0.17890609800815582\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 178001\n",
      "mean reward (100 episodes) 77.347590\n",
      "best mean reward 79.898125\n",
      "running time 262.695704\n",
      "Train_EnvstepsSoFar : 178001\n",
      "Train_AverageReturn : 77.34759044768924\n",
      "Train_BestReturn : 79.89812506696563\n",
      "TimeSinceStart : 262.69570422172546\n",
      "Training Loss : 0.09171626716852188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 179001\n",
      "mean reward (100 episodes) 83.478324\n",
      "best mean reward 83.478324\n",
      "running time 263.807368\n",
      "Train_EnvstepsSoFar : 179001\n",
      "Train_AverageReturn : 83.47832364367795\n",
      "Train_BestReturn : 83.47832364367795\n",
      "TimeSinceStart : 263.80736804008484\n",
      "Training Loss : 0.7756146788597107\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 81.028646\n",
      "best mean reward 83.478324\n",
      "running time 265.454117\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 81.02864607438472\n",
      "Train_BestReturn : 83.47832364367795\n",
      "TimeSinceStart : 265.45411682128906\n",
      "Training Loss : 0.2262202799320221\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 181001\n",
      "mean reward (100 episodes) 80.392842\n",
      "best mean reward 83.478324\n",
      "running time 266.518462\n",
      "Train_EnvstepsSoFar : 181001\n",
      "Train_AverageReturn : 80.39284182989343\n",
      "Train_BestReturn : 83.47832364367795\n",
      "TimeSinceStart : 266.51846194267273\n",
      "Training Loss : 0.2501804530620575\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 182001\n",
      "mean reward (100 episodes) 79.964996\n",
      "best mean reward 83.478324\n",
      "running time 267.532729\n",
      "Train_EnvstepsSoFar : 182001\n",
      "Train_AverageReturn : 79.96499577396101\n",
      "Train_BestReturn : 83.47832364367795\n",
      "TimeSinceStart : 267.53272914886475\n",
      "Training Loss : 4.095302104949951\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 183001\n",
      "mean reward (100 episodes) 80.570719\n",
      "best mean reward 83.478324\n",
      "running time 269.545724\n",
      "Train_EnvstepsSoFar : 183001\n",
      "Train_AverageReturn : 80.57071875152205\n",
      "Train_BestReturn : 83.47832364367795\n",
      "TimeSinceStart : 269.5457239151001\n",
      "Training Loss : 0.05612688139081001\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 184001\n",
      "mean reward (100 episodes) 79.029571\n",
      "best mean reward 83.478324\n",
      "running time 270.729574\n",
      "Train_EnvstepsSoFar : 184001\n",
      "Train_AverageReturn : 79.02957080702333\n",
      "Train_BestReturn : 83.47832364367795\n",
      "TimeSinceStart : 270.72957396507263\n",
      "Training Loss : 0.16654354333877563\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 185001\n",
      "mean reward (100 episodes) 77.181035\n",
      "best mean reward 83.478324\n",
      "running time 271.991383\n",
      "Train_EnvstepsSoFar : 185001\n",
      "Train_AverageReturn : 77.18103521129142\n",
      "Train_BestReturn : 83.47832364367795\n",
      "TimeSinceStart : 271.9913830757141\n",
      "Training Loss : 0.25713738799095154\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 186001\n",
      "mean reward (100 episodes) 79.176948\n",
      "best mean reward 83.478324\n",
      "running time 272.870410\n",
      "Train_EnvstepsSoFar : 186001\n",
      "Train_AverageReturn : 79.17694758040152\n",
      "Train_BestReturn : 83.47832364367795\n",
      "TimeSinceStart : 272.87040996551514\n",
      "Training Loss : 0.4308200180530548\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 187001\n",
      "mean reward (100 episodes) 79.772519\n",
      "best mean reward 83.478324\n",
      "running time 274.005781\n",
      "Train_EnvstepsSoFar : 187001\n",
      "Train_AverageReturn : 79.77251894410686\n",
      "Train_BestReturn : 83.47832364367795\n",
      "TimeSinceStart : 274.00578117370605\n",
      "Training Loss : 0.7862021327018738\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 188001\n",
      "mean reward (100 episodes) 83.970082\n",
      "best mean reward 83.970082\n",
      "running time 275.343006\n",
      "Train_EnvstepsSoFar : 188001\n",
      "Train_AverageReturn : 83.97008227676614\n",
      "Train_BestReturn : 83.97008227676614\n",
      "TimeSinceStart : 275.3430061340332\n",
      "Training Loss : 0.21706905961036682\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 189001\n",
      "mean reward (100 episodes) 82.724332\n",
      "best mean reward 83.970082\n",
      "running time 277.364637\n",
      "Train_EnvstepsSoFar : 189001\n",
      "Train_AverageReturn : 82.72433239439331\n",
      "Train_BestReturn : 83.97008227676614\n",
      "TimeSinceStart : 277.36463689804077\n",
      "Training Loss : 0.6268960237503052\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 84.175691\n",
      "best mean reward 84.175691\n",
      "running time 279.030476\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 84.1756914375351\n",
      "Train_BestReturn : 84.1756914375351\n",
      "TimeSinceStart : 279.03047609329224\n",
      "Training Loss : 1.1775307655334473\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 191001\n",
      "mean reward (100 episodes) 86.517095\n",
      "best mean reward 86.517095\n",
      "running time 280.247828\n",
      "Train_EnvstepsSoFar : 191001\n",
      "Train_AverageReturn : 86.51709548437681\n",
      "Train_BestReturn : 86.51709548437681\n",
      "TimeSinceStart : 280.2478280067444\n",
      "Training Loss : 0.32396045327186584\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 192001\n",
      "mean reward (100 episodes) 85.928195\n",
      "best mean reward 86.517095\n",
      "running time 281.283434\n",
      "Train_EnvstepsSoFar : 192001\n",
      "Train_AverageReturn : 85.9281951869316\n",
      "Train_BestReturn : 86.51709548437681\n",
      "TimeSinceStart : 281.28343415260315\n",
      "Training Loss : 1.1128630638122559\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 193001\n",
      "mean reward (100 episodes) 84.790305\n",
      "best mean reward 86.517095\n",
      "running time 283.040464\n",
      "Train_EnvstepsSoFar : 193001\n",
      "Train_AverageReturn : 84.79030483003967\n",
      "Train_BestReturn : 86.51709548437681\n",
      "TimeSinceStart : 283.04046392440796\n",
      "Training Loss : 3.249412775039673\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 194001\n",
      "mean reward (100 episodes) 83.768054\n",
      "best mean reward 86.517095\n",
      "running time 284.833892\n",
      "Train_EnvstepsSoFar : 194001\n",
      "Train_AverageReturn : 83.76805437531746\n",
      "Train_BestReturn : 86.51709548437681\n",
      "TimeSinceStart : 284.8338921070099\n",
      "Training Loss : 0.13425005972385406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 195001\n",
      "mean reward (100 episodes) 86.849874\n",
      "best mean reward 86.849874\n",
      "running time 286.910114\n",
      "Train_EnvstepsSoFar : 195001\n",
      "Train_AverageReturn : 86.84987433194395\n",
      "Train_BestReturn : 86.84987433194395\n",
      "TimeSinceStart : 286.9101140499115\n",
      "Training Loss : 0.8277413249015808\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 196001\n",
      "mean reward (100 episodes) 88.020502\n",
      "best mean reward 88.020502\n",
      "running time 288.429315\n",
      "Train_EnvstepsSoFar : 196001\n",
      "Train_AverageReturn : 88.02050218555173\n",
      "Train_BestReturn : 88.02050218555173\n",
      "TimeSinceStart : 288.42931509017944\n",
      "Training Loss : 0.5674149394035339\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 197001\n",
      "mean reward (100 episodes) 90.673039\n",
      "best mean reward 90.673039\n",
      "running time 289.529017\n",
      "Train_EnvstepsSoFar : 197001\n",
      "Train_AverageReturn : 90.67303862476592\n",
      "Train_BestReturn : 90.67303862476592\n",
      "TimeSinceStart : 289.5290172100067\n",
      "Training Loss : 3.2169981002807617\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 198001\n",
      "mean reward (100 episodes) 92.293058\n",
      "best mean reward 92.293058\n",
      "running time 290.475488\n",
      "Train_EnvstepsSoFar : 198001\n",
      "Train_AverageReturn : 92.29305847971065\n",
      "Train_BestReturn : 92.29305847971065\n",
      "TimeSinceStart : 290.475487947464\n",
      "Training Loss : 1.9698981046676636\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 199001\n",
      "mean reward (100 episodes) 92.848709\n",
      "best mean reward 92.848709\n",
      "running time 291.515630\n",
      "Train_EnvstepsSoFar : 199001\n",
      "Train_AverageReturn : 92.84870858985883\n",
      "Train_BestReturn : 92.84870858985883\n",
      "TimeSinceStart : 291.51563024520874\n",
      "Training Loss : 0.602810800075531\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 95.818168\n",
      "best mean reward 95.818168\n",
      "running time 292.540735\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 95.81816767999368\n",
      "Train_BestReturn : 95.81816767999368\n",
      "TimeSinceStart : 292.5407350063324\n",
      "Training Loss : 3.3591535091400146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 201001\n",
      "mean reward (100 episodes) 97.564936\n",
      "best mean reward 97.564936\n",
      "running time 293.920707\n",
      "Train_EnvstepsSoFar : 201001\n",
      "Train_AverageReturn : 97.56493560687971\n",
      "Train_BestReturn : 97.56493560687971\n",
      "TimeSinceStart : 293.92070722579956\n",
      "Training Loss : 0.10346796363592148\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 202001\n",
      "mean reward (100 episodes) 96.395052\n",
      "best mean reward 97.564936\n",
      "running time 294.944469\n",
      "Train_EnvstepsSoFar : 202001\n",
      "Train_AverageReturn : 96.3950518139761\n",
      "Train_BestReturn : 97.56493560687971\n",
      "TimeSinceStart : 294.94446897506714\n",
      "Training Loss : 1.8601170778274536\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 203001\n",
      "mean reward (100 episodes) 95.421652\n",
      "best mean reward 97.564936\n",
      "running time 296.085838\n",
      "Train_EnvstepsSoFar : 203001\n",
      "Train_AverageReturn : 95.42165211584371\n",
      "Train_BestReturn : 97.56493560687971\n",
      "TimeSinceStart : 296.0858380794525\n",
      "Training Loss : 2.2431135177612305\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 204001\n",
      "mean reward (100 episodes) 98.194537\n",
      "best mean reward 98.194537\n",
      "running time 296.932015\n",
      "Train_EnvstepsSoFar : 204001\n",
      "Train_AverageReturn : 98.1945372305695\n",
      "Train_BestReturn : 98.1945372305695\n",
      "TimeSinceStart : 296.93201518058777\n",
      "Training Loss : 2.0036723613739014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 205001\n",
      "mean reward (100 episodes) 98.900829\n",
      "best mean reward 98.900829\n",
      "running time 298.264874\n",
      "Train_EnvstepsSoFar : 205001\n",
      "Train_AverageReturn : 98.90082904443639\n",
      "Train_BestReturn : 98.90082904443639\n",
      "TimeSinceStart : 298.26487398147583\n",
      "Training Loss : 0.32886719703674316\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 206001\n",
      "mean reward (100 episodes) 107.751362\n",
      "best mean reward 107.751362\n",
      "running time 299.210035\n",
      "Train_EnvstepsSoFar : 206001\n",
      "Train_AverageReturn : 107.75136169393124\n",
      "Train_BestReturn : 107.75136169393124\n",
      "TimeSinceStart : 299.2100350856781\n",
      "Training Loss : 2.0561041831970215\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 207001\n",
      "mean reward (100 episodes) 108.619435\n",
      "best mean reward 108.619435\n",
      "running time 300.274644\n",
      "Train_EnvstepsSoFar : 207001\n",
      "Train_AverageReturn : 108.61943450290208\n",
      "Train_BestReturn : 108.61943450290208\n",
      "TimeSinceStart : 300.27464413642883\n",
      "Training Loss : 0.2915979325771332\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 208001\n",
      "mean reward (100 episodes) 106.345796\n",
      "best mean reward 108.619435\n",
      "running time 301.397867\n",
      "Train_EnvstepsSoFar : 208001\n",
      "Train_AverageReturn : 106.34579630317225\n",
      "Train_BestReturn : 108.61943450290208\n",
      "TimeSinceStart : 301.3978669643402\n",
      "Training Loss : 0.9450426697731018\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 209001\n",
      "mean reward (100 episodes) 106.370621\n",
      "best mean reward 108.619435\n",
      "running time 302.346386\n",
      "Train_EnvstepsSoFar : 209001\n",
      "Train_AverageReturn : 106.37062094116668\n",
      "Train_BestReturn : 108.61943450290208\n",
      "TimeSinceStart : 302.34638595581055\n",
      "Training Loss : 0.12665031850337982\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 108.908028\n",
      "best mean reward 108.908028\n",
      "running time 303.535838\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 108.90802774431931\n",
      "Train_BestReturn : 108.90802774431931\n",
      "TimeSinceStart : 303.53583812713623\n",
      "Training Loss : 0.6476582288742065\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 211001\n",
      "mean reward (100 episodes) 111.274876\n",
      "best mean reward 111.274876\n",
      "running time 304.412156\n",
      "Train_EnvstepsSoFar : 211001\n",
      "Train_AverageReturn : 111.27487601708522\n",
      "Train_BestReturn : 111.27487601708522\n",
      "TimeSinceStart : 304.4121558666229\n",
      "Training Loss : 2.221674680709839\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 212001\n",
      "mean reward (100 episodes) 115.683110\n",
      "best mean reward 115.683110\n",
      "running time 305.295163\n",
      "Train_EnvstepsSoFar : 212001\n",
      "Train_AverageReturn : 115.68311006458615\n",
      "Train_BestReturn : 115.68311006458615\n",
      "TimeSinceStart : 305.2951629161835\n",
      "Training Loss : 0.2388136237859726\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 213001\n",
      "mean reward (100 episodes) 118.896406\n",
      "best mean reward 118.896406\n",
      "running time 306.239584\n",
      "Train_EnvstepsSoFar : 213001\n",
      "Train_AverageReturn : 118.89640553254388\n",
      "Train_BestReturn : 118.89640553254388\n",
      "TimeSinceStart : 306.2395839691162\n",
      "Training Loss : 0.40618249773979187\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 214001\n",
      "mean reward (100 episodes) 119.044288\n",
      "best mean reward 119.044288\n",
      "running time 307.171532\n",
      "Train_EnvstepsSoFar : 214001\n",
      "Train_AverageReturn : 119.04428803688184\n",
      "Train_BestReturn : 119.04428803688184\n",
      "TimeSinceStart : 307.17153215408325\n",
      "Training Loss : 0.14628994464874268\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 215001\n",
      "mean reward (100 episodes) 119.756907\n",
      "best mean reward 119.756907\n",
      "running time 308.190470\n",
      "Train_EnvstepsSoFar : 215001\n",
      "Train_AverageReturn : 119.75690654379574\n",
      "Train_BestReturn : 119.75690654379574\n",
      "TimeSinceStart : 308.19046998023987\n",
      "Training Loss : 0.17728768289089203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 216001\n",
      "mean reward (100 episodes) 123.632237\n",
      "best mean reward 123.632237\n",
      "running time 309.260543\n",
      "Train_EnvstepsSoFar : 216001\n",
      "Train_AverageReturn : 123.63223721233356\n",
      "Train_BestReturn : 123.63223721233356\n",
      "TimeSinceStart : 309.26054310798645\n",
      "Training Loss : 1.5920238494873047\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 217001\n",
      "mean reward (100 episodes) 123.591817\n",
      "best mean reward 123.632237\n",
      "running time 310.245402\n",
      "Train_EnvstepsSoFar : 217001\n",
      "Train_AverageReturn : 123.59181659312878\n",
      "Train_BestReturn : 123.63223721233356\n",
      "TimeSinceStart : 310.245402097702\n",
      "Training Loss : 1.722204327583313\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 218001\n",
      "mean reward (100 episodes) 123.486196\n",
      "best mean reward 123.632237\n",
      "running time 311.145995\n",
      "Train_EnvstepsSoFar : 218001\n",
      "Train_AverageReturn : 123.48619611864797\n",
      "Train_BestReturn : 123.63223721233356\n",
      "TimeSinceStart : 311.1459951400757\n",
      "Training Loss : 0.06952706724405289\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 219001\n",
      "mean reward (100 episodes) 124.910333\n",
      "best mean reward 124.910333\n",
      "running time 312.851929\n",
      "Train_EnvstepsSoFar : 219001\n",
      "Train_AverageReturn : 124.91033281971778\n",
      "Train_BestReturn : 124.91033281971778\n",
      "TimeSinceStart : 312.85192918777466\n",
      "Training Loss : 0.2004193812608719\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 120.475068\n",
      "best mean reward 124.910333\n",
      "running time 313.766273\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 120.47506773293485\n",
      "Train_BestReturn : 124.91033281971778\n",
      "TimeSinceStart : 313.766273021698\n",
      "Training Loss : 0.5478079319000244\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 221001\n",
      "mean reward (100 episodes) 119.373269\n",
      "best mean reward 124.910333\n",
      "running time 314.887598\n",
      "Train_EnvstepsSoFar : 221001\n",
      "Train_AverageReturn : 119.37326851311288\n",
      "Train_BestReturn : 124.91033281971778\n",
      "TimeSinceStart : 314.8875980377197\n",
      "Training Loss : 0.1486806869506836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 222001\n",
      "mean reward (100 episodes) 119.209402\n",
      "best mean reward 124.910333\n",
      "running time 315.958684\n",
      "Train_EnvstepsSoFar : 222001\n",
      "Train_AverageReturn : 119.20940195002278\n",
      "Train_BestReturn : 124.91033281971778\n",
      "TimeSinceStart : 315.95868396759033\n",
      "Training Loss : 0.19763945043087006\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 223001\n",
      "mean reward (100 episodes) 120.284667\n",
      "best mean reward 124.910333\n",
      "running time 317.015342\n",
      "Train_EnvstepsSoFar : 223001\n",
      "Train_AverageReturn : 120.2846665003721\n",
      "Train_BestReturn : 124.91033281971778\n",
      "TimeSinceStart : 317.0153422355652\n",
      "Training Loss : 0.15034589171409607\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 224001\n",
      "mean reward (100 episodes) 121.089830\n",
      "best mean reward 124.910333\n",
      "running time 318.161605\n",
      "Train_EnvstepsSoFar : 224001\n",
      "Train_AverageReturn : 121.08982989595376\n",
      "Train_BestReturn : 124.91033281971778\n",
      "TimeSinceStart : 318.1616051197052\n",
      "Training Loss : 2.470921754837036\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 225001\n",
      "mean reward (100 episodes) 119.402005\n",
      "best mean reward 124.910333\n",
      "running time 320.049112\n",
      "Train_EnvstepsSoFar : 225001\n",
      "Train_AverageReturn : 119.40200480501284\n",
      "Train_BestReturn : 124.91033281971778\n",
      "TimeSinceStart : 320.0491120815277\n",
      "Training Loss : 0.8611655235290527\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 226001\n",
      "mean reward (100 episodes) 121.199102\n",
      "best mean reward 124.910333\n",
      "running time 320.969477\n",
      "Train_EnvstepsSoFar : 226001\n",
      "Train_AverageReturn : 121.19910154652787\n",
      "Train_BestReturn : 124.91033281971778\n",
      "TimeSinceStart : 320.96947717666626\n",
      "Training Loss : 0.09113122522830963\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 227001\n",
      "mean reward (100 episodes) 121.932066\n",
      "best mean reward 124.910333\n",
      "running time 322.159460\n",
      "Train_EnvstepsSoFar : 227001\n",
      "Train_AverageReturn : 121.93206622126209\n",
      "Train_BestReturn : 124.91033281971778\n",
      "TimeSinceStart : 322.159460067749\n",
      "Training Loss : 0.8423972725868225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 228001\n",
      "mean reward (100 episodes) 122.893871\n",
      "best mean reward 124.910333\n",
      "running time 323.121934\n",
      "Train_EnvstepsSoFar : 228001\n",
      "Train_AverageReturn : 122.89387132354865\n",
      "Train_BestReturn : 124.91033281971778\n",
      "TimeSinceStart : 323.12193417549133\n",
      "Training Loss : 0.4003131091594696\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 229001\n",
      "mean reward (100 episodes) 125.153324\n",
      "best mean reward 125.153324\n",
      "running time 324.280510\n",
      "Train_EnvstepsSoFar : 229001\n",
      "Train_AverageReturn : 125.15332353282429\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 324.28050994873047\n",
      "Training Loss : 2.274815559387207\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 123.233626\n",
      "best mean reward 125.153324\n",
      "running time 325.860836\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 123.23362632706895\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 325.8608362674713\n",
      "Training Loss : 1.089473843574524\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 231001\n",
      "mean reward (100 episodes) 120.320453\n",
      "best mean reward 125.153324\n",
      "running time 327.744678\n",
      "Train_EnvstepsSoFar : 231001\n",
      "Train_AverageReturn : 120.32045332572348\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 327.7446780204773\n",
      "Training Loss : 2.892181634902954\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 232001\n",
      "mean reward (100 episodes) 119.124156\n",
      "best mean reward 125.153324\n",
      "running time 329.317144\n",
      "Train_EnvstepsSoFar : 232001\n",
      "Train_AverageReturn : 119.12415604724198\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 329.3171441555023\n",
      "Training Loss : 2.141896963119507\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 233001\n",
      "mean reward (100 episodes) 116.646813\n",
      "best mean reward 125.153324\n",
      "running time 330.568875\n",
      "Train_EnvstepsSoFar : 233001\n",
      "Train_AverageReturn : 116.64681286993165\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 330.5688750743866\n",
      "Training Loss : 0.8904037475585938\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 234001\n",
      "mean reward (100 episodes) 119.605575\n",
      "best mean reward 125.153324\n",
      "running time 331.416330\n",
      "Train_EnvstepsSoFar : 234001\n",
      "Train_AverageReturn : 119.60557457410575\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 331.41633009910583\n",
      "Training Loss : 0.4399431049823761\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 235001\n",
      "mean reward (100 episodes) 118.524250\n",
      "best mean reward 125.153324\n",
      "running time 332.415973\n",
      "Train_EnvstepsSoFar : 235001\n",
      "Train_AverageReturn : 118.52425042478632\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 332.41597294807434\n",
      "Training Loss : 2.5917065143585205\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 236001\n",
      "mean reward (100 episodes) 118.698043\n",
      "best mean reward 125.153324\n",
      "running time 334.273458\n",
      "Train_EnvstepsSoFar : 236001\n",
      "Train_AverageReturn : 118.69804263282114\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 334.2734580039978\n",
      "Training Loss : 0.11840860545635223\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 237001\n",
      "mean reward (100 episodes) 118.196389\n",
      "best mean reward 125.153324\n",
      "running time 335.599206\n",
      "Train_EnvstepsSoFar : 237001\n",
      "Train_AverageReturn : 118.19638935917538\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 335.59920597076416\n",
      "Training Loss : 0.18543483316898346\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 238001\n",
      "mean reward (100 episodes) 120.931733\n",
      "best mean reward 125.153324\n",
      "running time 336.696442\n",
      "Train_EnvstepsSoFar : 238001\n",
      "Train_AverageReturn : 120.93173336340564\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 336.6964421272278\n",
      "Training Loss : 4.689846038818359\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 239001\n",
      "mean reward (100 episodes) 120.900288\n",
      "best mean reward 125.153324\n",
      "running time 338.162561\n",
      "Train_EnvstepsSoFar : 239001\n",
      "Train_AverageReturn : 120.90028778117313\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 338.1625611782074\n",
      "Training Loss : 1.4891629219055176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 118.765863\n",
      "best mean reward 125.153324\n",
      "running time 339.821371\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 118.76586267112556\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 339.82137084007263\n",
      "Training Loss : 1.0464783906936646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 241001\n",
      "mean reward (100 episodes) 121.317108\n",
      "best mean reward 125.153324\n",
      "running time 341.174074\n",
      "Train_EnvstepsSoFar : 241001\n",
      "Train_AverageReturn : 121.3171077514727\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 341.17407417297363\n",
      "Training Loss : 0.3934902846813202\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 242001\n",
      "mean reward (100 episodes) 120.644288\n",
      "best mean reward 125.153324\n",
      "running time 342.736393\n",
      "Train_EnvstepsSoFar : 242001\n",
      "Train_AverageReturn : 120.64428817030999\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 342.7363929748535\n",
      "Training Loss : 0.13378775119781494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 243001\n",
      "mean reward (100 episodes) 117.522092\n",
      "best mean reward 125.153324\n",
      "running time 344.735187\n",
      "Train_EnvstepsSoFar : 243001\n",
      "Train_AverageReturn : 117.52209207236928\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 344.7351870536804\n",
      "Training Loss : 4.232961654663086\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 244001\n",
      "mean reward (100 episodes) 118.347240\n",
      "best mean reward 125.153324\n",
      "running time 346.201175\n",
      "Train_EnvstepsSoFar : 244001\n",
      "Train_AverageReturn : 118.34724009254148\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 346.2011752128601\n",
      "Training Loss : 1.0488804578781128\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 245001\n",
      "mean reward (100 episodes) 116.831332\n",
      "best mean reward 125.153324\n",
      "running time 348.041946\n",
      "Train_EnvstepsSoFar : 245001\n",
      "Train_AverageReturn : 116.831332286067\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 348.04194617271423\n",
      "Training Loss : 0.1974693089723587\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 246001\n",
      "mean reward (100 episodes) 112.890357\n",
      "best mean reward 125.153324\n",
      "running time 349.340941\n",
      "Train_EnvstepsSoFar : 246001\n",
      "Train_AverageReturn : 112.8903574422533\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 349.3409411907196\n",
      "Training Loss : 1.7371805906295776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 247001\n",
      "mean reward (100 episodes) 111.769710\n",
      "best mean reward 125.153324\n",
      "running time 350.433350\n",
      "Train_EnvstepsSoFar : 247001\n",
      "Train_AverageReturn : 111.76970978910805\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 350.4333498477936\n",
      "Training Loss : 0.10108166933059692\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 248001\n",
      "mean reward (100 episodes) 115.737934\n",
      "best mean reward 125.153324\n",
      "running time 351.349746\n",
      "Train_EnvstepsSoFar : 248001\n",
      "Train_AverageReturn : 115.73793434135101\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 351.3497459888458\n",
      "Training Loss : 1.1440337896347046\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 249001\n",
      "mean reward (100 episodes) 115.006754\n",
      "best mean reward 125.153324\n",
      "running time 353.094843\n",
      "Train_EnvstepsSoFar : 249001\n",
      "Train_AverageReturn : 115.00675416187461\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 353.0948429107666\n",
      "Training Loss : 0.5416830778121948\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 114.544549\n",
      "best mean reward 125.153324\n",
      "running time 354.669959\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 114.54454895291897\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 354.66995906829834\n",
      "Training Loss : 0.23738275468349457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 251001\n",
      "mean reward (100 episodes) 106.537799\n",
      "best mean reward 125.153324\n",
      "running time 356.019371\n",
      "Train_EnvstepsSoFar : 251001\n",
      "Train_AverageReturn : 106.53779889918086\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 356.01937103271484\n",
      "Training Loss : 0.29207900166511536\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 252001\n",
      "mean reward (100 episodes) 105.248843\n",
      "best mean reward 125.153324\n",
      "running time 356.940431\n",
      "Train_EnvstepsSoFar : 252001\n",
      "Train_AverageReturn : 105.24884279704344\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 356.9404311180115\n",
      "Training Loss : 0.48615461587905884\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 253001\n",
      "mean reward (100 episodes) 105.311810\n",
      "best mean reward 125.153324\n",
      "running time 357.917734\n",
      "Train_EnvstepsSoFar : 253001\n",
      "Train_AverageReturn : 105.31181035325355\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 357.9177339076996\n",
      "Training Loss : 1.4685289859771729\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 254001\n",
      "mean reward (100 episodes) 106.876831\n",
      "best mean reward 125.153324\n",
      "running time 358.858647\n",
      "Train_EnvstepsSoFar : 254001\n",
      "Train_AverageReturn : 106.87683120875005\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 358.858647108078\n",
      "Training Loss : 1.6764321327209473\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 255001\n",
      "mean reward (100 episodes) 105.844722\n",
      "best mean reward 125.153324\n",
      "running time 360.247215\n",
      "Train_EnvstepsSoFar : 255001\n",
      "Train_AverageReturn : 105.84472185702458\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 360.2472152709961\n",
      "Training Loss : 0.2421027272939682\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 256001\n",
      "mean reward (100 episodes) 107.338437\n",
      "best mean reward 125.153324\n",
      "running time 361.081323\n",
      "Train_EnvstepsSoFar : 256001\n",
      "Train_AverageReturn : 107.3384371380074\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 361.08132314682007\n",
      "Training Loss : 0.18623319268226624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 257001\n",
      "mean reward (100 episodes) 108.306961\n",
      "best mean reward 125.153324\n",
      "running time 362.188224\n",
      "Train_EnvstepsSoFar : 257001\n",
      "Train_AverageReturn : 108.30696131206344\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 362.18822407722473\n",
      "Training Loss : 0.541606068611145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 258001\n",
      "mean reward (100 episodes) 108.169475\n",
      "best mean reward 125.153324\n",
      "running time 363.706154\n",
      "Train_EnvstepsSoFar : 258001\n",
      "Train_AverageReturn : 108.16947500470752\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 363.7061541080475\n",
      "Training Loss : 0.20231391489505768\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 259001\n",
      "mean reward (100 episodes) 105.249052\n",
      "best mean reward 125.153324\n",
      "running time 365.436709\n",
      "Train_EnvstepsSoFar : 259001\n",
      "Train_AverageReturn : 105.24905233919559\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 365.43670892715454\n",
      "Training Loss : 0.3345590829849243\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 103.151499\n",
      "best mean reward 125.153324\n",
      "running time 366.329608\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 103.15149887505103\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 366.329607963562\n",
      "Training Loss : 0.6723219156265259\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 261001\n",
      "mean reward (100 episodes) 103.302378\n",
      "best mean reward 125.153324\n",
      "running time 368.082628\n",
      "Train_EnvstepsSoFar : 261001\n",
      "Train_AverageReturn : 103.3023778980236\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 368.08262825012207\n",
      "Training Loss : 0.5069105625152588\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 262001\n",
      "mean reward (100 episodes) 110.613836\n",
      "best mean reward 125.153324\n",
      "running time 369.584752\n",
      "Train_EnvstepsSoFar : 262001\n",
      "Train_AverageReturn : 110.61383638257743\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 369.5847518444061\n",
      "Training Loss : 0.0891522467136383\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 263001\n",
      "mean reward (100 episodes) 108.954776\n",
      "best mean reward 125.153324\n",
      "running time 371.026649\n",
      "Train_EnvstepsSoFar : 263001\n",
      "Train_AverageReturn : 108.95477643279905\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 371.0266492366791\n",
      "Training Loss : 0.11044121533632278\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 264001\n",
      "mean reward (100 episodes) 109.727000\n",
      "best mean reward 125.153324\n",
      "running time 371.994203\n",
      "Train_EnvstepsSoFar : 264001\n",
      "Train_AverageReturn : 109.72700029321732\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 371.99420285224915\n",
      "Training Loss : 0.09917040169239044\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 265001\n",
      "mean reward (100 episodes) 111.747105\n",
      "best mean reward 125.153324\n",
      "running time 372.962736\n",
      "Train_EnvstepsSoFar : 265001\n",
      "Train_AverageReturn : 111.74710466698221\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 372.96273589134216\n",
      "Training Loss : 0.4436144232749939\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 266001\n",
      "mean reward (100 episodes) 111.263595\n",
      "best mean reward 125.153324\n",
      "running time 373.956427\n",
      "Train_EnvstepsSoFar : 266001\n",
      "Train_AverageReturn : 111.2635952744607\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 373.95642709732056\n",
      "Training Loss : 0.07740630954504013\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 267001\n",
      "mean reward (100 episodes) 111.112066\n",
      "best mean reward 125.153324\n",
      "running time 374.901076\n",
      "Train_EnvstepsSoFar : 267001\n",
      "Train_AverageReturn : 111.11206592477991\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 374.9010760784149\n",
      "Training Loss : 0.3938830494880676\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 268001\n",
      "mean reward (100 episodes) 114.066707\n",
      "best mean reward 125.153324\n",
      "running time 375.793250\n",
      "Train_EnvstepsSoFar : 268001\n",
      "Train_AverageReturn : 114.06670695040211\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 375.79325008392334\n",
      "Training Loss : 0.2576631009578705\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 269001\n",
      "mean reward (100 episodes) 119.201782\n",
      "best mean reward 125.153324\n",
      "running time 376.561172\n",
      "Train_EnvstepsSoFar : 269001\n",
      "Train_AverageReturn : 119.20178199965416\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 376.5611720085144\n",
      "Training Loss : 0.42580386996269226\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 121.698357\n",
      "best mean reward 125.153324\n",
      "running time 377.527198\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 121.6983569495033\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 377.52719807624817\n",
      "Training Loss : 0.176873117685318\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 271001\n",
      "mean reward (100 episodes) 120.806286\n",
      "best mean reward 125.153324\n",
      "running time 379.224201\n",
      "Train_EnvstepsSoFar : 271001\n",
      "Train_AverageReturn : 120.80628631263457\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 379.224200963974\n",
      "Training Loss : 0.1320773810148239\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 272001\n",
      "mean reward (100 episodes) 120.413474\n",
      "best mean reward 125.153324\n",
      "running time 380.840240\n",
      "Train_EnvstepsSoFar : 272001\n",
      "Train_AverageReturn : 120.41347376216248\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 380.84024000167847\n",
      "Training Loss : 2.0621514320373535\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 273001\n",
      "mean reward (100 episodes) 123.430829\n",
      "best mean reward 125.153324\n",
      "running time 381.717858\n",
      "Train_EnvstepsSoFar : 273001\n",
      "Train_AverageReturn : 123.43082857653586\n",
      "Train_BestReturn : 125.15332353282429\n",
      "TimeSinceStart : 381.7178580760956\n",
      "Training Loss : 0.7402823567390442\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 274001\n",
      "mean reward (100 episodes) 126.606695\n",
      "best mean reward 126.606695\n",
      "running time 382.892426\n",
      "Train_EnvstepsSoFar : 274001\n",
      "Train_AverageReturn : 126.60669526907404\n",
      "Train_BestReturn : 126.60669526907404\n",
      "TimeSinceStart : 382.89242601394653\n",
      "Training Loss : 0.7363512516021729\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 275001\n",
      "mean reward (100 episodes) 125.447197\n",
      "best mean reward 126.606695\n",
      "running time 384.014209\n",
      "Train_EnvstepsSoFar : 275001\n",
      "Train_AverageReturn : 125.44719702858798\n",
      "Train_BestReturn : 126.60669526907404\n",
      "TimeSinceStart : 384.0142090320587\n",
      "Training Loss : 0.21003060042858124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 276001\n",
      "mean reward (100 episodes) 126.379156\n",
      "best mean reward 126.606695\n",
      "running time 385.117072\n",
      "Train_EnvstepsSoFar : 276001\n",
      "Train_AverageReturn : 126.37915563890874\n",
      "Train_BestReturn : 126.60669526907404\n",
      "TimeSinceStart : 385.11707186698914\n",
      "Training Loss : 2.7914645671844482\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 277001\n",
      "mean reward (100 episodes) 127.329203\n",
      "best mean reward 127.329203\n",
      "running time 386.047613\n",
      "Train_EnvstepsSoFar : 277001\n",
      "Train_AverageReturn : 127.32920260871238\n",
      "Train_BestReturn : 127.32920260871238\n",
      "TimeSinceStart : 386.0476129055023\n",
      "Training Loss : 0.1125340536236763\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 278001\n",
      "mean reward (100 episodes) 127.043946\n",
      "best mean reward 127.329203\n",
      "running time 386.889741\n",
      "Train_EnvstepsSoFar : 278001\n",
      "Train_AverageReturn : 127.04394559127343\n",
      "Train_BestReturn : 127.32920260871238\n",
      "TimeSinceStart : 386.88974118232727\n",
      "Training Loss : 0.3366524279117584\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 279001\n",
      "mean reward (100 episodes) 125.269179\n",
      "best mean reward 127.329203\n",
      "running time 388.599325\n",
      "Train_EnvstepsSoFar : 279001\n",
      "Train_AverageReturn : 125.26917865488556\n",
      "Train_BestReturn : 127.32920260871238\n",
      "TimeSinceStart : 388.59932494163513\n",
      "Training Loss : 2.4403250217437744\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 130.438034\n",
      "best mean reward 130.438034\n",
      "running time 389.511400\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 130.43803440250088\n",
      "Train_BestReturn : 130.43803440250088\n",
      "TimeSinceStart : 389.51139998435974\n",
      "Training Loss : 1.6508235931396484\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 281001\n",
      "mean reward (100 episodes) 131.530543\n",
      "best mean reward 131.530543\n",
      "running time 390.376691\n",
      "Train_EnvstepsSoFar : 281001\n",
      "Train_AverageReturn : 131.53054304689073\n",
      "Train_BestReturn : 131.53054304689073\n",
      "TimeSinceStart : 390.37669110298157\n",
      "Training Loss : 0.23565012216567993\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 282001\n",
      "mean reward (100 episodes) 131.984800\n",
      "best mean reward 131.984800\n",
      "running time 391.250305\n",
      "Train_EnvstepsSoFar : 282001\n",
      "Train_AverageReturn : 131.98480033237726\n",
      "Train_BestReturn : 131.98480033237726\n",
      "TimeSinceStart : 391.25030517578125\n",
      "Training Loss : 2.0954978466033936\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 283001\n",
      "mean reward (100 episodes) 136.129746\n",
      "best mean reward 136.129746\n",
      "running time 392.364579\n",
      "Train_EnvstepsSoFar : 283001\n",
      "Train_AverageReturn : 136.1297463777889\n",
      "Train_BestReturn : 136.1297463777889\n",
      "TimeSinceStart : 392.36457920074463\n",
      "Training Loss : 0.5580897331237793\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 284001\n",
      "mean reward (100 episodes) 139.765287\n",
      "best mean reward 139.765287\n",
      "running time 393.230537\n",
      "Train_EnvstepsSoFar : 284001\n",
      "Train_AverageReturn : 139.7652873129927\n",
      "Train_BestReturn : 139.7652873129927\n",
      "TimeSinceStart : 393.2305369377136\n",
      "Training Loss : 0.23183049261569977\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 285001\n",
      "mean reward (100 episodes) 139.462935\n",
      "best mean reward 139.765287\n",
      "running time 394.072773\n",
      "Train_EnvstepsSoFar : 285001\n",
      "Train_AverageReturn : 139.4629354225276\n",
      "Train_BestReturn : 139.7652873129927\n",
      "TimeSinceStart : 394.0727732181549\n",
      "Training Loss : 1.2946057319641113\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 286001\n",
      "mean reward (100 episodes) 139.650947\n",
      "best mean reward 139.765287\n",
      "running time 395.015598\n",
      "Train_EnvstepsSoFar : 286001\n",
      "Train_AverageReturn : 139.65094740910408\n",
      "Train_BestReturn : 139.7652873129927\n",
      "TimeSinceStart : 395.01559805870056\n",
      "Training Loss : 0.6465302109718323\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 287001\n",
      "mean reward (100 episodes) 140.717123\n",
      "best mean reward 140.717123\n",
      "running time 396.212446\n",
      "Train_EnvstepsSoFar : 287001\n",
      "Train_AverageReturn : 140.71712282184677\n",
      "Train_BestReturn : 140.71712282184677\n",
      "TimeSinceStart : 396.21244597435\n",
      "Training Loss : 0.16189970076084137\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 288001\n",
      "mean reward (100 episodes) 140.706871\n",
      "best mean reward 140.717123\n",
      "running time 397.629643\n",
      "Train_EnvstepsSoFar : 288001\n",
      "Train_AverageReturn : 140.7068712139639\n",
      "Train_BestReturn : 140.71712282184677\n",
      "TimeSinceStart : 397.6296429634094\n",
      "Training Loss : 0.3435937464237213\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 289001\n",
      "mean reward (100 episodes) 147.002949\n",
      "best mean reward 147.002949\n",
      "running time 398.526091\n",
      "Train_EnvstepsSoFar : 289001\n",
      "Train_AverageReturn : 147.00294885487892\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 398.5260910987854\n",
      "Training Loss : 0.11838610470294952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 146.131302\n",
      "best mean reward 147.002949\n",
      "running time 399.618393\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 146.13130165371223\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 399.6183931827545\n",
      "Training Loss : 1.2030035257339478\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 291001\n",
      "mean reward (100 episodes) 143.731886\n",
      "best mean reward 147.002949\n",
      "running time 400.593921\n",
      "Train_EnvstepsSoFar : 291001\n",
      "Train_AverageReturn : 143.73188583060033\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 400.5939211845398\n",
      "Training Loss : 0.2865872383117676\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 292001\n",
      "mean reward (100 episodes) 137.380887\n",
      "best mean reward 147.002949\n",
      "running time 401.540879\n",
      "Train_EnvstepsSoFar : 292001\n",
      "Train_AverageReturn : 137.38088661472702\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 401.54087924957275\n",
      "Training Loss : 0.6181378960609436\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 293001\n",
      "mean reward (100 episodes) 136.270547\n",
      "best mean reward 147.002949\n",
      "running time 402.437946\n",
      "Train_EnvstepsSoFar : 293001\n",
      "Train_AverageReturn : 136.27054669625934\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 402.4379460811615\n",
      "Training Loss : 2.29303240776062\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 294001\n",
      "mean reward (100 episodes) 137.713515\n",
      "best mean reward 147.002949\n",
      "running time 403.555023\n",
      "Train_EnvstepsSoFar : 294001\n",
      "Train_AverageReturn : 137.71351466260495\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 403.5550231933594\n",
      "Training Loss : 0.32030215859413147\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 295001\n",
      "mean reward (100 episodes) 136.359791\n",
      "best mean reward 147.002949\n",
      "running time 404.422488\n",
      "Train_EnvstepsSoFar : 295001\n",
      "Train_AverageReturn : 136.3597912147842\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 404.42248821258545\n",
      "Training Loss : 0.33234328031539917\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 296001\n",
      "mean reward (100 episodes) 133.257831\n",
      "best mean reward 147.002949\n",
      "running time 405.633824\n",
      "Train_EnvstepsSoFar : 296001\n",
      "Train_AverageReturn : 133.2578307495588\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 405.6338241100311\n",
      "Training Loss : 1.3823305368423462\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 297001\n",
      "mean reward (100 episodes) 129.405487\n",
      "best mean reward 147.002949\n",
      "running time 407.192694\n",
      "Train_EnvstepsSoFar : 297001\n",
      "Train_AverageReturn : 129.4054867374655\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 407.1926941871643\n",
      "Training Loss : 0.8159860968589783\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 298001\n",
      "mean reward (100 episodes) 130.286560\n",
      "best mean reward 147.002949\n",
      "running time 408.061752\n",
      "Train_EnvstepsSoFar : 298001\n",
      "Train_AverageReturn : 130.2865604909894\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 408.06175208091736\n",
      "Training Loss : 0.10673843324184418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 299001\n",
      "mean reward (100 episodes) 133.381536\n",
      "best mean reward 147.002949\n",
      "running time 408.919427\n",
      "Train_EnvstepsSoFar : 299001\n",
      "Train_AverageReturn : 133.38153550604875\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 408.91942715644836\n",
      "Training Loss : 0.18273930251598358\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 130.384715\n",
      "best mean reward 147.002949\n",
      "running time 409.817172\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 130.38471532693282\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 409.8171720504761\n",
      "Training Loss : 0.2700422704219818\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 301001\n",
      "mean reward (100 episodes) 130.834316\n",
      "best mean reward 147.002949\n",
      "running time 410.826024\n",
      "Train_EnvstepsSoFar : 301001\n",
      "Train_AverageReturn : 130.834316496871\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 410.82602405548096\n",
      "Training Loss : 0.14344052970409393\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 302001\n",
      "mean reward (100 episodes) 133.457848\n",
      "best mean reward 147.002949\n",
      "running time 411.745332\n",
      "Train_EnvstepsSoFar : 302001\n",
      "Train_AverageReturn : 133.457848082886\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 411.74533200263977\n",
      "Training Loss : 0.16437901556491852\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 303001\n",
      "mean reward (100 episodes) 129.940176\n",
      "best mean reward 147.002949\n",
      "running time 412.773751\n",
      "Train_EnvstepsSoFar : 303001\n",
      "Train_AverageReturn : 129.94017572207355\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 412.7737510204315\n",
      "Training Loss : 0.9063111543655396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 304001\n",
      "mean reward (100 episodes) 129.500916\n",
      "best mean reward 147.002949\n",
      "running time 413.629790\n",
      "Train_EnvstepsSoFar : 304001\n",
      "Train_AverageReturn : 129.50091639564522\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 413.62979006767273\n",
      "Training Loss : 0.7393474578857422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 305001\n",
      "mean reward (100 episodes) 134.595776\n",
      "best mean reward 147.002949\n",
      "running time 414.760477\n",
      "Train_EnvstepsSoFar : 305001\n",
      "Train_AverageReturn : 134.59577634755203\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 414.76047706604004\n",
      "Training Loss : 0.27707549929618835\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 306001\n",
      "mean reward (100 episodes) 135.055351\n",
      "best mean reward 147.002949\n",
      "running time 415.723156\n",
      "Train_EnvstepsSoFar : 306001\n",
      "Train_AverageReturn : 135.05535102377218\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 415.7231559753418\n",
      "Training Loss : 0.14636139571666718\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 307001\n",
      "mean reward (100 episodes) 128.519718\n",
      "best mean reward 147.002949\n",
      "running time 416.588537\n",
      "Train_EnvstepsSoFar : 307001\n",
      "Train_AverageReturn : 128.51971818845618\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 416.5885372161865\n",
      "Training Loss : 0.1416395604610443\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 308001\n",
      "mean reward (100 episodes) 128.856775\n",
      "best mean reward 147.002949\n",
      "running time 417.424663\n",
      "Train_EnvstepsSoFar : 308001\n",
      "Train_AverageReturn : 128.85677485251523\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 417.424663066864\n",
      "Training Loss : 0.1348051130771637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 309001\n",
      "mean reward (100 episodes) 126.572147\n",
      "best mean reward 147.002949\n",
      "running time 418.349550\n",
      "Train_EnvstepsSoFar : 309001\n",
      "Train_AverageReturn : 126.57214664476797\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 418.3495502471924\n",
      "Training Loss : 1.0597243309020996\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 126.911848\n",
      "best mean reward 147.002949\n",
      "running time 419.402136\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 126.91184820038649\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 419.4021360874176\n",
      "Training Loss : 0.814254105091095\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 311001\n",
      "mean reward (100 episodes) 125.465644\n",
      "best mean reward 147.002949\n",
      "running time 420.431176\n",
      "Train_EnvstepsSoFar : 311001\n",
      "Train_AverageReturn : 125.46564375155204\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 420.43117594718933\n",
      "Training Loss : 0.16844472289085388\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 312001\n",
      "mean reward (100 episodes) 123.571519\n",
      "best mean reward 147.002949\n",
      "running time 421.735817\n",
      "Train_EnvstepsSoFar : 312001\n",
      "Train_AverageReturn : 123.57151863901154\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 421.7358169555664\n",
      "Training Loss : 0.5647568702697754\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 313001\n",
      "mean reward (100 episodes) 121.842910\n",
      "best mean reward 147.002949\n",
      "running time 423.238153\n",
      "Train_EnvstepsSoFar : 313001\n",
      "Train_AverageReturn : 121.84291007380237\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 423.23815298080444\n",
      "Training Loss : 0.1666332185268402\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 314001\n",
      "mean reward (100 episodes) 120.850860\n",
      "best mean reward 147.002949\n",
      "running time 424.198274\n",
      "Train_EnvstepsSoFar : 314001\n",
      "Train_AverageReturn : 120.85085955521697\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 424.1982741355896\n",
      "Training Loss : 0.11261779814958572\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 315001\n",
      "mean reward (100 episodes) 121.648992\n",
      "best mean reward 147.002949\n",
      "running time 425.000897\n",
      "Train_EnvstepsSoFar : 315001\n",
      "Train_AverageReturn : 121.64899217882763\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 425.00089716911316\n",
      "Training Loss : 0.20038409531116486\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 316001\n",
      "mean reward (100 episodes) 119.150598\n",
      "best mean reward 147.002949\n",
      "running time 425.887492\n",
      "Train_EnvstepsSoFar : 316001\n",
      "Train_AverageReturn : 119.150597843701\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 425.8874921798706\n",
      "Training Loss : 0.6123382449150085\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 317001\n",
      "mean reward (100 episodes) 124.457246\n",
      "best mean reward 147.002949\n",
      "running time 426.702153\n",
      "Train_EnvstepsSoFar : 317001\n",
      "Train_AverageReturn : 124.4572463601453\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 426.702152967453\n",
      "Training Loss : 0.13161951303482056\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 318001\n",
      "mean reward (100 episodes) 125.014229\n",
      "best mean reward 147.002949\n",
      "running time 427.592709\n",
      "Train_EnvstepsSoFar : 318001\n",
      "Train_AverageReturn : 125.01422917156522\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 427.5927093029022\n",
      "Training Loss : 0.13022741675376892\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 319001\n",
      "mean reward (100 episodes) 123.334294\n",
      "best mean reward 147.002949\n",
      "running time 429.167531\n",
      "Train_EnvstepsSoFar : 319001\n",
      "Train_AverageReturn : 123.33429429181899\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 429.16753101348877\n",
      "Training Loss : 0.1269054263830185\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 124.259717\n",
      "best mean reward 147.002949\n",
      "running time 430.328964\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 124.25971672683822\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 430.32896423339844\n",
      "Training Loss : 0.1143079623579979\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 321001\n",
      "mean reward (100 episodes) 124.108228\n",
      "best mean reward 147.002949\n",
      "running time 431.664004\n",
      "Train_EnvstepsSoFar : 321001\n",
      "Train_AverageReturn : 124.108228021721\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 431.6640040874481\n",
      "Training Loss : 2.2637269496917725\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 322001\n",
      "mean reward (100 episodes) 126.500939\n",
      "best mean reward 147.002949\n",
      "running time 433.115037\n",
      "Train_EnvstepsSoFar : 322001\n",
      "Train_AverageReturn : 126.50093908601721\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 433.1150369644165\n",
      "Training Loss : 5.985321521759033\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 323001\n",
      "mean reward (100 episodes) 124.207564\n",
      "best mean reward 147.002949\n",
      "running time 434.065418\n",
      "Train_EnvstepsSoFar : 323001\n",
      "Train_AverageReturn : 124.20756359917256\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 434.0654180049896\n",
      "Training Loss : 1.1997660398483276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 324001\n",
      "mean reward (100 episodes) 118.553982\n",
      "best mean reward 147.002949\n",
      "running time 434.911225\n",
      "Train_EnvstepsSoFar : 324001\n",
      "Train_AverageReturn : 118.55398156422714\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 434.9112250804901\n",
      "Training Loss : 0.259517639875412\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 325001\n",
      "mean reward (100 episodes) 114.959615\n",
      "best mean reward 147.002949\n",
      "running time 436.066332\n",
      "Train_EnvstepsSoFar : 325001\n",
      "Train_AverageReturn : 114.95961464645671\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 436.0663318634033\n",
      "Training Loss : 1.6130971908569336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 326001\n",
      "mean reward (100 episodes) 117.586312\n",
      "best mean reward 147.002949\n",
      "running time 436.877274\n",
      "Train_EnvstepsSoFar : 326001\n",
      "Train_AverageReturn : 117.58631154400143\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 436.87727427482605\n",
      "Training Loss : 0.1490994542837143\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 327001\n",
      "mean reward (100 episodes) 116.531950\n",
      "best mean reward 147.002949\n",
      "running time 437.970653\n",
      "Train_EnvstepsSoFar : 327001\n",
      "Train_AverageReturn : 116.53195027735119\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 437.9706530570984\n",
      "Training Loss : 2.772582530975342\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 328001\n",
      "mean reward (100 episodes) 117.983383\n",
      "best mean reward 147.002949\n",
      "running time 439.010643\n",
      "Train_EnvstepsSoFar : 328001\n",
      "Train_AverageReturn : 117.9833828385794\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 439.0106430053711\n",
      "Training Loss : 0.14444833993911743\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 329001\n",
      "mean reward (100 episodes) 118.672548\n",
      "best mean reward 147.002949\n",
      "running time 439.967367\n",
      "Train_EnvstepsSoFar : 329001\n",
      "Train_AverageReturn : 118.67254821013854\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 439.96736693382263\n",
      "Training Loss : 0.18810322880744934\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 117.564500\n",
      "best mean reward 147.002949\n",
      "running time 441.251394\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 117.56450046343865\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 441.251394033432\n",
      "Training Loss : 0.8569675087928772\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 331001\n",
      "mean reward (100 episodes) 115.609565\n",
      "best mean reward 147.002949\n",
      "running time 442.547314\n",
      "Train_EnvstepsSoFar : 331001\n",
      "Train_AverageReturn : 115.60956536404852\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 442.5473139286041\n",
      "Training Loss : 0.5423017740249634\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 332001\n",
      "mean reward (100 episodes) 116.420255\n",
      "best mean reward 147.002949\n",
      "running time 443.485249\n",
      "Train_EnvstepsSoFar : 332001\n",
      "Train_AverageReturn : 116.42025458936128\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 443.485249042511\n",
      "Training Loss : 2.4566726684570312\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 333001\n",
      "mean reward (100 episodes) 122.008081\n",
      "best mean reward 147.002949\n",
      "running time 444.374974\n",
      "Train_EnvstepsSoFar : 333001\n",
      "Train_AverageReturn : 122.0080811940493\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 444.3749740123749\n",
      "Training Loss : 3.214716911315918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 334001\n",
      "mean reward (100 episodes) 122.132322\n",
      "best mean reward 147.002949\n",
      "running time 445.212123\n",
      "Train_EnvstepsSoFar : 334001\n",
      "Train_AverageReturn : 122.13232244866226\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 445.2121229171753\n",
      "Training Loss : 0.2595692276954651\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 335001\n",
      "mean reward (100 episodes) 123.788310\n",
      "best mean reward 147.002949\n",
      "running time 446.433539\n",
      "Train_EnvstepsSoFar : 335001\n",
      "Train_AverageReturn : 123.7883099492622\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 446.4335391521454\n",
      "Training Loss : 0.3926863968372345\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 336001\n",
      "mean reward (100 episodes) 119.746454\n",
      "best mean reward 147.002949\n",
      "running time 447.830487\n",
      "Train_EnvstepsSoFar : 336001\n",
      "Train_AverageReturn : 119.74645413581658\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 447.83048701286316\n",
      "Training Loss : 0.23553329706192017\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 337001\n",
      "mean reward (100 episodes) 124.076883\n",
      "best mean reward 147.002949\n",
      "running time 448.890301\n",
      "Train_EnvstepsSoFar : 337001\n",
      "Train_AverageReturn : 124.07688285095298\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 448.890300989151\n",
      "Training Loss : 0.12124256044626236\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 338001\n",
      "mean reward (100 episodes) 123.765206\n",
      "best mean reward 147.002949\n",
      "running time 449.777455\n",
      "Train_EnvstepsSoFar : 338001\n",
      "Train_AverageReturn : 123.76520632119926\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 449.77745509147644\n",
      "Training Loss : 0.1296437829732895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 339001\n",
      "mean reward (100 episodes) 121.995972\n",
      "best mean reward 147.002949\n",
      "running time 450.687666\n",
      "Train_EnvstepsSoFar : 339001\n",
      "Train_AverageReturn : 121.99597180367635\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 450.68766593933105\n",
      "Training Loss : 0.68882155418396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 123.481016\n",
      "best mean reward 147.002949\n",
      "running time 451.508513\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 123.4810163238541\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 451.5085129737854\n",
      "Training Loss : 0.14573495090007782\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 341001\n",
      "mean reward (100 episodes) 125.831680\n",
      "best mean reward 147.002949\n",
      "running time 453.268641\n",
      "Train_EnvstepsSoFar : 341001\n",
      "Train_AverageReturn : 125.83167974636173\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 453.26864099502563\n",
      "Training Loss : 2.7940573692321777\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 342001\n",
      "mean reward (100 episodes) 121.082869\n",
      "best mean reward 147.002949\n",
      "running time 454.285949\n",
      "Train_EnvstepsSoFar : 342001\n",
      "Train_AverageReturn : 121.0828689400042\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 454.2859489917755\n",
      "Training Loss : 0.14397001266479492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 343001\n",
      "mean reward (100 episodes) 123.502613\n",
      "best mean reward 147.002949\n",
      "running time 455.959534\n",
      "Train_EnvstepsSoFar : 343001\n",
      "Train_AverageReturn : 123.50261301315368\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 455.9595341682434\n",
      "Training Loss : 1.1542761325836182\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 344001\n",
      "mean reward (100 episodes) 122.802760\n",
      "best mean reward 147.002949\n",
      "running time 456.767765\n",
      "Train_EnvstepsSoFar : 344001\n",
      "Train_AverageReturn : 122.80276004312394\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 456.767765045166\n",
      "Training Loss : 0.2791837155818939\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 345001\n",
      "mean reward (100 episodes) 120.436789\n",
      "best mean reward 147.002949\n",
      "running time 457.640838\n",
      "Train_EnvstepsSoFar : 345001\n",
      "Train_AverageReturn : 120.43678935061689\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 457.64083790779114\n",
      "Training Loss : 0.21347495913505554\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 346001\n",
      "mean reward (100 episodes) 121.421655\n",
      "best mean reward 147.002949\n",
      "running time 458.446322\n",
      "Train_EnvstepsSoFar : 346001\n",
      "Train_AverageReturn : 121.42165538360143\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 458.4463219642639\n",
      "Training Loss : 0.2824244499206543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 347001\n",
      "mean reward (100 episodes) 119.171891\n",
      "best mean reward 147.002949\n",
      "running time 460.148742\n",
      "Train_EnvstepsSoFar : 347001\n",
      "Train_AverageReturn : 119.17189098686607\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 460.1487419605255\n",
      "Training Loss : 0.13300581276416779\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 348001\n",
      "mean reward (100 episodes) 119.200656\n",
      "best mean reward 147.002949\n",
      "running time 462.008266\n",
      "Train_EnvstepsSoFar : 348001\n",
      "Train_AverageReturn : 119.20065591201733\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 462.00826597213745\n",
      "Training Loss : 0.4154968857765198\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 349001\n",
      "mean reward (100 episodes) 126.980298\n",
      "best mean reward 147.002949\n",
      "running time 462.828536\n",
      "Train_EnvstepsSoFar : 349001\n",
      "Train_AverageReturn : 126.98029831808294\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 462.82853603363037\n",
      "Training Loss : 0.7494857311248779\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 127.702226\n",
      "best mean reward 147.002949\n",
      "running time 463.691738\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 127.70222606424302\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 463.6917381286621\n",
      "Training Loss : 2.9192514419555664\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 351001\n",
      "mean reward (100 episodes) 123.362739\n",
      "best mean reward 147.002949\n",
      "running time 464.661964\n",
      "Train_EnvstepsSoFar : 351001\n",
      "Train_AverageReturn : 123.36273898092448\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 464.6619641780853\n",
      "Training Loss : 0.6679186820983887\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 352001\n",
      "mean reward (100 episodes) 122.604009\n",
      "best mean reward 147.002949\n",
      "running time 465.939372\n",
      "Train_EnvstepsSoFar : 352001\n",
      "Train_AverageReturn : 122.6040092112399\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 465.9393720626831\n",
      "Training Loss : 4.201681137084961\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 353001\n",
      "mean reward (100 episodes) 121.780691\n",
      "best mean reward 147.002949\n",
      "running time 466.919650\n",
      "Train_EnvstepsSoFar : 353001\n",
      "Train_AverageReturn : 121.78069142078593\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 466.9196500778198\n",
      "Training Loss : 0.28984037041664124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 354001\n",
      "mean reward (100 episodes) 117.028770\n",
      "best mean reward 147.002949\n",
      "running time 467.717413\n",
      "Train_EnvstepsSoFar : 354001\n",
      "Train_AverageReturn : 117.02876990109748\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 467.7174129486084\n",
      "Training Loss : 0.1275726556777954\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 355001\n",
      "mean reward (100 episodes) 118.775460\n",
      "best mean reward 147.002949\n",
      "running time 468.529404\n",
      "Train_EnvstepsSoFar : 355001\n",
      "Train_AverageReturn : 118.77546018628054\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 468.529403924942\n",
      "Training Loss : 0.18851162493228912\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 356001\n",
      "mean reward (100 episodes) 114.588331\n",
      "best mean reward 147.002949\n",
      "running time 469.379491\n",
      "Train_EnvstepsSoFar : 356001\n",
      "Train_AverageReturn : 114.58833063539396\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 469.37949109077454\n",
      "Training Loss : 1.0241702795028687\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 357001\n",
      "mean reward (100 episodes) 111.949779\n",
      "best mean reward 147.002949\n",
      "running time 470.314615\n",
      "Train_EnvstepsSoFar : 357001\n",
      "Train_AverageReturn : 111.94977885517748\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 470.3146150112152\n",
      "Training Loss : 1.4764502048492432\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 358001\n",
      "mean reward (100 episodes) 114.097298\n",
      "best mean reward 147.002949\n",
      "running time 471.177588\n",
      "Train_EnvstepsSoFar : 358001\n",
      "Train_AverageReturn : 114.09729783156995\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 471.17758798599243\n",
      "Training Loss : 1.5236809253692627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 359001\n",
      "mean reward (100 episodes) 110.918499\n",
      "best mean reward 147.002949\n",
      "running time 471.923033\n",
      "Train_EnvstepsSoFar : 359001\n",
      "Train_AverageReturn : 110.91849894309516\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 471.9230332374573\n",
      "Training Loss : 0.09924379736185074\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 112.873779\n",
      "best mean reward 147.002949\n",
      "running time 472.918134\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 112.87377851705254\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 472.9181339740753\n",
      "Training Loss : 0.44829440116882324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 361001\n",
      "mean reward (100 episodes) 112.341998\n",
      "best mean reward 147.002949\n",
      "running time 473.778300\n",
      "Train_EnvstepsSoFar : 361001\n",
      "Train_AverageReturn : 112.34199766573097\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 473.7783000469208\n",
      "Training Loss : 0.4466838240623474\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 362001\n",
      "mean reward (100 episodes) 111.019213\n",
      "best mean reward 147.002949\n",
      "running time 474.724253\n",
      "Train_EnvstepsSoFar : 362001\n",
      "Train_AverageReturn : 111.0192134128393\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 474.72425293922424\n",
      "Training Loss : 0.22322186827659607\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 363001\n",
      "mean reward (100 episodes) 111.862979\n",
      "best mean reward 147.002949\n",
      "running time 476.452885\n",
      "Train_EnvstepsSoFar : 363001\n",
      "Train_AverageReturn : 111.86297853753327\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 476.4528851509094\n",
      "Training Loss : 0.4936995804309845\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 364001\n",
      "mean reward (100 episodes) 110.886830\n",
      "best mean reward 147.002949\n",
      "running time 477.246746\n",
      "Train_EnvstepsSoFar : 364001\n",
      "Train_AverageReturn : 110.8868298335624\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 477.2467460632324\n",
      "Training Loss : 0.8893911838531494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 365001\n",
      "mean reward (100 episodes) 109.710571\n",
      "best mean reward 147.002949\n",
      "running time 478.104653\n",
      "Train_EnvstepsSoFar : 365001\n",
      "Train_AverageReturn : 109.71057054245902\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 478.1046531200409\n",
      "Training Loss : 0.5863947868347168\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 366001\n",
      "mean reward (100 episodes) 116.261380\n",
      "best mean reward 147.002949\n",
      "running time 478.941828\n",
      "Train_EnvstepsSoFar : 366001\n",
      "Train_AverageReturn : 116.26138041742725\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 478.941828250885\n",
      "Training Loss : 0.3662131428718567\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 367001\n",
      "mean reward (100 episodes) 117.223610\n",
      "best mean reward 147.002949\n",
      "running time 479.786435\n",
      "Train_EnvstepsSoFar : 367001\n",
      "Train_AverageReturn : 117.22361012424415\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 479.7864351272583\n",
      "Training Loss : 0.39200159907341003\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 368001\n",
      "mean reward (100 episodes) 121.733746\n",
      "best mean reward 147.002949\n",
      "running time 480.552182\n",
      "Train_EnvstepsSoFar : 368001\n",
      "Train_AverageReturn : 121.73374632935128\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 480.5521821975708\n",
      "Training Loss : 1.0516376495361328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 369001\n",
      "mean reward (100 episodes) 122.234402\n",
      "best mean reward 147.002949\n",
      "running time 481.361521\n",
      "Train_EnvstepsSoFar : 369001\n",
      "Train_AverageReturn : 122.23440195037139\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 481.3615210056305\n",
      "Training Loss : 0.10787567496299744\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 120.721833\n",
      "best mean reward 147.002949\n",
      "running time 482.268139\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 120.72183313581758\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 482.2681391239166\n",
      "Training Loss : 0.7537386417388916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 371001\n",
      "mean reward (100 episodes) 116.194118\n",
      "best mean reward 147.002949\n",
      "running time 483.061229\n",
      "Train_EnvstepsSoFar : 371001\n",
      "Train_AverageReturn : 116.19411785227445\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 483.0612292289734\n",
      "Training Loss : 0.6170059442520142\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 372001\n",
      "mean reward (100 episodes) 113.324855\n",
      "best mean reward 147.002949\n",
      "running time 483.860989\n",
      "Train_EnvstepsSoFar : 372001\n",
      "Train_AverageReturn : 113.3248553842987\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 483.8609890937805\n",
      "Training Loss : 0.10061328113079071\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 373001\n",
      "mean reward (100 episodes) 107.556654\n",
      "best mean reward 147.002949\n",
      "running time 484.979656\n",
      "Train_EnvstepsSoFar : 373001\n",
      "Train_AverageReturn : 107.55665370209213\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 484.97965598106384\n",
      "Training Loss : 0.17082910239696503\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 374001\n",
      "mean reward (100 episodes) 102.195640\n",
      "best mean reward 147.002949\n",
      "running time 485.717388\n",
      "Train_EnvstepsSoFar : 374001\n",
      "Train_AverageReturn : 102.19564031750255\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 485.7173881530762\n",
      "Training Loss : 0.49840185046195984\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 375001\n",
      "mean reward (100 episodes) 99.710272\n",
      "best mean reward 147.002949\n",
      "running time 487.745233\n",
      "Train_EnvstepsSoFar : 375001\n",
      "Train_AverageReturn : 99.71027196900656\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 487.74523305892944\n",
      "Training Loss : 0.5206359624862671\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 376001\n",
      "mean reward (100 episodes) 100.604411\n",
      "best mean reward 147.002949\n",
      "running time 488.781938\n",
      "Train_EnvstepsSoFar : 376001\n",
      "Train_AverageReturn : 100.60441109775792\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 488.7819380760193\n",
      "Training Loss : 0.8755804300308228\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 377001\n",
      "mean reward (100 episodes) 103.949475\n",
      "best mean reward 147.002949\n",
      "running time 490.219131\n",
      "Train_EnvstepsSoFar : 377001\n",
      "Train_AverageReturn : 103.94947461091694\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 490.2191309928894\n",
      "Training Loss : 0.21228641271591187\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 378001\n",
      "mean reward (100 episodes) 95.588047\n",
      "best mean reward 147.002949\n",
      "running time 490.991762\n",
      "Train_EnvstepsSoFar : 378001\n",
      "Train_AverageReturn : 95.58804655658042\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 490.9917619228363\n",
      "Training Loss : 1.490605115890503\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 379001\n",
      "mean reward (100 episodes) 95.132443\n",
      "best mean reward 147.002949\n",
      "running time 491.993202\n",
      "Train_EnvstepsSoFar : 379001\n",
      "Train_AverageReturn : 95.13244269159065\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 491.9932019710541\n",
      "Training Loss : 0.21774594485759735\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 92.148850\n",
      "best mean reward 147.002949\n",
      "running time 492.718966\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 92.14884964130397\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 492.71896600723267\n",
      "Training Loss : 0.22408156096935272\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 381001\n",
      "mean reward (100 episodes) 90.909774\n",
      "best mean reward 147.002949\n",
      "running time 493.816762\n",
      "Train_EnvstepsSoFar : 381001\n",
      "Train_AverageReturn : 90.90977449824486\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 493.8167622089386\n",
      "Training Loss : 0.6710328459739685\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 382001\n",
      "mean reward (100 episodes) 94.551506\n",
      "best mean reward 147.002949\n",
      "running time 494.877615\n",
      "Train_EnvstepsSoFar : 382001\n",
      "Train_AverageReturn : 94.55150559157931\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 494.87761521339417\n",
      "Training Loss : 0.5535122752189636\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 383001\n",
      "mean reward (100 episodes) 96.153755\n",
      "best mean reward 147.002949\n",
      "running time 495.733609\n",
      "Train_EnvstepsSoFar : 383001\n",
      "Train_AverageReturn : 96.1537548331091\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 495.73360896110535\n",
      "Training Loss : 2.3892292976379395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 384001\n",
      "mean reward (100 episodes) 93.951291\n",
      "best mean reward 147.002949\n",
      "running time 496.733198\n",
      "Train_EnvstepsSoFar : 384001\n",
      "Train_AverageReturn : 93.9512907875632\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 496.73319816589355\n",
      "Training Loss : 6.622611999511719\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 385001\n",
      "mean reward (100 episodes) 90.881223\n",
      "best mean reward 147.002949\n",
      "running time 497.503462\n",
      "Train_EnvstepsSoFar : 385001\n",
      "Train_AverageReturn : 90.88122306149796\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 497.50346183776855\n",
      "Training Loss : 2.310436248779297\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 386001\n",
      "mean reward (100 episodes) 87.556470\n",
      "best mean reward 147.002949\n",
      "running time 498.466368\n",
      "Train_EnvstepsSoFar : 386001\n",
      "Train_AverageReturn : 87.55647004789218\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 498.4663679599762\n",
      "Training Loss : 0.11174165457487106\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 387001\n",
      "mean reward (100 episodes) 83.782292\n",
      "best mean reward 147.002949\n",
      "running time 499.372571\n",
      "Train_EnvstepsSoFar : 387001\n",
      "Train_AverageReturn : 83.78229224454194\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 499.3725709915161\n",
      "Training Loss : 0.4767448306083679\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 388001\n",
      "mean reward (100 episodes) 81.771129\n",
      "best mean reward 147.002949\n",
      "running time 500.380212\n",
      "Train_EnvstepsSoFar : 388001\n",
      "Train_AverageReturn : 81.7711292603107\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 500.38021206855774\n",
      "Training Loss : 0.17776907980442047\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 389001\n",
      "mean reward (100 episodes) 81.263054\n",
      "best mean reward 147.002949\n",
      "running time 501.130067\n",
      "Train_EnvstepsSoFar : 389001\n",
      "Train_AverageReturn : 81.26305444845997\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 501.13006711006165\n",
      "Training Loss : 0.5836462378501892\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 86.347300\n",
      "best mean reward 147.002949\n",
      "running time 501.886752\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 86.34729989621205\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 501.8867521286011\n",
      "Training Loss : 0.20112359523773193\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 391001\n",
      "mean reward (100 episodes) 83.486067\n",
      "best mean reward 147.002949\n",
      "running time 502.676665\n",
      "Train_EnvstepsSoFar : 391001\n",
      "Train_AverageReturn : 83.48606712670862\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 502.67666506767273\n",
      "Training Loss : 0.527886152267456\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 392001\n",
      "mean reward (100 episodes) 92.831554\n",
      "best mean reward 147.002949\n",
      "running time 503.530549\n",
      "Train_EnvstepsSoFar : 392001\n",
      "Train_AverageReturn : 92.83155441854694\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 503.53054904937744\n",
      "Training Loss : 1.3046565055847168\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 393001\n",
      "mean reward (100 episodes) 94.014704\n",
      "best mean reward 147.002949\n",
      "running time 504.609299\n",
      "Train_EnvstepsSoFar : 393001\n",
      "Train_AverageReturn : 94.0147039941768\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 504.60929894447327\n",
      "Training Loss : 0.8557524681091309\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 394001\n",
      "mean reward (100 episodes) 97.830046\n",
      "best mean reward 147.002949\n",
      "running time 505.534610\n",
      "Train_EnvstepsSoFar : 394001\n",
      "Train_AverageReturn : 97.8300463200716\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 505.5346100330353\n",
      "Training Loss : 0.18057990074157715\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 395001\n",
      "mean reward (100 episodes) 101.937862\n",
      "best mean reward 147.002949\n",
      "running time 506.809135\n",
      "Train_EnvstepsSoFar : 395001\n",
      "Train_AverageReturn : 101.93786215162125\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 506.80913519859314\n",
      "Training Loss : 0.22064004838466644\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 396001\n",
      "mean reward (100 episodes) 98.340553\n",
      "best mean reward 147.002949\n",
      "running time 507.975262\n",
      "Train_EnvstepsSoFar : 396001\n",
      "Train_AverageReturn : 98.34055349618372\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 507.975261926651\n",
      "Training Loss : 1.9155442714691162\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 397001\n",
      "mean reward (100 episodes) 94.161608\n",
      "best mean reward 147.002949\n",
      "running time 508.987336\n",
      "Train_EnvstepsSoFar : 397001\n",
      "Train_AverageReturn : 94.16160843772968\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 508.98733592033386\n",
      "Training Loss : 0.1756431609392166\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 398001\n",
      "mean reward (100 episodes) 91.434615\n",
      "best mean reward 147.002949\n",
      "running time 509.870868\n",
      "Train_EnvstepsSoFar : 398001\n",
      "Train_AverageReturn : 91.43461528204942\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 509.8708679676056\n",
      "Training Loss : 6.456418514251709\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 399001\n",
      "mean reward (100 episodes) 98.507366\n",
      "best mean reward 147.002949\n",
      "running time 510.675581\n",
      "Train_EnvstepsSoFar : 399001\n",
      "Train_AverageReturn : 98.50736604732238\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 510.67558097839355\n",
      "Training Loss : 1.5247833728790283\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 98.447639\n",
      "best mean reward 147.002949\n",
      "running time 511.763539\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 98.44763859481132\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 511.76353907585144\n",
      "Training Loss : 0.15878015756607056\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 401001\n",
      "mean reward (100 episodes) 104.537576\n",
      "best mean reward 147.002949\n",
      "running time 512.906247\n",
      "Train_EnvstepsSoFar : 401001\n",
      "Train_AverageReturn : 104.53757580126289\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 512.9062469005585\n",
      "Training Loss : 0.2158258557319641\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 402001\n",
      "mean reward (100 episodes) 105.273737\n",
      "best mean reward 147.002949\n",
      "running time 513.855220\n",
      "Train_EnvstepsSoFar : 402001\n",
      "Train_AverageReturn : 105.27373692288185\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 513.855220079422\n",
      "Training Loss : 0.5158487558364868\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 403001\n",
      "mean reward (100 episodes) 102.312666\n",
      "best mean reward 147.002949\n",
      "running time 514.784794\n",
      "Train_EnvstepsSoFar : 403001\n",
      "Train_AverageReturn : 102.31266627882478\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 514.7847940921783\n",
      "Training Loss : 0.8240277767181396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 404001\n",
      "mean reward (100 episodes) 102.225847\n",
      "best mean reward 147.002949\n",
      "running time 515.598721\n",
      "Train_EnvstepsSoFar : 404001\n",
      "Train_AverageReturn : 102.2258466399269\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 515.5987210273743\n",
      "Training Loss : 0.5008822679519653\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 405001\n",
      "mean reward (100 episodes) 101.639979\n",
      "best mean reward 147.002949\n",
      "running time 516.600617\n",
      "Train_EnvstepsSoFar : 405001\n",
      "Train_AverageReturn : 101.63997911386222\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 516.6006169319153\n",
      "Training Loss : 2.719357967376709\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 406001\n",
      "mean reward (100 episodes) 106.242955\n",
      "best mean reward 147.002949\n",
      "running time 517.472577\n",
      "Train_EnvstepsSoFar : 406001\n",
      "Train_AverageReturn : 106.24295515130558\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 517.4725770950317\n",
      "Training Loss : 1.8373126983642578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 407001\n",
      "mean reward (100 episodes) 111.285311\n",
      "best mean reward 147.002949\n",
      "running time 518.193620\n",
      "Train_EnvstepsSoFar : 407001\n",
      "Train_AverageReturn : 111.28531146798487\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 518.193619966507\n",
      "Training Loss : 0.17732194066047668\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 408001\n",
      "mean reward (100 episodes) 110.992499\n",
      "best mean reward 147.002949\n",
      "running time 519.080593\n",
      "Train_EnvstepsSoFar : 408001\n",
      "Train_AverageReturn : 110.99249894707017\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 519.0805928707123\n",
      "Training Loss : 0.5599607825279236\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 409001\n",
      "mean reward (100 episodes) 108.482860\n",
      "best mean reward 147.002949\n",
      "running time 520.318956\n",
      "Train_EnvstepsSoFar : 409001\n",
      "Train_AverageReturn : 108.48286048776015\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 520.3189561367035\n",
      "Training Loss : 0.6813815832138062\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 106.676763\n",
      "best mean reward 147.002949\n",
      "running time 521.497430\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 106.67676305397669\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 521.4974300861359\n",
      "Training Loss : 0.20948465168476105\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 411001\n",
      "mean reward (100 episodes) 106.714960\n",
      "best mean reward 147.002949\n",
      "running time 523.078293\n",
      "Train_EnvstepsSoFar : 411001\n",
      "Train_AverageReturn : 106.71495994989806\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 523.0782930850983\n",
      "Training Loss : 0.13162383437156677\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 412001\n",
      "mean reward (100 episodes) 106.150708\n",
      "best mean reward 147.002949\n",
      "running time 524.531791\n",
      "Train_EnvstepsSoFar : 412001\n",
      "Train_AverageReturn : 106.1507075097759\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 524.5317912101746\n",
      "Training Loss : 0.19253985583782196\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 413001\n",
      "mean reward (100 episodes) 105.788083\n",
      "best mean reward 147.002949\n",
      "running time 525.408309\n",
      "Train_EnvstepsSoFar : 413001\n",
      "Train_AverageReturn : 105.78808282738588\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 525.4083089828491\n",
      "Training Loss : 0.19838327169418335\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 414001\n",
      "mean reward (100 episodes) 102.733876\n",
      "best mean reward 147.002949\n",
      "running time 526.806371\n",
      "Train_EnvstepsSoFar : 414001\n",
      "Train_AverageReturn : 102.73387573257995\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 526.8063712120056\n",
      "Training Loss : 0.18055981397628784\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 415001\n",
      "mean reward (100 episodes) 100.049858\n",
      "best mean reward 147.002949\n",
      "running time 528.556535\n",
      "Train_EnvstepsSoFar : 415001\n",
      "Train_AverageReturn : 100.04985766277845\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 528.5565350055695\n",
      "Training Loss : 0.29651764035224915\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 416001\n",
      "mean reward (100 episodes) 103.146453\n",
      "best mean reward 147.002949\n",
      "running time 529.502070\n",
      "Train_EnvstepsSoFar : 416001\n",
      "Train_AverageReturn : 103.14645256417704\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 529.5020701885223\n",
      "Training Loss : 0.22945979237556458\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 417001\n",
      "mean reward (100 episodes) 98.605976\n",
      "best mean reward 147.002949\n",
      "running time 530.484726\n",
      "Train_EnvstepsSoFar : 417001\n",
      "Train_AverageReturn : 98.60597567642732\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 530.484726190567\n",
      "Training Loss : 0.21334518492221832\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 418001\n",
      "mean reward (100 episodes) 101.090580\n",
      "best mean reward 147.002949\n",
      "running time 531.280283\n",
      "Train_EnvstepsSoFar : 418001\n",
      "Train_AverageReturn : 101.0905798249902\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 531.2802829742432\n",
      "Training Loss : 0.24028800427913666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 419001\n",
      "mean reward (100 episodes) 100.963631\n",
      "best mean reward 147.002949\n",
      "running time 532.208510\n",
      "Train_EnvstepsSoFar : 419001\n",
      "Train_AverageReturn : 100.9636306538709\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 532.2085099220276\n",
      "Training Loss : 3.39481520652771\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 102.812680\n",
      "best mean reward 147.002949\n",
      "running time 533.306814\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 102.81268014808728\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 533.306813955307\n",
      "Training Loss : 0.43782326579093933\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 421001\n",
      "mean reward (100 episodes) 103.514000\n",
      "best mean reward 147.002949\n",
      "running time 534.090467\n",
      "Train_EnvstepsSoFar : 421001\n",
      "Train_AverageReturn : 103.51400006532653\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 534.0904672145844\n",
      "Training Loss : 0.2949798107147217\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 422001\n",
      "mean reward (100 episodes) 103.403424\n",
      "best mean reward 147.002949\n",
      "running time 535.286508\n",
      "Train_EnvstepsSoFar : 422001\n",
      "Train_AverageReturn : 103.40342427052073\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 535.2865080833435\n",
      "Training Loss : 0.4611890912055969\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 423001\n",
      "mean reward (100 episodes) 101.776341\n",
      "best mean reward 147.002949\n",
      "running time 536.171488\n",
      "Train_EnvstepsSoFar : 423001\n",
      "Train_AverageReturn : 101.77634083993308\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 536.1714880466461\n",
      "Training Loss : 0.1662994772195816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 424001\n",
      "mean reward (100 episodes) 92.242353\n",
      "best mean reward 147.002949\n",
      "running time 536.894184\n",
      "Train_EnvstepsSoFar : 424001\n",
      "Train_AverageReturn : 92.24235292961075\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 536.8941841125488\n",
      "Training Loss : 0.2781824469566345\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 425001\n",
      "mean reward (100 episodes) 95.127219\n",
      "best mean reward 147.002949\n",
      "running time 537.728805\n",
      "Train_EnvstepsSoFar : 425001\n",
      "Train_AverageReturn : 95.1272190159433\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 537.7288048267365\n",
      "Training Loss : 3.5771257877349854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 426001\n",
      "mean reward (100 episodes) 91.257379\n",
      "best mean reward 147.002949\n",
      "running time 538.797190\n",
      "Train_EnvstepsSoFar : 426001\n",
      "Train_AverageReturn : 91.25737932977019\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 538.797189950943\n",
      "Training Loss : 0.39307302236557007\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 427001\n",
      "mean reward (100 episodes) 91.810640\n",
      "best mean reward 147.002949\n",
      "running time 539.832111\n",
      "Train_EnvstepsSoFar : 427001\n",
      "Train_AverageReturn : 91.81063963246704\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 539.832111120224\n",
      "Training Loss : 0.20704074203968048\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 428001\n",
      "mean reward (100 episodes) 90.454550\n",
      "best mean reward 147.002949\n",
      "running time 542.088974\n",
      "Train_EnvstepsSoFar : 428001\n",
      "Train_AverageReturn : 90.45455018081924\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 542.088974237442\n",
      "Training Loss : 0.14419850707054138\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 429001\n",
      "mean reward (100 episodes) 86.002901\n",
      "best mean reward 147.002949\n",
      "running time 543.045664\n",
      "Train_EnvstepsSoFar : 429001\n",
      "Train_AverageReturn : 86.00290145362695\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 543.0456640720367\n",
      "Training Loss : 0.14698629081249237\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 84.566581\n",
      "best mean reward 147.002949\n",
      "running time 544.273942\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 84.56658123243845\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 544.2739419937134\n",
      "Training Loss : 0.1912599503993988\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 431001\n",
      "mean reward (100 episodes) 87.754490\n",
      "best mean reward 147.002949\n",
      "running time 545.402677\n",
      "Train_EnvstepsSoFar : 431001\n",
      "Train_AverageReturn : 87.75449001837016\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 545.4026770591736\n",
      "Training Loss : 0.1882757544517517\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 432001\n",
      "mean reward (100 episodes) 84.304040\n",
      "best mean reward 147.002949\n",
      "running time 546.282690\n",
      "Train_EnvstepsSoFar : 432001\n",
      "Train_AverageReturn : 84.30404021561026\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 546.2826900482178\n",
      "Training Loss : 0.304400235414505\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 433001\n",
      "mean reward (100 episodes) 85.282773\n",
      "best mean reward 147.002949\n",
      "running time 547.299654\n",
      "Train_EnvstepsSoFar : 433001\n",
      "Train_AverageReturn : 85.28277341888679\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 547.299654006958\n",
      "Training Loss : 0.19712379574775696\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 434001\n",
      "mean reward (100 episodes) 86.029373\n",
      "best mean reward 147.002949\n",
      "running time 549.378902\n",
      "Train_EnvstepsSoFar : 434001\n",
      "Train_AverageReturn : 86.02937300728296\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 549.3789021968842\n",
      "Training Loss : 0.26054394245147705\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 435001\n",
      "mean reward (100 episodes) 81.465071\n",
      "best mean reward 147.002949\n",
      "running time 550.561354\n",
      "Train_EnvstepsSoFar : 435001\n",
      "Train_AverageReturn : 81.46507118110674\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 550.5613541603088\n",
      "Training Loss : 0.1553470492362976\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 436001\n",
      "mean reward (100 episodes) 82.200291\n",
      "best mean reward 147.002949\n",
      "running time 551.855318\n",
      "Train_EnvstepsSoFar : 436001\n",
      "Train_AverageReturn : 82.20029116554022\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 551.855318069458\n",
      "Training Loss : 0.6483260989189148\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 437001\n",
      "mean reward (100 episodes) 80.518535\n",
      "best mean reward 147.002949\n",
      "running time 552.644389\n",
      "Train_EnvstepsSoFar : 437001\n",
      "Train_AverageReturn : 80.51853484159047\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 552.6443889141083\n",
      "Training Loss : 0.1778700351715088\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 438001\n",
      "mean reward (100 episodes) 80.629532\n",
      "best mean reward 147.002949\n",
      "running time 553.852187\n",
      "Train_EnvstepsSoFar : 438001\n",
      "Train_AverageReturn : 80.6295318768824\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 553.8521869182587\n",
      "Training Loss : 0.45594775676727295\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 439001\n",
      "mean reward (100 episodes) 80.091956\n",
      "best mean reward 147.002949\n",
      "running time 554.818935\n",
      "Train_EnvstepsSoFar : 439001\n",
      "Train_AverageReturn : 80.09195553013564\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 554.8189351558685\n",
      "Training Loss : 0.694742739200592\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 82.592409\n",
      "best mean reward 147.002949\n",
      "running time 555.682925\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 82.59240931353872\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 555.6829252243042\n",
      "Training Loss : 0.2295348048210144\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 441001\n",
      "mean reward (100 episodes) 85.205088\n",
      "best mean reward 147.002949\n",
      "running time 556.687699\n",
      "Train_EnvstepsSoFar : 441001\n",
      "Train_AverageReturn : 85.20508838609578\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 556.6876990795135\n",
      "Training Loss : 0.21767306327819824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 442001\n",
      "mean reward (100 episodes) 81.524335\n",
      "best mean reward 147.002949\n",
      "running time 557.394156\n",
      "Train_EnvstepsSoFar : 442001\n",
      "Train_AverageReturn : 81.52433466191304\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 557.3941559791565\n",
      "Training Loss : 0.2212999314069748\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 443001\n",
      "mean reward (100 episodes) 76.750701\n",
      "best mean reward 147.002949\n",
      "running time 558.269595\n",
      "Train_EnvstepsSoFar : 443001\n",
      "Train_AverageReturn : 76.75070125011736\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 558.2695949077606\n",
      "Training Loss : 0.14330029487609863\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 444001\n",
      "mean reward (100 episodes) 75.788024\n",
      "best mean reward 147.002949\n",
      "running time 559.376205\n",
      "Train_EnvstepsSoFar : 444001\n",
      "Train_AverageReturn : 75.78802401800189\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 559.3762052059174\n",
      "Training Loss : 0.14492042362689972\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 445001\n",
      "mean reward (100 episodes) 73.156251\n",
      "best mean reward 147.002949\n",
      "running time 561.051835\n",
      "Train_EnvstepsSoFar : 445001\n",
      "Train_AverageReturn : 73.15625125731066\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 561.0518350601196\n",
      "Training Loss : 0.15917417407035828\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 446001\n",
      "mean reward (100 episodes) 72.740805\n",
      "best mean reward 147.002949\n",
      "running time 563.567509\n",
      "Train_EnvstepsSoFar : 446001\n",
      "Train_AverageReturn : 72.74080497302238\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 563.5675091743469\n",
      "Training Loss : 0.4051552414894104\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 447001\n",
      "mean reward (100 episodes) 70.187968\n",
      "best mean reward 147.002949\n",
      "running time 565.443341\n",
      "Train_EnvstepsSoFar : 447001\n",
      "Train_AverageReturn : 70.18796751101952\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 565.4433410167694\n",
      "Training Loss : 0.24833960831165314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 448001\n",
      "mean reward (100 episodes) 75.742867\n",
      "best mean reward 147.002949\n",
      "running time 566.214909\n",
      "Train_EnvstepsSoFar : 448001\n",
      "Train_AverageReturn : 75.74286730114468\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 566.2149090766907\n",
      "Training Loss : 0.21294879913330078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 449001\n",
      "mean reward (100 episodes) 73.080092\n",
      "best mean reward 147.002949\n",
      "running time 567.764475\n",
      "Train_EnvstepsSoFar : 449001\n",
      "Train_AverageReturn : 73.08009166372284\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 567.7644748687744\n",
      "Training Loss : 0.18419325351715088\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 71.346990\n",
      "best mean reward 147.002949\n",
      "running time 568.953557\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 71.34699006571576\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 568.9535570144653\n",
      "Training Loss : 0.19547301530838013\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 451001\n",
      "mean reward (100 episodes) 75.635936\n",
      "best mean reward 147.002949\n",
      "running time 569.657631\n",
      "Train_EnvstepsSoFar : 451001\n",
      "Train_AverageReturn : 75.63593573132826\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 569.6576311588287\n",
      "Training Loss : 0.20773816108703613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 452001\n",
      "mean reward (100 episodes) 72.680028\n",
      "best mean reward 147.002949\n",
      "running time 570.755894\n",
      "Train_EnvstepsSoFar : 452001\n",
      "Train_AverageReturn : 72.68002836337524\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 570.7558941841125\n",
      "Training Loss : 0.24662739038467407\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 453001\n",
      "mean reward (100 episodes) 72.918534\n",
      "best mean reward 147.002949\n",
      "running time 571.518407\n",
      "Train_EnvstepsSoFar : 453001\n",
      "Train_AverageReturn : 72.9185337408702\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 571.518406867981\n",
      "Training Loss : 1.4100900888442993\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 454001\n",
      "mean reward (100 episodes) 75.960996\n",
      "best mean reward 147.002949\n",
      "running time 572.611338\n",
      "Train_EnvstepsSoFar : 454001\n",
      "Train_AverageReturn : 75.96099563481158\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 572.6113381385803\n",
      "Training Loss : 0.1204226091504097\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 455001\n",
      "mean reward (100 episodes) 75.134693\n",
      "best mean reward 147.002949\n",
      "running time 574.223008\n",
      "Train_EnvstepsSoFar : 455001\n",
      "Train_AverageReturn : 75.13469313480269\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 574.2230081558228\n",
      "Training Loss : 3.482517957687378\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 456001\n",
      "mean reward (100 episodes) 72.493210\n",
      "best mean reward 147.002949\n",
      "running time 576.561788\n",
      "Train_EnvstepsSoFar : 456001\n",
      "Train_AverageReturn : 72.49321048089341\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 576.5617880821228\n",
      "Training Loss : 0.15668056905269623\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 457001\n",
      "mean reward (100 episodes) 75.097717\n",
      "best mean reward 147.002949\n",
      "running time 577.310729\n",
      "Train_EnvstepsSoFar : 457001\n",
      "Train_AverageReturn : 75.09771650657036\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 577.3107290267944\n",
      "Training Loss : 0.6031721830368042\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 458001\n",
      "mean reward (100 episodes) 74.791374\n",
      "best mean reward 147.002949\n",
      "running time 579.059859\n",
      "Train_EnvstepsSoFar : 458001\n",
      "Train_AverageReturn : 74.79137356464751\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 579.0598590373993\n",
      "Training Loss : 0.2992883026599884\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 459001\n",
      "mean reward (100 episodes) 78.642039\n",
      "best mean reward 147.002949\n",
      "running time 580.516947\n",
      "Train_EnvstepsSoFar : 459001\n",
      "Train_AverageReturn : 78.6420387114626\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 580.5169470310211\n",
      "Training Loss : 0.29607853293418884\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 74.643204\n",
      "best mean reward 147.002949\n",
      "running time 582.338075\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 74.64320384945249\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 582.3380749225616\n",
      "Training Loss : 0.6606071591377258\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 461001\n",
      "mean reward (100 episodes) 76.232373\n",
      "best mean reward 147.002949\n",
      "running time 583.651246\n",
      "Train_EnvstepsSoFar : 461001\n",
      "Train_AverageReturn : 76.23237277150875\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 583.6512460708618\n",
      "Training Loss : 0.17567755281925201\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 462001\n",
      "mean reward (100 episodes) 81.691020\n",
      "best mean reward 147.002949\n",
      "running time 584.664100\n",
      "Train_EnvstepsSoFar : 462001\n",
      "Train_AverageReturn : 81.6910196263325\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 584.6641001701355\n",
      "Training Loss : 4.998988628387451\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 463001\n",
      "mean reward (100 episodes) 83.601214\n",
      "best mean reward 147.002949\n",
      "running time 585.748817\n",
      "Train_EnvstepsSoFar : 463001\n",
      "Train_AverageReturn : 83.6012135247854\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 585.7488169670105\n",
      "Training Loss : 0.18350207805633545\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 464001\n",
      "mean reward (100 episodes) 81.645564\n",
      "best mean reward 147.002949\n",
      "running time 587.799236\n",
      "Train_EnvstepsSoFar : 464001\n",
      "Train_AverageReturn : 81.64556400509777\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 587.7992360591888\n",
      "Training Loss : 0.330547034740448\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 465001\n",
      "mean reward (100 episodes) 79.161773\n",
      "best mean reward 147.002949\n",
      "running time 589.565877\n",
      "Train_EnvstepsSoFar : 465001\n",
      "Train_AverageReturn : 79.16177346804952\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 589.5658769607544\n",
      "Training Loss : 0.11399602144956589\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 466001\n",
      "mean reward (100 episodes) 74.371195\n",
      "best mean reward 147.002949\n",
      "running time 590.330453\n",
      "Train_EnvstepsSoFar : 466001\n",
      "Train_AverageReturn : 74.37119540089301\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 590.3304531574249\n",
      "Training Loss : 0.1070222407579422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 467001\n",
      "mean reward (100 episodes) 75.332181\n",
      "best mean reward 147.002949\n",
      "running time 591.678514\n",
      "Train_EnvstepsSoFar : 467001\n",
      "Train_AverageReturn : 75.3321807741984\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 591.6785140037537\n",
      "Training Loss : 0.23294691741466522\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 468001\n",
      "mean reward (100 episodes) 77.691145\n",
      "best mean reward 147.002949\n",
      "running time 592.976139\n",
      "Train_EnvstepsSoFar : 468001\n",
      "Train_AverageReturn : 77.69114478075045\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 592.9761390686035\n",
      "Training Loss : 0.18074515461921692\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 469001\n",
      "mean reward (100 episodes) 79.677629\n",
      "best mean reward 147.002949\n",
      "running time 594.784136\n",
      "Train_EnvstepsSoFar : 469001\n",
      "Train_AverageReturn : 79.67762869113285\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 594.7841360569\n",
      "Training Loss : 3.181135892868042\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 77.305492\n",
      "best mean reward 147.002949\n",
      "running time 596.074518\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 77.30549166347387\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 596.0745179653168\n",
      "Training Loss : 7.116273403167725\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 471001\n",
      "mean reward (100 episodes) 77.775755\n",
      "best mean reward 147.002949\n",
      "running time 597.183459\n",
      "Train_EnvstepsSoFar : 471001\n",
      "Train_AverageReturn : 77.77575452182242\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 597.1834590435028\n",
      "Training Loss : 0.44073420763015747\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 472001\n",
      "mean reward (100 episodes) 77.628746\n",
      "best mean reward 147.002949\n",
      "running time 598.850745\n",
      "Train_EnvstepsSoFar : 472001\n",
      "Train_AverageReturn : 77.62874572031127\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 598.8507452011108\n",
      "Training Loss : 0.20399528741836548\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 473001\n",
      "mean reward (100 episodes) 77.254517\n",
      "best mean reward 147.002949\n",
      "running time 600.792181\n",
      "Train_EnvstepsSoFar : 473001\n",
      "Train_AverageReturn : 77.25451746037749\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 600.7921810150146\n",
      "Training Loss : 0.20411768555641174\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 474001\n",
      "mean reward (100 episodes) 77.245087\n",
      "best mean reward 147.002949\n",
      "running time 602.366118\n",
      "Train_EnvstepsSoFar : 474001\n",
      "Train_AverageReturn : 77.24508682293988\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 602.3661179542542\n",
      "Training Loss : 1.4977918863296509\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 475001\n",
      "mean reward (100 episodes) 72.625379\n",
      "best mean reward 147.002949\n",
      "running time 603.762241\n",
      "Train_EnvstepsSoFar : 475001\n",
      "Train_AverageReturn : 72.62537948151437\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 603.7622411251068\n",
      "Training Loss : 0.2546599507331848\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 476001\n",
      "mean reward (100 episodes) 78.506716\n",
      "best mean reward 147.002949\n",
      "running time 604.821266\n",
      "Train_EnvstepsSoFar : 476001\n",
      "Train_AverageReturn : 78.5067163605575\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 604.8212659358978\n",
      "Training Loss : 0.26387739181518555\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 477001\n",
      "mean reward (100 episodes) 78.670232\n",
      "best mean reward 147.002949\n",
      "running time 605.585188\n",
      "Train_EnvstepsSoFar : 477001\n",
      "Train_AverageReturn : 78.67023198353137\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 605.5851881504059\n",
      "Training Loss : 0.08547109365463257\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 478001\n",
      "mean reward (100 episodes) 66.932568\n",
      "best mean reward 147.002949\n",
      "running time 606.327703\n",
      "Train_EnvstepsSoFar : 478001\n",
      "Train_AverageReturn : 66.93256758156444\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 606.327702999115\n",
      "Training Loss : 0.1616079956293106\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 479001\n",
      "mean reward (100 episodes) 72.499475\n",
      "best mean reward 147.002949\n",
      "running time 607.218160\n",
      "Train_EnvstepsSoFar : 479001\n",
      "Train_AverageReturn : 72.49947468593415\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 607.2181599140167\n",
      "Training Loss : 0.1140550971031189\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 73.732438\n",
      "best mean reward 147.002949\n",
      "running time 608.453966\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 73.73243768780515\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 608.4539661407471\n",
      "Training Loss : 0.14281578361988068\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 481001\n",
      "mean reward (100 episodes) 70.036345\n",
      "best mean reward 147.002949\n",
      "running time 609.702100\n",
      "Train_EnvstepsSoFar : 481001\n",
      "Train_AverageReturn : 70.03634477344298\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 609.7021000385284\n",
      "Training Loss : 0.3431599736213684\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 482001\n",
      "mean reward (100 episodes) 73.066485\n",
      "best mean reward 147.002949\n",
      "running time 610.536746\n",
      "Train_EnvstepsSoFar : 482001\n",
      "Train_AverageReturn : 73.06648497790728\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 610.5367460250854\n",
      "Training Loss : 0.21574686467647552\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 483001\n",
      "mean reward (100 episodes) 73.008565\n",
      "best mean reward 147.002949\n",
      "running time 611.915492\n",
      "Train_EnvstepsSoFar : 483001\n",
      "Train_AverageReturn : 73.00856519886663\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 611.9154920578003\n",
      "Training Loss : 0.9114602208137512\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 484001\n",
      "mean reward (100 episodes) 74.818030\n",
      "best mean reward 147.002949\n",
      "running time 613.521636\n",
      "Train_EnvstepsSoFar : 484001\n",
      "Train_AverageReturn : 74.818029954768\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 613.5216362476349\n",
      "Training Loss : 0.18152621388435364\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 485001\n",
      "mean reward (100 episodes) 82.494487\n",
      "best mean reward 147.002949\n",
      "running time 614.266070\n",
      "Train_EnvstepsSoFar : 485001\n",
      "Train_AverageReturn : 82.49448709400777\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 614.2660701274872\n",
      "Training Loss : 0.1288623958826065\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 486001\n",
      "mean reward (100 episodes) 79.389928\n",
      "best mean reward 147.002949\n",
      "running time 616.049139\n",
      "Train_EnvstepsSoFar : 486001\n",
      "Train_AverageReturn : 79.38992783897135\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 616.0491390228271\n",
      "Training Loss : 0.6147403120994568\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 487001\n",
      "mean reward (100 episodes) 73.679413\n",
      "best mean reward 147.002949\n",
      "running time 617.080027\n",
      "Train_EnvstepsSoFar : 487001\n",
      "Train_AverageReturn : 73.67941315660092\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 617.0800268650055\n",
      "Training Loss : 0.3144836723804474\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 488001\n",
      "mean reward (100 episodes) 75.731793\n",
      "best mean reward 147.002949\n",
      "running time 618.102150\n",
      "Train_EnvstepsSoFar : 488001\n",
      "Train_AverageReturn : 75.73179275251584\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 618.1021499633789\n",
      "Training Loss : 0.26188981533050537\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 489001\n",
      "mean reward (100 episodes) 75.407612\n",
      "best mean reward 147.002949\n",
      "running time 618.833995\n",
      "Train_EnvstepsSoFar : 489001\n",
      "Train_AverageReturn : 75.40761181273194\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 618.8339951038361\n",
      "Training Loss : 2.6343557834625244\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 76.850519\n",
      "best mean reward 147.002949\n",
      "running time 619.905670\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 76.85051945384345\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 619.905669927597\n",
      "Training Loss : 0.4253386855125427\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 491001\n",
      "mean reward (100 episodes) 70.282377\n",
      "best mean reward 147.002949\n",
      "running time 621.167384\n",
      "Train_EnvstepsSoFar : 491001\n",
      "Train_AverageReturn : 70.28237660089496\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 621.167384147644\n",
      "Training Loss : 0.14534470438957214\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 492001\n",
      "mean reward (100 episodes) 68.474847\n",
      "best mean reward 147.002949\n",
      "running time 622.989939\n",
      "Train_EnvstepsSoFar : 492001\n",
      "Train_AverageReturn : 68.47484672925775\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 622.9899389743805\n",
      "Training Loss : 1.0301330089569092\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 493001\n",
      "mean reward (100 episodes) 69.728725\n",
      "best mean reward 147.002949\n",
      "running time 623.748524\n",
      "Train_EnvstepsSoFar : 493001\n",
      "Train_AverageReturn : 69.72872474183293\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 623.7485241889954\n",
      "Training Loss : 0.21215170621871948\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 494001\n",
      "mean reward (100 episodes) 72.709161\n",
      "best mean reward 147.002949\n",
      "running time 625.502872\n",
      "Train_EnvstepsSoFar : 494001\n",
      "Train_AverageReturn : 72.70916073130213\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 625.5028722286224\n",
      "Training Loss : 1.3548095226287842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 495001\n",
      "mean reward (100 episodes) 75.102691\n",
      "best mean reward 147.002949\n",
      "running time 626.340353\n",
      "Train_EnvstepsSoFar : 495001\n",
      "Train_AverageReturn : 75.1026906396381\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 626.340353012085\n",
      "Training Loss : 1.0631877183914185\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 496001\n",
      "mean reward (100 episodes) 63.292953\n",
      "best mean reward 147.002949\n",
      "running time 627.163323\n",
      "Train_EnvstepsSoFar : 496001\n",
      "Train_AverageReturn : 63.29295342927577\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 627.1633229255676\n",
      "Training Loss : 6.1752824783325195\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 497001\n",
      "mean reward (100 episodes) 61.580554\n",
      "best mean reward 147.002949\n",
      "running time 629.280912\n",
      "Train_EnvstepsSoFar : 497001\n",
      "Train_AverageReturn : 61.58055421705743\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 629.2809119224548\n",
      "Training Loss : 1.259521245956421\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 498001\n",
      "mean reward (100 episodes) 60.724634\n",
      "best mean reward 147.002949\n",
      "running time 630.067398\n",
      "Train_EnvstepsSoFar : 498001\n",
      "Train_AverageReturn : 60.72463448441853\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 630.0673980712891\n",
      "Training Loss : 0.26193901896476746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 499001\n",
      "mean reward (100 episodes) 70.200836\n",
      "best mean reward 147.002949\n",
      "running time 630.945282\n",
      "Train_EnvstepsSoFar : 499001\n",
      "Train_AverageReturn : 70.20083611202989\n",
      "Train_BestReturn : 147.00294885487892\n",
      "TimeSinceStart : 630.9452822208405\n",
      "Training Loss : 0.38708239793777466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running DQN experiment with seed 2\n",
      "########################\n",
      "logging outputs to  logs/dqn/LunarLander/vanilla_dqn/seed2\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.000276\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.0002760887145996094\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1001\n",
      "mean reward (100 episodes) -302.396783\n",
      "best mean reward -inf\n",
      "running time 0.234718\n",
      "Train_EnvstepsSoFar : 1001\n",
      "Train_AverageReturn : -302.39678306662\n",
      "TimeSinceStart : 0.23471808433532715\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2001\n",
      "mean reward (100 episodes) -271.187812\n",
      "best mean reward -inf\n",
      "running time 0.898247\n",
      "Train_EnvstepsSoFar : 2001\n",
      "Train_AverageReturn : -271.18781224968865\n",
      "TimeSinceStart : 0.8982470035552979\n",
      "Training Loss : 0.21661245822906494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3001\n",
      "mean reward (100 episodes) -267.236796\n",
      "best mean reward -inf\n",
      "running time 1.619698\n",
      "Train_EnvstepsSoFar : 3001\n",
      "Train_AverageReturn : -267.23679553881243\n",
      "TimeSinceStart : 1.6196980476379395\n",
      "Training Loss : 0.4179266095161438\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4001\n",
      "mean reward (100 episodes) -293.768968\n",
      "best mean reward -inf\n",
      "running time 2.300306\n",
      "Train_EnvstepsSoFar : 4001\n",
      "Train_AverageReturn : -293.7689684471303\n",
      "TimeSinceStart : 2.3003060817718506\n",
      "Training Loss : 3.214160203933716\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5001\n",
      "mean reward (100 episodes) -281.902913\n",
      "best mean reward -inf\n",
      "running time 2.997748\n",
      "Train_EnvstepsSoFar : 5001\n",
      "Train_AverageReturn : -281.90291269259154\n",
      "TimeSinceStart : 2.9977478981018066\n",
      "Training Loss : 0.3517701029777527\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6001\n",
      "mean reward (100 episodes) -284.185864\n",
      "best mean reward -inf\n",
      "running time 3.680805\n",
      "Train_EnvstepsSoFar : 6001\n",
      "Train_AverageReturn : -284.1858642826895\n",
      "TimeSinceStart : 3.680805206298828\n",
      "Training Loss : 0.33734607696533203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7001\n",
      "mean reward (100 episodes) -281.687635\n",
      "best mean reward -inf\n",
      "running time 4.396305\n",
      "Train_EnvstepsSoFar : 7001\n",
      "Train_AverageReturn : -281.6876345828654\n",
      "TimeSinceStart : 4.3963048458099365\n",
      "Training Loss : 0.27826955914497375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8001\n",
      "mean reward (100 episodes) -278.708584\n",
      "best mean reward -inf\n",
      "running time 5.519654\n",
      "Train_EnvstepsSoFar : 8001\n",
      "Train_AverageReturn : -278.70858388452484\n",
      "TimeSinceStart : 5.519654035568237\n",
      "Training Loss : 0.39561551809310913\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9001\n",
      "mean reward (100 episodes) -255.190800\n",
      "best mean reward -inf\n",
      "running time 6.932010\n",
      "Train_EnvstepsSoFar : 9001\n",
      "Train_AverageReturn : -255.1907996496189\n",
      "TimeSinceStart : 6.932009935379028\n",
      "Training Loss : 0.3979049026966095\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -252.365775\n",
      "best mean reward -inf\n",
      "running time 7.786045\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -252.36577544560822\n",
      "TimeSinceStart : 7.786045074462891\n",
      "Training Loss : 1.267993450164795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 11001\n",
      "mean reward (100 episodes) -252.025342\n",
      "best mean reward -inf\n",
      "running time 8.711791\n",
      "Train_EnvstepsSoFar : 11001\n",
      "Train_AverageReturn : -252.02534229221894\n",
      "TimeSinceStart : 8.711791038513184\n",
      "Training Loss : 0.5197059512138367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 12001\n",
      "mean reward (100 episodes) -249.867185\n",
      "best mean reward -inf\n",
      "running time 10.987903\n",
      "Train_EnvstepsSoFar : 12001\n",
      "Train_AverageReturn : -249.86718517814197\n",
      "TimeSinceStart : 10.987903118133545\n",
      "Training Loss : 0.28078216314315796\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 13001\n",
      "mean reward (100 episodes) -248.104693\n",
      "best mean reward -inf\n",
      "running time 12.265046\n",
      "Train_EnvstepsSoFar : 13001\n",
      "Train_AverageReturn : -248.1046932418004\n",
      "TimeSinceStart : 12.265046119689941\n",
      "Training Loss : 4.835732936859131\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 14001\n",
      "mean reward (100 episodes) -245.306550\n",
      "best mean reward -inf\n",
      "running time 14.087497\n",
      "Train_EnvstepsSoFar : 14001\n",
      "Train_AverageReturn : -245.30654953449817\n",
      "TimeSinceStart : 14.087496995925903\n",
      "Training Loss : 1.2466495037078857\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 15001\n",
      "mean reward (100 episodes) -242.016441\n",
      "best mean reward -inf\n",
      "running time 14.977108\n",
      "Train_EnvstepsSoFar : 15001\n",
      "Train_AverageReturn : -242.01644135196804\n",
      "TimeSinceStart : 14.977108001708984\n",
      "Training Loss : 1.2368019819259644\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 16001\n",
      "mean reward (100 episodes) -241.925234\n",
      "best mean reward -inf\n",
      "running time 16.563374\n",
      "Train_EnvstepsSoFar : 16001\n",
      "Train_AverageReturn : -241.92523432498206\n",
      "TimeSinceStart : 16.563374042510986\n",
      "Training Loss : 9.600582122802734\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 17001\n",
      "mean reward (100 episodes) -232.339329\n",
      "best mean reward -inf\n",
      "running time 17.586224\n",
      "Train_EnvstepsSoFar : 17001\n",
      "Train_AverageReturn : -232.33932919094877\n",
      "TimeSinceStart : 17.58622407913208\n",
      "Training Loss : 4.010268211364746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 18001\n",
      "mean reward (100 episodes) -230.030413\n",
      "best mean reward -inf\n",
      "running time 18.732338\n",
      "Train_EnvstepsSoFar : 18001\n",
      "Train_AverageReturn : -230.0304125785511\n",
      "TimeSinceStart : 18.732337951660156\n",
      "Training Loss : 0.8268043994903564\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 19001\n",
      "mean reward (100 episodes) -223.081843\n",
      "best mean reward -223.081843\n",
      "running time 20.000096\n",
      "Train_EnvstepsSoFar : 19001\n",
      "Train_AverageReturn : -223.0818427148462\n",
      "Train_BestReturn : -223.0818427148462\n",
      "TimeSinceStart : 20.000096082687378\n",
      "Training Loss : 0.9234219789505005\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -217.718383\n",
      "best mean reward -217.718383\n",
      "running time 21.047460\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -217.71838284128873\n",
      "Train_BestReturn : -217.71838284128873\n",
      "TimeSinceStart : 21.047460079193115\n",
      "Training Loss : 0.42202892899513245\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 21001\n",
      "mean reward (100 episodes) -214.167109\n",
      "best mean reward -214.167109\n",
      "running time 22.991687\n",
      "Train_EnvstepsSoFar : 21001\n",
      "Train_AverageReturn : -214.16710914176372\n",
      "Train_BestReturn : -214.16710914176372\n",
      "TimeSinceStart : 22.991687059402466\n",
      "Training Loss : 0.9416752457618713\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 22001\n",
      "mean reward (100 episodes) -212.696296\n",
      "best mean reward -212.696296\n",
      "running time 25.050797\n",
      "Train_EnvstepsSoFar : 22001\n",
      "Train_AverageReturn : -212.69629647448528\n",
      "Train_BestReturn : -212.69629647448528\n",
      "TimeSinceStart : 25.05079698562622\n",
      "Training Loss : 1.0456531047821045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 23001\n",
      "mean reward (100 episodes) -208.659032\n",
      "best mean reward -208.659032\n",
      "running time 26.294780\n",
      "Train_EnvstepsSoFar : 23001\n",
      "Train_AverageReturn : -208.6590316677717\n",
      "Train_BestReturn : -208.6590316677717\n",
      "TimeSinceStart : 26.294780015945435\n",
      "Training Loss : 0.5909700393676758\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 24001\n",
      "mean reward (100 episodes) -206.924423\n",
      "best mean reward -206.924423\n",
      "running time 27.766606\n",
      "Train_EnvstepsSoFar : 24001\n",
      "Train_AverageReturn : -206.92442309518904\n",
      "Train_BestReturn : -206.92442309518904\n",
      "TimeSinceStart : 27.766606092453003\n",
      "Training Loss : 0.9512801170349121\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 25001\n",
      "mean reward (100 episodes) -205.556461\n",
      "best mean reward -205.556461\n",
      "running time 29.667292\n",
      "Train_EnvstepsSoFar : 25001\n",
      "Train_AverageReturn : -205.55646087380822\n",
      "Train_BestReturn : -205.55646087380822\n",
      "TimeSinceStart : 29.66729187965393\n",
      "Training Loss : 0.3490566313266754\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 26001\n",
      "mean reward (100 episodes) -204.530244\n",
      "best mean reward -204.530244\n",
      "running time 31.888238\n",
      "Train_EnvstepsSoFar : 26001\n",
      "Train_AverageReturn : -204.5302438674017\n",
      "Train_BestReturn : -204.5302438674017\n",
      "TimeSinceStart : 31.888238191604614\n",
      "Training Loss : 0.36912891268730164\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 27001\n",
      "mean reward (100 episodes) -202.138386\n",
      "best mean reward -202.138386\n",
      "running time 34.661243\n",
      "Train_EnvstepsSoFar : 27001\n",
      "Train_AverageReturn : -202.1383856425413\n",
      "Train_BestReturn : -202.1383856425413\n",
      "TimeSinceStart : 34.661242961883545\n",
      "Training Loss : 0.33427199721336365\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 28001\n",
      "mean reward (100 episodes) -202.514904\n",
      "best mean reward -202.138386\n",
      "running time 36.596407\n",
      "Train_EnvstepsSoFar : 28001\n",
      "Train_AverageReturn : -202.5149036042501\n",
      "Train_BestReturn : -202.1383856425413\n",
      "TimeSinceStart : 36.59640717506409\n",
      "Training Loss : 1.7616010904312134\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 29001\n",
      "mean reward (100 episodes) -200.065864\n",
      "best mean reward -200.065864\n",
      "running time 38.551101\n",
      "Train_EnvstepsSoFar : 29001\n",
      "Train_AverageReturn : -200.065864366728\n",
      "Train_BestReturn : -200.065864366728\n",
      "TimeSinceStart : 38.551100969314575\n",
      "Training Loss : 0.6938372850418091\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -199.199945\n",
      "best mean reward -199.199945\n",
      "running time 40.383282\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -199.19994480768088\n",
      "Train_BestReturn : -199.19994480768088\n",
      "TimeSinceStart : 40.38328194618225\n",
      "Training Loss : 0.8375864624977112\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 31001\n",
      "mean reward (100 episodes) -196.725993\n",
      "best mean reward -196.725993\n",
      "running time 42.108619\n",
      "Train_EnvstepsSoFar : 31001\n",
      "Train_AverageReturn : -196.72599332059156\n",
      "Train_BestReturn : -196.72599332059156\n",
      "TimeSinceStart : 42.10861897468567\n",
      "Training Loss : 0.48209407925605774\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 32001\n",
      "mean reward (100 episodes) -196.580980\n",
      "best mean reward -196.580980\n",
      "running time 43.582020\n",
      "Train_EnvstepsSoFar : 32001\n",
      "Train_AverageReturn : -196.58098013266488\n",
      "Train_BestReturn : -196.58098013266488\n",
      "TimeSinceStart : 43.58202004432678\n",
      "Training Loss : 0.3795592784881592\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 33001\n",
      "mean reward (100 episodes) -196.684772\n",
      "best mean reward -196.580980\n",
      "running time 45.070240\n",
      "Train_EnvstepsSoFar : 33001\n",
      "Train_AverageReturn : -196.68477232540124\n",
      "Train_BestReturn : -196.58098013266488\n",
      "TimeSinceStart : 45.07024002075195\n",
      "Training Loss : 0.35200345516204834\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 34001\n",
      "mean reward (100 episodes) -195.068135\n",
      "best mean reward -195.068135\n",
      "running time 46.793704\n",
      "Train_EnvstepsSoFar : 34001\n",
      "Train_AverageReturn : -195.06813516298433\n",
      "Train_BestReturn : -195.06813516298433\n",
      "TimeSinceStart : 46.79370403289795\n",
      "Training Loss : 1.4973464012145996\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 35001\n",
      "mean reward (100 episodes) -191.523785\n",
      "best mean reward -191.523785\n",
      "running time 48.689176\n",
      "Train_EnvstepsSoFar : 35001\n",
      "Train_AverageReturn : -191.52378505667514\n",
      "Train_BestReturn : -191.52378505667514\n",
      "TimeSinceStart : 48.689176082611084\n",
      "Training Loss : 0.4463135302066803\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 36001\n",
      "mean reward (100 episodes) -190.855704\n",
      "best mean reward -190.855704\n",
      "running time 50.529651\n",
      "Train_EnvstepsSoFar : 36001\n",
      "Train_AverageReturn : -190.855704345984\n",
      "Train_BestReturn : -190.855704345984\n",
      "TimeSinceStart : 50.529650926589966\n",
      "Training Loss : 0.4511440098285675\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 37001\n",
      "mean reward (100 episodes) -187.650165\n",
      "best mean reward -187.650165\n",
      "running time 52.134183\n",
      "Train_EnvstepsSoFar : 37001\n",
      "Train_AverageReturn : -187.65016498015584\n",
      "Train_BestReturn : -187.65016498015584\n",
      "TimeSinceStart : 52.134182929992676\n",
      "Training Loss : 3.9925718307495117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 38001\n",
      "mean reward (100 episodes) -187.111712\n",
      "best mean reward -187.111712\n",
      "running time 53.987151\n",
      "Train_EnvstepsSoFar : 38001\n",
      "Train_AverageReturn : -187.1117119944178\n",
      "Train_BestReturn : -187.1117119944178\n",
      "TimeSinceStart : 53.98715114593506\n",
      "Training Loss : 1.2966269254684448\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 39001\n",
      "mean reward (100 episodes) -181.741227\n",
      "best mean reward -181.741227\n",
      "running time 55.893103\n",
      "Train_EnvstepsSoFar : 39001\n",
      "Train_AverageReturn : -181.74122694593197\n",
      "Train_BestReturn : -181.74122694593197\n",
      "TimeSinceStart : 55.89310312271118\n",
      "Training Loss : 0.3250509798526764\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -180.506606\n",
      "best mean reward -180.506606\n",
      "running time 57.983329\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -180.50660566288207\n",
      "Train_BestReturn : -180.50660566288207\n",
      "TimeSinceStart : 57.98332905769348\n",
      "Training Loss : 4.024380683898926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 41001\n",
      "mean reward (100 episodes) -178.321931\n",
      "best mean reward -178.321931\n",
      "running time 59.863368\n",
      "Train_EnvstepsSoFar : 41001\n",
      "Train_AverageReturn : -178.32193147273367\n",
      "Train_BestReturn : -178.32193147273367\n",
      "TimeSinceStart : 59.86336803436279\n",
      "Training Loss : 0.2797645926475525\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 42001\n",
      "mean reward (100 episodes) -175.663948\n",
      "best mean reward -175.663948\n",
      "running time 61.679148\n",
      "Train_EnvstepsSoFar : 42001\n",
      "Train_AverageReturn : -175.663948158868\n",
      "Train_BestReturn : -175.663948158868\n",
      "TimeSinceStart : 61.67914795875549\n",
      "Training Loss : 0.49356305599212646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 43001\n",
      "mean reward (100 episodes) -173.574810\n",
      "best mean reward -173.574810\n",
      "running time 63.369829\n",
      "Train_EnvstepsSoFar : 43001\n",
      "Train_AverageReturn : -173.57481042485026\n",
      "Train_BestReturn : -173.57481042485026\n",
      "TimeSinceStart : 63.369829177856445\n",
      "Training Loss : 0.34212878346443176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 44001\n",
      "mean reward (100 episodes) -173.850173\n",
      "best mean reward -173.574810\n",
      "running time 65.211501\n",
      "Train_EnvstepsSoFar : 44001\n",
      "Train_AverageReturn : -173.85017347946624\n",
      "Train_BestReturn : -173.57481042485026\n",
      "TimeSinceStart : 65.211501121521\n",
      "Training Loss : 6.623932361602783\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 45001\n",
      "mean reward (100 episodes) -170.514563\n",
      "best mean reward -170.514563\n",
      "running time 68.633388\n",
      "Train_EnvstepsSoFar : 45001\n",
      "Train_AverageReturn : -170.51456268029582\n",
      "Train_BestReturn : -170.51456268029582\n",
      "TimeSinceStart : 68.63338804244995\n",
      "Training Loss : 0.3521100878715515\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 46001\n",
      "mean reward (100 episodes) -169.967191\n",
      "best mean reward -169.967191\n",
      "running time 71.364827\n",
      "Train_EnvstepsSoFar : 46001\n",
      "Train_AverageReturn : -169.9671913010044\n",
      "Train_BestReturn : -169.9671913010044\n",
      "TimeSinceStart : 71.3648271560669\n",
      "Training Loss : 0.3163248896598816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 47001\n",
      "mean reward (100 episodes) -169.619330\n",
      "best mean reward -169.619330\n",
      "running time 74.270259\n",
      "Train_EnvstepsSoFar : 47001\n",
      "Train_AverageReturn : -169.6193300890319\n",
      "Train_BestReturn : -169.6193300890319\n",
      "TimeSinceStart : 74.27025890350342\n",
      "Training Loss : 0.9811154007911682\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 48001\n",
      "mean reward (100 episodes) -170.097890\n",
      "best mean reward -169.619330\n",
      "running time 75.964816\n",
      "Train_EnvstepsSoFar : 48001\n",
      "Train_AverageReturn : -170.09788993471665\n",
      "Train_BestReturn : -169.6193300890319\n",
      "TimeSinceStart : 75.96481609344482\n",
      "Training Loss : 0.5581797361373901\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 49001\n",
      "mean reward (100 episodes) -168.056541\n",
      "best mean reward -168.056541\n",
      "running time 78.071703\n",
      "Train_EnvstepsSoFar : 49001\n",
      "Train_AverageReturn : -168.05654090130628\n",
      "Train_BestReturn : -168.05654090130628\n",
      "TimeSinceStart : 78.0717031955719\n",
      "Training Loss : 0.9381848573684692\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -166.607808\n",
      "best mean reward -166.607808\n",
      "running time 80.489391\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -166.60780766907882\n",
      "Train_BestReturn : -166.60780766907882\n",
      "TimeSinceStart : 80.48939108848572\n",
      "Training Loss : 0.4051830470561981\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 51001\n",
      "mean reward (100 episodes) -166.114936\n",
      "best mean reward -166.114936\n",
      "running time 82.010921\n",
      "Train_EnvstepsSoFar : 51001\n",
      "Train_AverageReturn : -166.11493588884485\n",
      "Train_BestReturn : -166.11493588884485\n",
      "TimeSinceStart : 82.01092100143433\n",
      "Training Loss : 0.28488460183143616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 52001\n",
      "mean reward (100 episodes) -164.553052\n",
      "best mean reward -164.553052\n",
      "running time 83.856537\n",
      "Train_EnvstepsSoFar : 52001\n",
      "Train_AverageReturn : -164.55305245622955\n",
      "Train_BestReturn : -164.55305245622955\n",
      "TimeSinceStart : 83.85653710365295\n",
      "Training Loss : 0.37994542717933655\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 53001\n",
      "mean reward (100 episodes) -162.839942\n",
      "best mean reward -162.839942\n",
      "running time 85.436208\n",
      "Train_EnvstepsSoFar : 53001\n",
      "Train_AverageReturn : -162.83994211867181\n",
      "Train_BestReturn : -162.83994211867181\n",
      "TimeSinceStart : 85.43620824813843\n",
      "Training Loss : 0.3054059147834778\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 54001\n",
      "mean reward (100 episodes) -159.176160\n",
      "best mean reward -159.176160\n",
      "running time 86.923197\n",
      "Train_EnvstepsSoFar : 54001\n",
      "Train_AverageReturn : -159.17616030470467\n",
      "Train_BestReturn : -159.17616030470467\n",
      "TimeSinceStart : 86.92319703102112\n",
      "Training Loss : 0.3316146731376648\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 55001\n",
      "mean reward (100 episodes) -157.407091\n",
      "best mean reward -157.407091\n",
      "running time 88.432276\n",
      "Train_EnvstepsSoFar : 55001\n",
      "Train_AverageReturn : -157.40709069531894\n",
      "Train_BestReturn : -157.40709069531894\n",
      "TimeSinceStart : 88.4322760105133\n",
      "Training Loss : 0.3970963656902313\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 56001\n",
      "mean reward (100 episodes) -155.621716\n",
      "best mean reward -155.621716\n",
      "running time 90.016957\n",
      "Train_EnvstepsSoFar : 56001\n",
      "Train_AverageReturn : -155.62171594934586\n",
      "Train_BestReturn : -155.62171594934586\n",
      "TimeSinceStart : 90.01695704460144\n",
      "Training Loss : 0.448373019695282\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 57001\n",
      "mean reward (100 episodes) -154.856743\n",
      "best mean reward -154.856743\n",
      "running time 92.060482\n",
      "Train_EnvstepsSoFar : 57001\n",
      "Train_AverageReturn : -154.85674259788502\n",
      "Train_BestReturn : -154.85674259788502\n",
      "TimeSinceStart : 92.06048202514648\n",
      "Training Loss : 0.932842493057251\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 58001\n",
      "mean reward (100 episodes) -151.857824\n",
      "best mean reward -151.857824\n",
      "running time 93.550228\n",
      "Train_EnvstepsSoFar : 58001\n",
      "Train_AverageReturn : -151.85782387348493\n",
      "Train_BestReturn : -151.85782387348493\n",
      "TimeSinceStart : 93.55022811889648\n",
      "Training Loss : 0.3659675717353821\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 59001\n",
      "mean reward (100 episodes) -150.023536\n",
      "best mean reward -150.023536\n",
      "running time 94.600479\n",
      "Train_EnvstepsSoFar : 59001\n",
      "Train_AverageReturn : -150.02353643444562\n",
      "Train_BestReturn : -150.02353643444562\n",
      "TimeSinceStart : 94.60047912597656\n",
      "Training Loss : 0.9203421473503113\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -147.411895\n",
      "best mean reward -147.411895\n",
      "running time 96.096234\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -147.41189494699086\n",
      "Train_BestReturn : -147.41189494699086\n",
      "TimeSinceStart : 96.09623408317566\n",
      "Training Loss : 0.2946775555610657\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 61001\n",
      "mean reward (100 episodes) -145.223857\n",
      "best mean reward -145.223857\n",
      "running time 97.989686\n",
      "Train_EnvstepsSoFar : 61001\n",
      "Train_AverageReturn : -145.2238569867738\n",
      "Train_BestReturn : -145.2238569867738\n",
      "TimeSinceStart : 97.98968601226807\n",
      "Training Loss : 0.819635272026062\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 62001\n",
      "mean reward (100 episodes) -143.806638\n",
      "best mean reward -143.806638\n",
      "running time 99.422905\n",
      "Train_EnvstepsSoFar : 62001\n",
      "Train_AverageReturn : -143.80663831859013\n",
      "Train_BestReturn : -143.80663831859013\n",
      "TimeSinceStart : 99.4229052066803\n",
      "Training Loss : 0.11329276859760284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 63001\n",
      "mean reward (100 episodes) -143.862005\n",
      "best mean reward -143.806638\n",
      "running time 101.702579\n",
      "Train_EnvstepsSoFar : 63001\n",
      "Train_AverageReturn : -143.8620047901513\n",
      "Train_BestReturn : -143.80663831859013\n",
      "TimeSinceStart : 101.70257902145386\n",
      "Training Loss : 0.3696970045566559\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 64001\n",
      "mean reward (100 episodes) -143.329167\n",
      "best mean reward -143.329167\n",
      "running time 103.878604\n",
      "Train_EnvstepsSoFar : 64001\n",
      "Train_AverageReturn : -143.32916736553886\n",
      "Train_BestReturn : -143.32916736553886\n",
      "TimeSinceStart : 103.87860417366028\n",
      "Training Loss : 0.3486463725566864\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 65001\n",
      "mean reward (100 episodes) -141.664163\n",
      "best mean reward -141.664163\n",
      "running time 105.995683\n",
      "Train_EnvstepsSoFar : 65001\n",
      "Train_AverageReturn : -141.6641631895751\n",
      "Train_BestReturn : -141.6641631895751\n",
      "TimeSinceStart : 105.99568319320679\n",
      "Training Loss : 0.2093159705400467\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 66001\n",
      "mean reward (100 episodes) -140.324957\n",
      "best mean reward -140.324957\n",
      "running time 108.321266\n",
      "Train_EnvstepsSoFar : 66001\n",
      "Train_AverageReturn : -140.32495672361915\n",
      "Train_BestReturn : -140.32495672361915\n",
      "TimeSinceStart : 108.32126593589783\n",
      "Training Loss : 0.2724825441837311\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 67001\n",
      "mean reward (100 episodes) -142.890227\n",
      "best mean reward -140.324957\n",
      "running time 111.120934\n",
      "Train_EnvstepsSoFar : 67001\n",
      "Train_AverageReturn : -142.8902274013861\n",
      "Train_BestReturn : -140.32495672361915\n",
      "TimeSinceStart : 111.120934009552\n",
      "Training Loss : 0.1929532140493393\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 68001\n",
      "mean reward (100 episodes) -145.662383\n",
      "best mean reward -140.324957\n",
      "running time 113.252193\n",
      "Train_EnvstepsSoFar : 68001\n",
      "Train_AverageReturn : -145.66238296955973\n",
      "Train_BestReturn : -140.32495672361915\n",
      "TimeSinceStart : 113.25219297409058\n",
      "Training Loss : 0.21306705474853516\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 69001\n",
      "mean reward (100 episodes) -144.766706\n",
      "best mean reward -140.324957\n",
      "running time 114.983122\n",
      "Train_EnvstepsSoFar : 69001\n",
      "Train_AverageReturn : -144.76670630587512\n",
      "Train_BestReturn : -140.32495672361915\n",
      "TimeSinceStart : 114.98312187194824\n",
      "Training Loss : 0.27339425683021545\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -144.618161\n",
      "best mean reward -140.324957\n",
      "running time 118.342412\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -144.6181610054238\n",
      "Train_BestReturn : -140.32495672361915\n",
      "TimeSinceStart : 118.34241199493408\n",
      "Training Loss : 0.8982177376747131\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 71001\n",
      "mean reward (100 episodes) -139.654917\n",
      "best mean reward -139.654917\n",
      "running time 119.605434\n",
      "Train_EnvstepsSoFar : 71001\n",
      "Train_AverageReturn : -139.65491656373396\n",
      "Train_BestReturn : -139.65491656373396\n",
      "TimeSinceStart : 119.60543417930603\n",
      "Training Loss : 0.17408333718776703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 72001\n",
      "mean reward (100 episodes) -136.869659\n",
      "best mean reward -136.869659\n",
      "running time 122.437143\n",
      "Train_EnvstepsSoFar : 72001\n",
      "Train_AverageReturn : -136.86965917388565\n",
      "Train_BestReturn : -136.86965917388565\n",
      "TimeSinceStart : 122.43714308738708\n",
      "Training Loss : 0.11634701490402222\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 73001\n",
      "mean reward (100 episodes) -136.946965\n",
      "best mean reward -136.869659\n",
      "running time 126.531869\n",
      "Train_EnvstepsSoFar : 73001\n",
      "Train_AverageReturn : -136.94696546494472\n",
      "Train_BestReturn : -136.86965917388565\n",
      "TimeSinceStart : 126.53186893463135\n",
      "Training Loss : 0.5393089056015015\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 74001\n",
      "mean reward (100 episodes) -137.770394\n",
      "best mean reward -136.869659\n",
      "running time 129.073469\n",
      "Train_EnvstepsSoFar : 74001\n",
      "Train_AverageReturn : -137.77039435103075\n",
      "Train_BestReturn : -136.86965917388565\n",
      "TimeSinceStart : 129.07346892356873\n",
      "Training Loss : 0.313008576631546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 75001\n",
      "mean reward (100 episodes) -134.600924\n",
      "best mean reward -134.600924\n",
      "running time 131.932207\n",
      "Train_EnvstepsSoFar : 75001\n",
      "Train_AverageReturn : -134.6009237734531\n",
      "Train_BestReturn : -134.6009237734531\n",
      "TimeSinceStart : 131.93220710754395\n",
      "Training Loss : 0.13078564405441284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 76001\n",
      "mean reward (100 episodes) -134.831672\n",
      "best mean reward -134.600924\n",
      "running time 133.995269\n",
      "Train_EnvstepsSoFar : 76001\n",
      "Train_AverageReturn : -134.83167161879695\n",
      "Train_BestReturn : -134.6009237734531\n",
      "TimeSinceStart : 133.9952690601349\n",
      "Training Loss : 0.20907649397850037\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 77001\n",
      "mean reward (100 episodes) -133.566346\n",
      "best mean reward -133.566346\n",
      "running time 136.422655\n",
      "Train_EnvstepsSoFar : 77001\n",
      "Train_AverageReturn : -133.56634624470465\n",
      "Train_BestReturn : -133.56634624470465\n",
      "TimeSinceStart : 136.42265510559082\n",
      "Training Loss : 0.1739518940448761\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 78001\n",
      "mean reward (100 episodes) -127.481813\n",
      "best mean reward -127.481813\n",
      "running time 138.502443\n",
      "Train_EnvstepsSoFar : 78001\n",
      "Train_AverageReturn : -127.48181277683769\n",
      "Train_BestReturn : -127.48181277683769\n",
      "TimeSinceStart : 138.50244307518005\n",
      "Training Loss : 0.10268052667379379\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 79001\n",
      "mean reward (100 episodes) -124.438345\n",
      "best mean reward -124.438345\n",
      "running time 140.805176\n",
      "Train_EnvstepsSoFar : 79001\n",
      "Train_AverageReturn : -124.43834481087507\n",
      "Train_BestReturn : -124.43834481087507\n",
      "TimeSinceStart : 140.80517601966858\n",
      "Training Loss : 0.2973601818084717\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -123.230753\n",
      "best mean reward -123.230753\n",
      "running time 144.134386\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -123.2307534924576\n",
      "Train_BestReturn : -123.2307534924576\n",
      "TimeSinceStart : 144.13438606262207\n",
      "Training Loss : 0.2073395997285843\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 81001\n",
      "mean reward (100 episodes) -121.902611\n",
      "best mean reward -121.902611\n",
      "running time 145.656876\n",
      "Train_EnvstepsSoFar : 81001\n",
      "Train_AverageReturn : -121.902610681672\n",
      "Train_BestReturn : -121.902610681672\n",
      "TimeSinceStart : 145.65687608718872\n",
      "Training Loss : 0.0666479766368866\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 82001\n",
      "mean reward (100 episodes) -124.557036\n",
      "best mean reward -121.902611\n",
      "running time 147.560750\n",
      "Train_EnvstepsSoFar : 82001\n",
      "Train_AverageReturn : -124.55703643963449\n",
      "Train_BestReturn : -121.902610681672\n",
      "TimeSinceStart : 147.5607500076294\n",
      "Training Loss : 0.23897039890289307\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 83001\n",
      "mean reward (100 episodes) -122.357693\n",
      "best mean reward -121.902611\n",
      "running time 150.184117\n",
      "Train_EnvstepsSoFar : 83001\n",
      "Train_AverageReturn : -122.35769343764282\n",
      "Train_BestReturn : -121.902610681672\n",
      "TimeSinceStart : 150.18411707878113\n",
      "Training Loss : 0.08454637229442596\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 84001\n",
      "mean reward (100 episodes) -122.354054\n",
      "best mean reward -121.902611\n",
      "running time 152.556348\n",
      "Train_EnvstepsSoFar : 84001\n",
      "Train_AverageReturn : -122.35405423523291\n",
      "Train_BestReturn : -121.902610681672\n",
      "TimeSinceStart : 152.55634784698486\n",
      "Training Loss : 0.13490495085716248\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 85001\n",
      "mean reward (100 episodes) -117.952824\n",
      "best mean reward -117.952824\n",
      "running time 155.195065\n",
      "Train_EnvstepsSoFar : 85001\n",
      "Train_AverageReturn : -117.95282444999735\n",
      "Train_BestReturn : -117.95282444999735\n",
      "TimeSinceStart : 155.1950650215149\n",
      "Training Loss : 0.21929670870304108\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 86001\n",
      "mean reward (100 episodes) -110.265087\n",
      "best mean reward -110.265087\n",
      "running time 157.981662\n",
      "Train_EnvstepsSoFar : 86001\n",
      "Train_AverageReturn : -110.26508670340868\n",
      "Train_BestReturn : -110.26508670340868\n",
      "TimeSinceStart : 157.9816620349884\n",
      "Training Loss : 0.11401467025279999\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 87001\n",
      "mean reward (100 episodes) -107.131229\n",
      "best mean reward -107.131229\n",
      "running time 160.117848\n",
      "Train_EnvstepsSoFar : 87001\n",
      "Train_AverageReturn : -107.13122925398561\n",
      "Train_BestReturn : -107.13122925398561\n",
      "TimeSinceStart : 160.1178481578827\n",
      "Training Loss : 0.07343445718288422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 88001\n",
      "mean reward (100 episodes) -105.662874\n",
      "best mean reward -105.662874\n",
      "running time 162.112574\n",
      "Train_EnvstepsSoFar : 88001\n",
      "Train_AverageReturn : -105.66287428687616\n",
      "Train_BestReturn : -105.66287428687616\n",
      "TimeSinceStart : 162.11257410049438\n",
      "Training Loss : 0.1353798806667328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 89001\n",
      "mean reward (100 episodes) -98.012859\n",
      "best mean reward -98.012859\n",
      "running time 163.817965\n",
      "Train_EnvstepsSoFar : 89001\n",
      "Train_AverageReturn : -98.0128591316306\n",
      "Train_BestReturn : -98.0128591316306\n",
      "TimeSinceStart : 163.81796503067017\n",
      "Training Loss : 0.34880292415618896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -100.315273\n",
      "best mean reward -98.012859\n",
      "running time 165.562563\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -100.31527260968043\n",
      "Train_BestReturn : -98.0128591316306\n",
      "TimeSinceStart : 165.56256318092346\n",
      "Training Loss : 0.10875733196735382\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 91001\n",
      "mean reward (100 episodes) -96.258249\n",
      "best mean reward -96.258249\n",
      "running time 168.750254\n",
      "Train_EnvstepsSoFar : 91001\n",
      "Train_AverageReturn : -96.25824908450289\n",
      "Train_BestReturn : -96.25824908450289\n",
      "TimeSinceStart : 168.75025415420532\n",
      "Training Loss : 0.11085883527994156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 92001\n",
      "mean reward (100 episodes) -94.952132\n",
      "best mean reward -94.952132\n",
      "running time 172.015651\n",
      "Train_EnvstepsSoFar : 92001\n",
      "Train_AverageReturn : -94.95213212438915\n",
      "Train_BestReturn : -94.95213212438915\n",
      "TimeSinceStart : 172.0156512260437\n",
      "Training Loss : 0.9191960692405701\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 93001\n",
      "mean reward (100 episodes) -93.897529\n",
      "best mean reward -93.897529\n",
      "running time 173.703175\n",
      "Train_EnvstepsSoFar : 93001\n",
      "Train_AverageReturn : -93.89752872791327\n",
      "Train_BestReturn : -93.89752872791327\n",
      "TimeSinceStart : 173.7031750679016\n",
      "Training Loss : 0.3010163903236389\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 94001\n",
      "mean reward (100 episodes) -88.359805\n",
      "best mean reward -88.359805\n",
      "running time 175.383436\n",
      "Train_EnvstepsSoFar : 94001\n",
      "Train_AverageReturn : -88.35980463686326\n",
      "Train_BestReturn : -88.35980463686326\n",
      "TimeSinceStart : 175.38343596458435\n",
      "Training Loss : 0.1968233585357666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 95001\n",
      "mean reward (100 episodes) -88.138948\n",
      "best mean reward -88.138948\n",
      "running time 177.230089\n",
      "Train_EnvstepsSoFar : 95001\n",
      "Train_AverageReturn : -88.13894758072001\n",
      "Train_BestReturn : -88.13894758072001\n",
      "TimeSinceStart : 177.2300889492035\n",
      "Training Loss : 0.20088009536266327\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 96001\n",
      "mean reward (100 episodes) -83.491751\n",
      "best mean reward -83.491751\n",
      "running time 178.316740\n",
      "Train_EnvstepsSoFar : 96001\n",
      "Train_AverageReturn : -83.49175110960292\n",
      "Train_BestReturn : -83.49175110960292\n",
      "TimeSinceStart : 178.31674003601074\n",
      "Training Loss : 0.12485859543085098\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 97001\n",
      "mean reward (100 episodes) -84.549825\n",
      "best mean reward -83.491751\n",
      "running time 179.524871\n",
      "Train_EnvstepsSoFar : 97001\n",
      "Train_AverageReturn : -84.54982496247243\n",
      "Train_BestReturn : -83.49175110960292\n",
      "TimeSinceStart : 179.52487087249756\n",
      "Training Loss : 0.08641243726015091\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 98001\n",
      "mean reward (100 episodes) -81.885166\n",
      "best mean reward -81.885166\n",
      "running time 180.796221\n",
      "Train_EnvstepsSoFar : 98001\n",
      "Train_AverageReturn : -81.88516614998694\n",
      "Train_BestReturn : -81.88516614998694\n",
      "TimeSinceStart : 180.79622101783752\n",
      "Training Loss : 0.950239360332489\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 99001\n",
      "mean reward (100 episodes) -80.370283\n",
      "best mean reward -80.370283\n",
      "running time 182.276310\n",
      "Train_EnvstepsSoFar : 99001\n",
      "Train_AverageReturn : -80.37028306188185\n",
      "Train_BestReturn : -80.37028306188185\n",
      "TimeSinceStart : 182.27630996704102\n",
      "Training Loss : 0.11938083171844482\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -75.875499\n",
      "best mean reward -75.875499\n",
      "running time 183.637647\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -75.87549852191755\n",
      "Train_BestReturn : -75.87549852191755\n",
      "TimeSinceStart : 183.63764715194702\n",
      "Training Loss : 0.8066297769546509\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 101001\n",
      "mean reward (100 episodes) -74.749632\n",
      "best mean reward -74.749632\n",
      "running time 185.095235\n",
      "Train_EnvstepsSoFar : 101001\n",
      "Train_AverageReturn : -74.74963178302052\n",
      "Train_BestReturn : -74.74963178302052\n",
      "TimeSinceStart : 185.09523510932922\n",
      "Training Loss : 0.22106832265853882\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 102001\n",
      "mean reward (100 episodes) -72.515685\n",
      "best mean reward -72.515685\n",
      "running time 186.804865\n",
      "Train_EnvstepsSoFar : 102001\n",
      "Train_AverageReturn : -72.51568517942941\n",
      "Train_BestReturn : -72.51568517942941\n",
      "TimeSinceStart : 186.80486512184143\n",
      "Training Loss : 0.5811572670936584\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 103001\n",
      "mean reward (100 episodes) -73.144088\n",
      "best mean reward -72.515685\n",
      "running time 188.358704\n",
      "Train_EnvstepsSoFar : 103001\n",
      "Train_AverageReturn : -73.14408847137705\n",
      "Train_BestReturn : -72.51568517942941\n",
      "TimeSinceStart : 188.3587040901184\n",
      "Training Loss : 0.14954492449760437\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 104001\n",
      "mean reward (100 episodes) -72.143530\n",
      "best mean reward -72.143530\n",
      "running time 189.817479\n",
      "Train_EnvstepsSoFar : 104001\n",
      "Train_AverageReturn : -72.14353028922851\n",
      "Train_BestReturn : -72.14353028922851\n",
      "TimeSinceStart : 189.81747889518738\n",
      "Training Loss : 0.11047258973121643\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 105001\n",
      "mean reward (100 episodes) -72.820793\n",
      "best mean reward -72.143530\n",
      "running time 191.597489\n",
      "Train_EnvstepsSoFar : 105001\n",
      "Train_AverageReturn : -72.82079345724162\n",
      "Train_BestReturn : -72.14353028922851\n",
      "TimeSinceStart : 191.59748888015747\n",
      "Training Loss : 0.12722811102867126\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 106001\n",
      "mean reward (100 episodes) -70.886240\n",
      "best mean reward -70.886240\n",
      "running time 193.401464\n",
      "Train_EnvstepsSoFar : 106001\n",
      "Train_AverageReturn : -70.88624017472134\n",
      "Train_BestReturn : -70.88624017472134\n",
      "TimeSinceStart : 193.4014642238617\n",
      "Training Loss : 0.615674614906311\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 107001\n",
      "mean reward (100 episodes) -70.083441\n",
      "best mean reward -70.083441\n",
      "running time 194.810805\n",
      "Train_EnvstepsSoFar : 107001\n",
      "Train_AverageReturn : -70.08344126891548\n",
      "Train_BestReturn : -70.08344126891548\n",
      "TimeSinceStart : 194.81080508232117\n",
      "Training Loss : 0.14102989435195923\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 108001\n",
      "mean reward (100 episodes) -70.463742\n",
      "best mean reward -70.083441\n",
      "running time 196.139740\n",
      "Train_EnvstepsSoFar : 108001\n",
      "Train_AverageReturn : -70.46374217034275\n",
      "Train_BestReturn : -70.08344126891548\n",
      "TimeSinceStart : 196.13973999023438\n",
      "Training Loss : 0.22285684943199158\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 109001\n",
      "mean reward (100 episodes) -70.791349\n",
      "best mean reward -70.083441\n",
      "running time 197.719874\n",
      "Train_EnvstepsSoFar : 109001\n",
      "Train_AverageReturn : -70.79134900840285\n",
      "Train_BestReturn : -70.08344126891548\n",
      "TimeSinceStart : 197.71987414360046\n",
      "Training Loss : 0.16413475573062897\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -71.355651\n",
      "best mean reward -70.083441\n",
      "running time 200.281904\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -71.35565123511668\n",
      "Train_BestReturn : -70.08344126891548\n",
      "TimeSinceStart : 200.28190422058105\n",
      "Training Loss : 0.17366839945316315\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 111001\n",
      "mean reward (100 episodes) -70.870151\n",
      "best mean reward -70.083441\n",
      "running time 202.457401\n",
      "Train_EnvstepsSoFar : 111001\n",
      "Train_AverageReturn : -70.87015083929134\n",
      "Train_BestReturn : -70.08344126891548\n",
      "TimeSinceStart : 202.4574010372162\n",
      "Training Loss : 0.15038636326789856\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 112001\n",
      "mean reward (100 episodes) -68.257934\n",
      "best mean reward -68.257934\n",
      "running time 204.260451\n",
      "Train_EnvstepsSoFar : 112001\n",
      "Train_AverageReturn : -68.2579343030918\n",
      "Train_BestReturn : -68.2579343030918\n",
      "TimeSinceStart : 204.26045107841492\n",
      "Training Loss : 0.09544381499290466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 113001\n",
      "mean reward (100 episodes) -66.938099\n",
      "best mean reward -66.938099\n",
      "running time 205.718145\n",
      "Train_EnvstepsSoFar : 113001\n",
      "Train_AverageReturn : -66.93809905296786\n",
      "Train_BestReturn : -66.93809905296786\n",
      "TimeSinceStart : 205.71814513206482\n",
      "Training Loss : 0.23233725130558014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 114001\n",
      "mean reward (100 episodes) -66.670441\n",
      "best mean reward -66.670441\n",
      "running time 207.107598\n",
      "Train_EnvstepsSoFar : 114001\n",
      "Train_AverageReturn : -66.67044080716609\n",
      "Train_BestReturn : -66.67044080716609\n",
      "TimeSinceStart : 207.10759806632996\n",
      "Training Loss : 0.21011117100715637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 115001\n",
      "mean reward (100 episodes) -64.942720\n",
      "best mean reward -64.942720\n",
      "running time 209.157507\n",
      "Train_EnvstepsSoFar : 115001\n",
      "Train_AverageReturn : -64.94272032112181\n",
      "Train_BestReturn : -64.94272032112181\n",
      "TimeSinceStart : 209.1575071811676\n",
      "Training Loss : 0.1489979326725006\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 116001\n",
      "mean reward (100 episodes) -63.051677\n",
      "best mean reward -63.051677\n",
      "running time 210.534792\n",
      "Train_EnvstepsSoFar : 116001\n",
      "Train_AverageReturn : -63.051676605834444\n",
      "Train_BestReturn : -63.051676605834444\n",
      "TimeSinceStart : 210.53479194641113\n",
      "Training Loss : 0.08689509332180023\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 117001\n",
      "mean reward (100 episodes) -60.734998\n",
      "best mean reward -60.734998\n",
      "running time 212.073316\n",
      "Train_EnvstepsSoFar : 117001\n",
      "Train_AverageReturn : -60.73499766610005\n",
      "Train_BestReturn : -60.73499766610005\n",
      "TimeSinceStart : 212.07331609725952\n",
      "Training Loss : 0.22932134568691254\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 118001\n",
      "mean reward (100 episodes) -57.802228\n",
      "best mean reward -57.802228\n",
      "running time 213.387356\n",
      "Train_EnvstepsSoFar : 118001\n",
      "Train_AverageReturn : -57.80222781195952\n",
      "Train_BestReturn : -57.80222781195952\n",
      "TimeSinceStart : 213.38735604286194\n",
      "Training Loss : 0.2131643444299698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 119001\n",
      "mean reward (100 episodes) -53.395022\n",
      "best mean reward -53.395022\n",
      "running time 214.687482\n",
      "Train_EnvstepsSoFar : 119001\n",
      "Train_AverageReturn : -53.39502193026693\n",
      "Train_BestReturn : -53.39502193026693\n",
      "TimeSinceStart : 214.68748211860657\n",
      "Training Loss : 0.06167854368686676\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -53.224675\n",
      "best mean reward -53.224675\n",
      "running time 216.401323\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -53.224675429515266\n",
      "Train_BestReturn : -53.224675429515266\n",
      "TimeSinceStart : 216.40132308006287\n",
      "Training Loss : 0.10474808514118195\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 121001\n",
      "mean reward (100 episodes) -52.556263\n",
      "best mean reward -52.556263\n",
      "running time 217.850397\n",
      "Train_EnvstepsSoFar : 121001\n",
      "Train_AverageReturn : -52.556262732601915\n",
      "Train_BestReturn : -52.556262732601915\n",
      "TimeSinceStart : 217.85039687156677\n",
      "Training Loss : 0.17110031843185425\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 122001\n",
      "mean reward (100 episodes) -49.239340\n",
      "best mean reward -49.239340\n",
      "running time 219.136745\n",
      "Train_EnvstepsSoFar : 122001\n",
      "Train_AverageReturn : -49.239339995374394\n",
      "Train_BestReturn : -49.239339995374394\n",
      "TimeSinceStart : 219.1367449760437\n",
      "Training Loss : 0.1132570207118988\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 123001\n",
      "mean reward (100 episodes) -43.723087\n",
      "best mean reward -43.723087\n",
      "running time 220.471663\n",
      "Train_EnvstepsSoFar : 123001\n",
      "Train_AverageReturn : -43.72308697928802\n",
      "Train_BestReturn : -43.72308697928802\n",
      "TimeSinceStart : 220.47166323661804\n",
      "Training Loss : 0.33282604813575745\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 124001\n",
      "mean reward (100 episodes) -41.218111\n",
      "best mean reward -41.218111\n",
      "running time 221.671915\n",
      "Train_EnvstepsSoFar : 124001\n",
      "Train_AverageReturn : -41.218110511838866\n",
      "Train_BestReturn : -41.218110511838866\n",
      "TimeSinceStart : 221.6719150543213\n",
      "Training Loss : 0.0919577106833458\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 125001\n",
      "mean reward (100 episodes) -38.495378\n",
      "best mean reward -38.495378\n",
      "running time 223.509976\n",
      "Train_EnvstepsSoFar : 125001\n",
      "Train_AverageReturn : -38.49537803802882\n",
      "Train_BestReturn : -38.49537803802882\n",
      "TimeSinceStart : 223.50997614860535\n",
      "Training Loss : 0.23832640051841736\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 126001\n",
      "mean reward (100 episodes) -36.954676\n",
      "best mean reward -36.954676\n",
      "running time 224.981448\n",
      "Train_EnvstepsSoFar : 126001\n",
      "Train_AverageReturn : -36.95467577099022\n",
      "Train_BestReturn : -36.95467577099022\n",
      "TimeSinceStart : 224.98144793510437\n",
      "Training Loss : 0.8578073978424072\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 127001\n",
      "mean reward (100 episodes) -36.464800\n",
      "best mean reward -36.464800\n",
      "running time 227.193001\n",
      "Train_EnvstepsSoFar : 127001\n",
      "Train_AverageReturn : -36.464800376334345\n",
      "Train_BestReturn : -36.464800376334345\n",
      "TimeSinceStart : 227.1930010318756\n",
      "Training Loss : 0.12550953030586243\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 128001\n",
      "mean reward (100 episodes) -32.208246\n",
      "best mean reward -32.208246\n",
      "running time 228.685692\n",
      "Train_EnvstepsSoFar : 128001\n",
      "Train_AverageReturn : -32.20824576031971\n",
      "Train_BestReturn : -32.20824576031971\n",
      "TimeSinceStart : 228.6856918334961\n",
      "Training Loss : 0.10378776490688324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 129001\n",
      "mean reward (100 episodes) -31.021940\n",
      "best mean reward -31.021940\n",
      "running time 229.824557\n",
      "Train_EnvstepsSoFar : 129001\n",
      "Train_AverageReturn : -31.021939914176123\n",
      "Train_BestReturn : -31.021939914176123\n",
      "TimeSinceStart : 229.82455706596375\n",
      "Training Loss : 1.0306140184402466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -27.215518\n",
      "best mean reward -27.215518\n",
      "running time 231.518754\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -27.21551812780799\n",
      "Train_BestReturn : -27.21551812780799\n",
      "TimeSinceStart : 231.51875400543213\n",
      "Training Loss : 4.674423694610596\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 131001\n",
      "mean reward (100 episodes) -26.990893\n",
      "best mean reward -26.990893\n",
      "running time 233.338336\n",
      "Train_EnvstepsSoFar : 131001\n",
      "Train_AverageReturn : -26.990893385806498\n",
      "Train_BestReturn : -26.990893385806498\n",
      "TimeSinceStart : 233.33833599090576\n",
      "Training Loss : 3.356550693511963\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 132001\n",
      "mean reward (100 episodes) -19.923402\n",
      "best mean reward -19.923402\n",
      "running time 234.483032\n",
      "Train_EnvstepsSoFar : 132001\n",
      "Train_AverageReturn : -19.92340151989681\n",
      "Train_BestReturn : -19.92340151989681\n",
      "TimeSinceStart : 234.4830322265625\n",
      "Training Loss : 0.1254003494977951\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 133001\n",
      "mean reward (100 episodes) -16.047119\n",
      "best mean reward -16.047119\n",
      "running time 236.080047\n",
      "Train_EnvstepsSoFar : 133001\n",
      "Train_AverageReturn : -16.047118715470774\n",
      "Train_BestReturn : -16.047118715470774\n",
      "TimeSinceStart : 236.08004713058472\n",
      "Training Loss : 0.29048052430152893\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 134001\n",
      "mean reward (100 episodes) -16.215600\n",
      "best mean reward -16.047119\n",
      "running time 237.739162\n",
      "Train_EnvstepsSoFar : 134001\n",
      "Train_AverageReturn : -16.21560009984206\n",
      "Train_BestReturn : -16.047118715470774\n",
      "TimeSinceStart : 237.73916220664978\n",
      "Training Loss : 0.732316792011261\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 135001\n",
      "mean reward (100 episodes) -11.604190\n",
      "best mean reward -11.604190\n",
      "running time 238.849323\n",
      "Train_EnvstepsSoFar : 135001\n",
      "Train_AverageReturn : -11.604189555864368\n",
      "Train_BestReturn : -11.604189555864368\n",
      "TimeSinceStart : 238.8493230342865\n",
      "Training Loss : 0.10831072926521301\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 136001\n",
      "mean reward (100 episodes) -9.108886\n",
      "best mean reward -9.108886\n",
      "running time 240.108392\n",
      "Train_EnvstepsSoFar : 136001\n",
      "Train_AverageReturn : -9.108885717525318\n",
      "Train_BestReturn : -9.108885717525318\n",
      "TimeSinceStart : 240.10839200019836\n",
      "Training Loss : 0.29996398091316223\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 137001\n",
      "mean reward (100 episodes) -8.158023\n",
      "best mean reward -8.158023\n",
      "running time 241.987121\n",
      "Train_EnvstepsSoFar : 137001\n",
      "Train_AverageReturn : -8.158023184038342\n",
      "Train_BestReturn : -8.158023184038342\n",
      "TimeSinceStart : 241.9871208667755\n",
      "Training Loss : 0.5020495653152466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 138001\n",
      "mean reward (100 episodes) -8.399736\n",
      "best mean reward -8.158023\n",
      "running time 244.091291\n",
      "Train_EnvstepsSoFar : 138001\n",
      "Train_AverageReturn : -8.399735612729305\n",
      "Train_BestReturn : -8.158023184038342\n",
      "TimeSinceStart : 244.09129118919373\n",
      "Training Loss : 0.0743071436882019\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 139001\n",
      "mean reward (100 episodes) -6.562738\n",
      "best mean reward -6.562738\n",
      "running time 246.079757\n",
      "Train_EnvstepsSoFar : 139001\n",
      "Train_AverageReturn : -6.562738399180913\n",
      "Train_BestReturn : -6.562738399180913\n",
      "TimeSinceStart : 246.07975721359253\n",
      "Training Loss : 0.061442069709300995\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) -4.225134\n",
      "best mean reward -4.225134\n",
      "running time 247.326613\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : -4.225134029644019\n",
      "Train_BestReturn : -4.225134029644019\n",
      "TimeSinceStart : 247.32661294937134\n",
      "Training Loss : 0.25806793570518494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 141001\n",
      "mean reward (100 episodes) -3.231617\n",
      "best mean reward -3.231617\n",
      "running time 249.457292\n",
      "Train_EnvstepsSoFar : 141001\n",
      "Train_AverageReturn : -3.2316168087928525\n",
      "Train_BestReturn : -3.2316168087928525\n",
      "TimeSinceStart : 249.45729207992554\n",
      "Training Loss : 1.0172710418701172\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 142001\n",
      "mean reward (100 episodes) -3.595845\n",
      "best mean reward -3.231617\n",
      "running time 251.186880\n",
      "Train_EnvstepsSoFar : 142001\n",
      "Train_AverageReturn : -3.5958451249362446\n",
      "Train_BestReturn : -3.2316168087928525\n",
      "TimeSinceStart : 251.18688011169434\n",
      "Training Loss : 0.8061370849609375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 143001\n",
      "mean reward (100 episodes) -2.915117\n",
      "best mean reward -2.915117\n",
      "running time 253.611301\n",
      "Train_EnvstepsSoFar : 143001\n",
      "Train_AverageReturn : -2.9151173880298766\n",
      "Train_BestReturn : -2.9151173880298766\n",
      "TimeSinceStart : 253.61130118370056\n",
      "Training Loss : 1.7078510522842407\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 144001\n",
      "mean reward (100 episodes) 0.178353\n",
      "best mean reward 0.178353\n",
      "running time 254.511163\n",
      "Train_EnvstepsSoFar : 144001\n",
      "Train_AverageReturn : 0.17835301654464672\n",
      "Train_BestReturn : 0.17835301654464672\n",
      "TimeSinceStart : 254.51116299629211\n",
      "Training Loss : 0.3118578791618347\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 145001\n",
      "mean reward (100 episodes) 2.207084\n",
      "best mean reward 2.207084\n",
      "running time 256.391278\n",
      "Train_EnvstepsSoFar : 145001\n",
      "Train_AverageReturn : 2.2070839472770776\n",
      "Train_BestReturn : 2.2070839472770776\n",
      "TimeSinceStart : 256.39127802848816\n",
      "Training Loss : 0.8934835195541382\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 146001\n",
      "mean reward (100 episodes) 3.403741\n",
      "best mean reward 3.403741\n",
      "running time 257.870817\n",
      "Train_EnvstepsSoFar : 146001\n",
      "Train_AverageReturn : 3.4037405870188824\n",
      "Train_BestReturn : 3.4037405870188824\n",
      "TimeSinceStart : 257.87081718444824\n",
      "Training Loss : 0.5221611857414246\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 147001\n",
      "mean reward (100 episodes) 5.098128\n",
      "best mean reward 5.098128\n",
      "running time 258.950376\n",
      "Train_EnvstepsSoFar : 147001\n",
      "Train_AverageReturn : 5.098128462893404\n",
      "Train_BestReturn : 5.098128462893404\n",
      "TimeSinceStart : 258.95037603378296\n",
      "Training Loss : 0.07997296005487442\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 148001\n",
      "mean reward (100 episodes) 5.208616\n",
      "best mean reward 5.208616\n",
      "running time 260.060073\n",
      "Train_EnvstepsSoFar : 148001\n",
      "Train_AverageReturn : 5.208615954173573\n",
      "Train_BestReturn : 5.208615954173573\n",
      "TimeSinceStart : 260.06007289886475\n",
      "Training Loss : 0.21710826456546783\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 149001\n",
      "mean reward (100 episodes) 7.664053\n",
      "best mean reward 7.664053\n",
      "running time 261.318489\n",
      "Train_EnvstepsSoFar : 149001\n",
      "Train_AverageReturn : 7.664052718584815\n",
      "Train_BestReturn : 7.664052718584815\n",
      "TimeSinceStart : 261.31848907470703\n",
      "Training Loss : 0.14042852818965912\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 13.791329\n",
      "best mean reward 13.791329\n",
      "running time 262.417822\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 13.79132947072468\n",
      "Train_BestReturn : 13.79132947072468\n",
      "TimeSinceStart : 262.4178218841553\n",
      "Training Loss : 1.7937722206115723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 151001\n",
      "mean reward (100 episodes) 16.355069\n",
      "best mean reward 16.355069\n",
      "running time 263.925330\n",
      "Train_EnvstepsSoFar : 151001\n",
      "Train_AverageReturn : 16.355069021398197\n",
      "Train_BestReturn : 16.355069021398197\n",
      "TimeSinceStart : 263.92532992362976\n",
      "Training Loss : 0.17715579271316528\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 152001\n",
      "mean reward (100 episodes) 20.612438\n",
      "best mean reward 20.612438\n",
      "running time 264.873869\n",
      "Train_EnvstepsSoFar : 152001\n",
      "Train_AverageReturn : 20.61243766586558\n",
      "Train_BestReturn : 20.61243766586558\n",
      "TimeSinceStart : 264.8738691806793\n",
      "Training Loss : 0.19572116434574127\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 153001\n",
      "mean reward (100 episodes) 26.530147\n",
      "best mean reward 26.530147\n",
      "running time 265.955250\n",
      "Train_EnvstepsSoFar : 153001\n",
      "Train_AverageReturn : 26.53014671964727\n",
      "Train_BestReturn : 26.53014671964727\n",
      "TimeSinceStart : 265.95525002479553\n",
      "Training Loss : 0.30841150879859924\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 154001\n",
      "mean reward (100 episodes) 26.765352\n",
      "best mean reward 26.765352\n",
      "running time 266.939583\n",
      "Train_EnvstepsSoFar : 154001\n",
      "Train_AverageReturn : 26.76535223079485\n",
      "Train_BestReturn : 26.76535223079485\n",
      "TimeSinceStart : 266.9395830631256\n",
      "Training Loss : 0.08088034391403198\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 155001\n",
      "mean reward (100 episodes) 25.659565\n",
      "best mean reward 26.765352\n",
      "running time 269.073371\n",
      "Train_EnvstepsSoFar : 155001\n",
      "Train_AverageReturn : 25.65956527440956\n",
      "Train_BestReturn : 26.76535223079485\n",
      "TimeSinceStart : 269.0733711719513\n",
      "Training Loss : 0.6368024349212646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 156001\n",
      "mean reward (100 episodes) 30.164473\n",
      "best mean reward 30.164473\n",
      "running time 270.415462\n",
      "Train_EnvstepsSoFar : 156001\n",
      "Train_AverageReturn : 30.164473420573685\n",
      "Train_BestReturn : 30.164473420573685\n",
      "TimeSinceStart : 270.4154620170593\n",
      "Training Loss : 0.4378395080566406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 157001\n",
      "mean reward (100 episodes) 32.787122\n",
      "best mean reward 32.787122\n",
      "running time 271.499931\n",
      "Train_EnvstepsSoFar : 157001\n",
      "Train_AverageReturn : 32.787122108146875\n",
      "Train_BestReturn : 32.787122108146875\n",
      "TimeSinceStart : 271.49993109703064\n",
      "Training Loss : 0.1900455504655838\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 158001\n",
      "mean reward (100 episodes) 35.939439\n",
      "best mean reward 35.939439\n",
      "running time 272.554731\n",
      "Train_EnvstepsSoFar : 158001\n",
      "Train_AverageReturn : 35.93943921798933\n",
      "Train_BestReturn : 35.93943921798933\n",
      "TimeSinceStart : 272.5547311306\n",
      "Training Loss : 0.06874816864728928\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 159001\n",
      "mean reward (100 episodes) 40.027431\n",
      "best mean reward 40.027431\n",
      "running time 274.040056\n",
      "Train_EnvstepsSoFar : 159001\n",
      "Train_AverageReturn : 40.027430505838616\n",
      "Train_BestReturn : 40.027430505838616\n",
      "TimeSinceStart : 274.0400559902191\n",
      "Training Loss : 0.4380980432033539\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 41.349139\n",
      "best mean reward 41.349139\n",
      "running time 275.111105\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 41.349138764539205\n",
      "Train_BestReturn : 41.349138764539205\n",
      "TimeSinceStart : 275.11110496520996\n",
      "Training Loss : 0.1926247626543045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 161001\n",
      "mean reward (100 episodes) 41.733593\n",
      "best mean reward 41.733593\n",
      "running time 276.757473\n",
      "Train_EnvstepsSoFar : 161001\n",
      "Train_AverageReturn : 41.733593484956835\n",
      "Train_BestReturn : 41.733593484956835\n",
      "TimeSinceStart : 276.75747299194336\n",
      "Training Loss : 1.381348729133606\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 162001\n",
      "mean reward (100 episodes) 41.234036\n",
      "best mean reward 41.733593\n",
      "running time 278.295911\n",
      "Train_EnvstepsSoFar : 162001\n",
      "Train_AverageReturn : 41.234035651102666\n",
      "Train_BestReturn : 41.733593484956835\n",
      "TimeSinceStart : 278.2959110736847\n",
      "Training Loss : 0.13123229146003723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 163001\n",
      "mean reward (100 episodes) 46.249095\n",
      "best mean reward 46.249095\n",
      "running time 279.285980\n",
      "Train_EnvstepsSoFar : 163001\n",
      "Train_AverageReturn : 46.24909530393837\n",
      "Train_BestReturn : 46.24909530393837\n",
      "TimeSinceStart : 279.2859799861908\n",
      "Training Loss : 0.22504451870918274\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 164001\n",
      "mean reward (100 episodes) 48.755648\n",
      "best mean reward 48.755648\n",
      "running time 281.117548\n",
      "Train_EnvstepsSoFar : 164001\n",
      "Train_AverageReturn : 48.75564830227199\n",
      "Train_BestReturn : 48.75564830227199\n",
      "TimeSinceStart : 281.1175479888916\n",
      "Training Loss : 0.08548649400472641\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 165001\n",
      "mean reward (100 episodes) 51.673218\n",
      "best mean reward 51.673218\n",
      "running time 282.434452\n",
      "Train_EnvstepsSoFar : 165001\n",
      "Train_AverageReturn : 51.67321805691192\n",
      "Train_BestReturn : 51.67321805691192\n",
      "TimeSinceStart : 282.43445205688477\n",
      "Training Loss : 0.42816510796546936\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 166001\n",
      "mean reward (100 episodes) 55.442988\n",
      "best mean reward 55.442988\n",
      "running time 283.630668\n",
      "Train_EnvstepsSoFar : 166001\n",
      "Train_AverageReturn : 55.4429879164985\n",
      "Train_BestReturn : 55.4429879164985\n",
      "TimeSinceStart : 283.63066816329956\n",
      "Training Loss : 1.7328964471817017\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 167001\n",
      "mean reward (100 episodes) 56.000089\n",
      "best mean reward 56.000089\n",
      "running time 285.368019\n",
      "Train_EnvstepsSoFar : 167001\n",
      "Train_AverageReturn : 56.000088523166184\n",
      "Train_BestReturn : 56.000088523166184\n",
      "TimeSinceStart : 285.3680191040039\n",
      "Training Loss : 0.16393038630485535\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 168001\n",
      "mean reward (100 episodes) 58.253393\n",
      "best mean reward 58.253393\n",
      "running time 287.103741\n",
      "Train_EnvstepsSoFar : 168001\n",
      "Train_AverageReturn : 58.253392861566205\n",
      "Train_BestReturn : 58.253392861566205\n",
      "TimeSinceStart : 287.10374093055725\n",
      "Training Loss : 0.24483101069927216\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 169001\n",
      "mean reward (100 episodes) 60.300504\n",
      "best mean reward 60.300504\n",
      "running time 288.670819\n",
      "Train_EnvstepsSoFar : 169001\n",
      "Train_AverageReturn : 60.30050394018384\n",
      "Train_BestReturn : 60.30050394018384\n",
      "TimeSinceStart : 288.67081904411316\n",
      "Training Loss : 0.2287171632051468\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 60.692584\n",
      "best mean reward 60.692584\n",
      "running time 289.814749\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 60.692583968002516\n",
      "Train_BestReturn : 60.692583968002516\n",
      "TimeSinceStart : 289.81474900245667\n",
      "Training Loss : 0.1976296454668045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 171001\n",
      "mean reward (100 episodes) 62.320860\n",
      "best mean reward 62.320860\n",
      "running time 290.914826\n",
      "Train_EnvstepsSoFar : 171001\n",
      "Train_AverageReturn : 62.32085978295646\n",
      "Train_BestReturn : 62.32085978295646\n",
      "TimeSinceStart : 290.9148259162903\n",
      "Training Loss : 0.29110920429229736\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 172001\n",
      "mean reward (100 episodes) 67.579998\n",
      "best mean reward 67.579998\n",
      "running time 291.974570\n",
      "Train_EnvstepsSoFar : 172001\n",
      "Train_AverageReturn : 67.57999787069755\n",
      "Train_BestReturn : 67.57999787069755\n",
      "TimeSinceStart : 291.97457003593445\n",
      "Training Loss : 0.3339981138706207\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 173001\n",
      "mean reward (100 episodes) 68.370503\n",
      "best mean reward 68.370503\n",
      "running time 292.927155\n",
      "Train_EnvstepsSoFar : 173001\n",
      "Train_AverageReturn : 68.3705027634554\n",
      "Train_BestReturn : 68.3705027634554\n",
      "TimeSinceStart : 292.9271550178528\n",
      "Training Loss : 0.08588875830173492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 174001\n",
      "mean reward (100 episodes) 68.407503\n",
      "best mean reward 68.407503\n",
      "running time 294.015894\n",
      "Train_EnvstepsSoFar : 174001\n",
      "Train_AverageReturn : 68.40750260144401\n",
      "Train_BestReturn : 68.40750260144401\n",
      "TimeSinceStart : 294.0158941745758\n",
      "Training Loss : 0.1781606376171112\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 175001\n",
      "mean reward (100 episodes) 71.570771\n",
      "best mean reward 71.570771\n",
      "running time 295.322983\n",
      "Train_EnvstepsSoFar : 175001\n",
      "Train_AverageReturn : 71.57077128532481\n",
      "Train_BestReturn : 71.57077128532481\n",
      "TimeSinceStart : 295.3229830265045\n",
      "Training Loss : 0.2606339752674103\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 176001\n",
      "mean reward (100 episodes) 74.876853\n",
      "best mean reward 74.876853\n",
      "running time 296.547958\n",
      "Train_EnvstepsSoFar : 176001\n",
      "Train_AverageReturn : 74.8768526043597\n",
      "Train_BestReturn : 74.8768526043597\n",
      "TimeSinceStart : 296.54795813560486\n",
      "Training Loss : 1.677049160003662\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 177001\n",
      "mean reward (100 episodes) 74.032819\n",
      "best mean reward 74.876853\n",
      "running time 297.802202\n",
      "Train_EnvstepsSoFar : 177001\n",
      "Train_AverageReturn : 74.03281858957642\n",
      "Train_BestReturn : 74.8768526043597\n",
      "TimeSinceStart : 297.80220222473145\n",
      "Training Loss : 1.4198757410049438\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 178001\n",
      "mean reward (100 episodes) 72.722013\n",
      "best mean reward 74.876853\n",
      "running time 299.457469\n",
      "Train_EnvstepsSoFar : 178001\n",
      "Train_AverageReturn : 72.72201287941779\n",
      "Train_BestReturn : 74.8768526043597\n",
      "TimeSinceStart : 299.45746898651123\n",
      "Training Loss : 0.3414873778820038\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 179001\n",
      "mean reward (100 episodes) 75.834958\n",
      "best mean reward 75.834958\n",
      "running time 300.503110\n",
      "Train_EnvstepsSoFar : 179001\n",
      "Train_AverageReturn : 75.83495827768854\n",
      "Train_BestReturn : 75.83495827768854\n",
      "TimeSinceStart : 300.5031099319458\n",
      "Training Loss : 0.08470863848924637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 76.221429\n",
      "best mean reward 76.221429\n",
      "running time 301.698757\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 76.22142933387624\n",
      "Train_BestReturn : 76.22142933387624\n",
      "TimeSinceStart : 301.69875717163086\n",
      "Training Loss : 0.15625585615634918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 181001\n",
      "mean reward (100 episodes) 79.046621\n",
      "best mean reward 79.046621\n",
      "running time 302.873912\n",
      "Train_EnvstepsSoFar : 181001\n",
      "Train_AverageReturn : 79.04662057808518\n",
      "Train_BestReturn : 79.04662057808518\n",
      "TimeSinceStart : 302.87391209602356\n",
      "Training Loss : 0.8042507171630859\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 182001\n",
      "mean reward (100 episodes) 79.362431\n",
      "best mean reward 79.362431\n",
      "running time 304.350840\n",
      "Train_EnvstepsSoFar : 182001\n",
      "Train_AverageReturn : 79.3624308868187\n",
      "Train_BestReturn : 79.3624308868187\n",
      "TimeSinceStart : 304.3508400917053\n",
      "Training Loss : 0.4094831645488739\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 183001\n",
      "mean reward (100 episodes) 80.208872\n",
      "best mean reward 80.208872\n",
      "running time 305.803104\n",
      "Train_EnvstepsSoFar : 183001\n",
      "Train_AverageReturn : 80.20887171750333\n",
      "Train_BestReturn : 80.20887171750333\n",
      "TimeSinceStart : 305.8031041622162\n",
      "Training Loss : 1.1810945272445679\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 184001\n",
      "mean reward (100 episodes) 89.582801\n",
      "best mean reward 89.582801\n",
      "running time 307.021723\n",
      "Train_EnvstepsSoFar : 184001\n",
      "Train_AverageReturn : 89.58280128496571\n",
      "Train_BestReturn : 89.58280128496571\n",
      "TimeSinceStart : 307.02172327041626\n",
      "Training Loss : 0.8051272630691528\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 185001\n",
      "mean reward (100 episodes) 93.422304\n",
      "best mean reward 93.422304\n",
      "running time 308.125829\n",
      "Train_EnvstepsSoFar : 185001\n",
      "Train_AverageReturn : 93.42230372701009\n",
      "Train_BestReturn : 93.42230372701009\n",
      "TimeSinceStart : 308.1258292198181\n",
      "Training Loss : 0.5152523517608643\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 186001\n",
      "mean reward (100 episodes) 96.038605\n",
      "best mean reward 96.038605\n",
      "running time 309.870307\n",
      "Train_EnvstepsSoFar : 186001\n",
      "Train_AverageReturn : 96.03860495684215\n",
      "Train_BestReturn : 96.03860495684215\n",
      "TimeSinceStart : 309.87030720710754\n",
      "Training Loss : 0.14135406911373138\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 187001\n",
      "mean reward (100 episodes) 101.657511\n",
      "best mean reward 101.657511\n",
      "running time 310.764400\n",
      "Train_EnvstepsSoFar : 187001\n",
      "Train_AverageReturn : 101.65751079082142\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 310.76440024375916\n",
      "Training Loss : 0.11773087084293365\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 188001\n",
      "mean reward (100 episodes) 101.184473\n",
      "best mean reward 101.657511\n",
      "running time 312.122926\n",
      "Train_EnvstepsSoFar : 188001\n",
      "Train_AverageReturn : 101.1844732225924\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 312.122926235199\n",
      "Training Loss : 0.18084494769573212\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 189001\n",
      "mean reward (100 episodes) 98.165735\n",
      "best mean reward 101.657511\n",
      "running time 313.299625\n",
      "Train_EnvstepsSoFar : 189001\n",
      "Train_AverageReturn : 98.16573545042793\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 313.29962491989136\n",
      "Training Loss : 0.08690309524536133\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 98.069210\n",
      "best mean reward 101.657511\n",
      "running time 314.295335\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 98.0692099056701\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 314.2953350543976\n",
      "Training Loss : 0.20571020245552063\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 191001\n",
      "mean reward (100 episodes) 96.535074\n",
      "best mean reward 101.657511\n",
      "running time 316.002990\n",
      "Train_EnvstepsSoFar : 191001\n",
      "Train_AverageReturn : 96.53507371728121\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 316.0029900074005\n",
      "Training Loss : 0.09390168637037277\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 192001\n",
      "mean reward (100 episodes) 94.704521\n",
      "best mean reward 101.657511\n",
      "running time 317.123399\n",
      "Train_EnvstepsSoFar : 192001\n",
      "Train_AverageReturn : 94.70452114182257\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 317.12339901924133\n",
      "Training Loss : 0.38122865557670593\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 193001\n",
      "mean reward (100 episodes) 93.063024\n",
      "best mean reward 101.657511\n",
      "running time 319.363656\n",
      "Train_EnvstepsSoFar : 193001\n",
      "Train_AverageReturn : 93.06302426853706\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 319.36365604400635\n",
      "Training Loss : 0.11136352270841599\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 194001\n",
      "mean reward (100 episodes) 97.050187\n",
      "best mean reward 101.657511\n",
      "running time 320.487715\n",
      "Train_EnvstepsSoFar : 194001\n",
      "Train_AverageReturn : 97.05018675893447\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 320.48771500587463\n",
      "Training Loss : 1.509011149406433\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 195001\n",
      "mean reward (100 episodes) 94.922587\n",
      "best mean reward 101.657511\n",
      "running time 321.644036\n",
      "Train_EnvstepsSoFar : 195001\n",
      "Train_AverageReturn : 94.92258708038942\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 321.6440360546112\n",
      "Training Loss : 0.6223700046539307\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 196001\n",
      "mean reward (100 episodes) 93.338306\n",
      "best mean reward 101.657511\n",
      "running time 323.751451\n",
      "Train_EnvstepsSoFar : 196001\n",
      "Train_AverageReturn : 93.33830590637226\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 323.751451253891\n",
      "Training Loss : 0.22415615618228912\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 197001\n",
      "mean reward (100 episodes) 92.352029\n",
      "best mean reward 101.657511\n",
      "running time 325.056232\n",
      "Train_EnvstepsSoFar : 197001\n",
      "Train_AverageReturn : 92.3520291015584\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 325.0562319755554\n",
      "Training Loss : 0.2308586835861206\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 198001\n",
      "mean reward (100 episodes) 88.695487\n",
      "best mean reward 101.657511\n",
      "running time 326.560957\n",
      "Train_EnvstepsSoFar : 198001\n",
      "Train_AverageReturn : 88.6954866144792\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 326.56095695495605\n",
      "Training Loss : 1.3508069515228271\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 199001\n",
      "mean reward (100 episodes) 89.237498\n",
      "best mean reward 101.657511\n",
      "running time 327.577127\n",
      "Train_EnvstepsSoFar : 199001\n",
      "Train_AverageReturn : 89.23749750358019\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 327.5771269798279\n",
      "Training Loss : 0.08175376802682877\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 91.130443\n",
      "best mean reward 101.657511\n",
      "running time 328.569941\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 91.13044327378105\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 328.56994104385376\n",
      "Training Loss : 0.0641724169254303\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 201001\n",
      "mean reward (100 episodes) 92.455639\n",
      "best mean reward 101.657511\n",
      "running time 330.428059\n",
      "Train_EnvstepsSoFar : 201001\n",
      "Train_AverageReturn : 92.45563886456603\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 330.42805910110474\n",
      "Training Loss : 0.10327041149139404\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 202001\n",
      "mean reward (100 episodes) 92.000336\n",
      "best mean reward 101.657511\n",
      "running time 331.457950\n",
      "Train_EnvstepsSoFar : 202001\n",
      "Train_AverageReturn : 92.00033647038636\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 331.45795011520386\n",
      "Training Loss : 0.7463746666908264\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 203001\n",
      "mean reward (100 episodes) 90.542950\n",
      "best mean reward 101.657511\n",
      "running time 332.985526\n",
      "Train_EnvstepsSoFar : 203001\n",
      "Train_AverageReturn : 90.54295035279179\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 332.9855260848999\n",
      "Training Loss : 0.3195495307445526\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 204001\n",
      "mean reward (100 episodes) 88.597552\n",
      "best mean reward 101.657511\n",
      "running time 335.028812\n",
      "Train_EnvstepsSoFar : 204001\n",
      "Train_AverageReturn : 88.59755195757857\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 335.0288121700287\n",
      "Training Loss : 1.4079571962356567\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 205001\n",
      "mean reward (100 episodes) 88.017926\n",
      "best mean reward 101.657511\n",
      "running time 336.924259\n",
      "Train_EnvstepsSoFar : 205001\n",
      "Train_AverageReturn : 88.01792604456413\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 336.924259185791\n",
      "Training Loss : 0.30338233709335327\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 206001\n",
      "mean reward (100 episodes) 91.276703\n",
      "best mean reward 101.657511\n",
      "running time 338.149687\n",
      "Train_EnvstepsSoFar : 206001\n",
      "Train_AverageReturn : 91.27670257607132\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 338.14968705177307\n",
      "Training Loss : 0.310538649559021\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 207001\n",
      "mean reward (100 episodes) 90.448989\n",
      "best mean reward 101.657511\n",
      "running time 339.568625\n",
      "Train_EnvstepsSoFar : 207001\n",
      "Train_AverageReturn : 90.44898906074792\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 339.5686249732971\n",
      "Training Loss : 0.5152984857559204\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 208001\n",
      "mean reward (100 episodes) 91.628118\n",
      "best mean reward 101.657511\n",
      "running time 340.904178\n",
      "Train_EnvstepsSoFar : 208001\n",
      "Train_AverageReturn : 91.62811825346705\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 340.904177904129\n",
      "Training Loss : 1.8881205320358276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 209001\n",
      "mean reward (100 episodes) 94.884125\n",
      "best mean reward 101.657511\n",
      "running time 342.482194\n",
      "Train_EnvstepsSoFar : 209001\n",
      "Train_AverageReturn : 94.88412514498862\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 342.4821939468384\n",
      "Training Loss : 0.08363920450210571\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 95.866312\n",
      "best mean reward 101.657511\n",
      "running time 343.432610\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 95.86631246367746\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 343.4326100349426\n",
      "Training Loss : 0.10918692499399185\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 211001\n",
      "mean reward (100 episodes) 95.360012\n",
      "best mean reward 101.657511\n",
      "running time 344.683140\n",
      "Train_EnvstepsSoFar : 211001\n",
      "Train_AverageReturn : 95.36001197474087\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 344.68314003944397\n",
      "Training Loss : 4.512022495269775\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 212001\n",
      "mean reward (100 episodes) 98.123482\n",
      "best mean reward 101.657511\n",
      "running time 345.986227\n",
      "Train_EnvstepsSoFar : 212001\n",
      "Train_AverageReturn : 98.12348202082954\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 345.98622703552246\n",
      "Training Loss : 0.255077600479126\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 213001\n",
      "mean reward (100 episodes) 95.723729\n",
      "best mean reward 101.657511\n",
      "running time 347.939347\n",
      "Train_EnvstepsSoFar : 213001\n",
      "Train_AverageReturn : 95.72372856368742\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 347.9393470287323\n",
      "Training Loss : 0.10890840739011765\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 214001\n",
      "mean reward (100 episodes) 92.729499\n",
      "best mean reward 101.657511\n",
      "running time 349.330846\n",
      "Train_EnvstepsSoFar : 214001\n",
      "Train_AverageReturn : 92.7294987967296\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 349.3308460712433\n",
      "Training Loss : 0.17228084802627563\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 215001\n",
      "mean reward (100 episodes) 94.065335\n",
      "best mean reward 101.657511\n",
      "running time 350.640475\n",
      "Train_EnvstepsSoFar : 215001\n",
      "Train_AverageReturn : 94.06533497826481\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 350.6404752731323\n",
      "Training Loss : 0.29376062750816345\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 216001\n",
      "mean reward (100 episodes) 95.249834\n",
      "best mean reward 101.657511\n",
      "running time 351.867152\n",
      "Train_EnvstepsSoFar : 216001\n",
      "Train_AverageReturn : 95.2498336530388\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 351.8671522140503\n",
      "Training Loss : 0.07382595539093018\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 217001\n",
      "mean reward (100 episodes) 98.839219\n",
      "best mean reward 101.657511\n",
      "running time 352.792716\n",
      "Train_EnvstepsSoFar : 217001\n",
      "Train_AverageReturn : 98.83921943267012\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 352.79271602630615\n",
      "Training Loss : 0.129474937915802\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 218001\n",
      "mean reward (100 episodes) 97.288927\n",
      "best mean reward 101.657511\n",
      "running time 354.593771\n",
      "Train_EnvstepsSoFar : 218001\n",
      "Train_AverageReturn : 97.28892696777852\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 354.59377098083496\n",
      "Training Loss : 0.19026149809360504\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 219001\n",
      "mean reward (100 episodes) 100.150390\n",
      "best mean reward 101.657511\n",
      "running time 355.475331\n",
      "Train_EnvstepsSoFar : 219001\n",
      "Train_AverageReturn : 100.15038975021989\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 355.47533106803894\n",
      "Training Loss : 0.2144169956445694\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 99.779452\n",
      "best mean reward 101.657511\n",
      "running time 357.265422\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 99.77945223407522\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 357.2654221057892\n",
      "Training Loss : 0.6063336730003357\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 221001\n",
      "mean reward (100 episodes) 94.589147\n",
      "best mean reward 101.657511\n",
      "running time 358.312064\n",
      "Train_EnvstepsSoFar : 221001\n",
      "Train_AverageReturn : 94.58914747785072\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 358.3120639324188\n",
      "Training Loss : 0.28886643052101135\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 222001\n",
      "mean reward (100 episodes) 93.640761\n",
      "best mean reward 101.657511\n",
      "running time 360.308742\n",
      "Train_EnvstepsSoFar : 222001\n",
      "Train_AverageReturn : 93.64076136670785\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 360.3087420463562\n",
      "Training Loss : 0.5623675584793091\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 223001\n",
      "mean reward (100 episodes) 93.783028\n",
      "best mean reward 101.657511\n",
      "running time 361.508946\n",
      "Train_EnvstepsSoFar : 223001\n",
      "Train_AverageReturn : 93.78302759079784\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 361.50894594192505\n",
      "Training Loss : 0.07588493078947067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 224001\n",
      "mean reward (100 episodes) 96.378213\n",
      "best mean reward 101.657511\n",
      "running time 363.521770\n",
      "Train_EnvstepsSoFar : 224001\n",
      "Train_AverageReturn : 96.37821344169839\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 363.52177000045776\n",
      "Training Loss : 0.1925191730260849\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 225001\n",
      "mean reward (100 episodes) 97.059071\n",
      "best mean reward 101.657511\n",
      "running time 364.887785\n",
      "Train_EnvstepsSoFar : 225001\n",
      "Train_AverageReturn : 97.05907090512764\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 364.88778495788574\n",
      "Training Loss : 0.4806636869907379\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 226001\n",
      "mean reward (100 episodes) 96.369516\n",
      "best mean reward 101.657511\n",
      "running time 366.049709\n",
      "Train_EnvstepsSoFar : 226001\n",
      "Train_AverageReturn : 96.36951603511947\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 366.0497090816498\n",
      "Training Loss : 0.6215541958808899\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 227001\n",
      "mean reward (100 episodes) 96.877265\n",
      "best mean reward 101.657511\n",
      "running time 368.102068\n",
      "Train_EnvstepsSoFar : 227001\n",
      "Train_AverageReturn : 96.87726488632013\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 368.1020679473877\n",
      "Training Loss : 4.444819450378418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 228001\n",
      "mean reward (100 episodes) 94.640230\n",
      "best mean reward 101.657511\n",
      "running time 369.331844\n",
      "Train_EnvstepsSoFar : 228001\n",
      "Train_AverageReturn : 94.64023044855894\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 369.3318440914154\n",
      "Training Loss : 0.16608776152133942\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 229001\n",
      "mean reward (100 episodes) 95.293910\n",
      "best mean reward 101.657511\n",
      "running time 370.699358\n",
      "Train_EnvstepsSoFar : 229001\n",
      "Train_AverageReturn : 95.293910367434\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 370.6993579864502\n",
      "Training Loss : 3.33520245552063\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 94.114015\n",
      "best mean reward 101.657511\n",
      "running time 371.897747\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 94.11401495924356\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 371.8977470397949\n",
      "Training Loss : 0.11638795584440231\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 231001\n",
      "mean reward (100 episodes) 99.542760\n",
      "best mean reward 101.657511\n",
      "running time 373.052278\n",
      "Train_EnvstepsSoFar : 231001\n",
      "Train_AverageReturn : 99.54276039375982\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 373.0522780418396\n",
      "Training Loss : 0.42298364639282227\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 232001\n",
      "mean reward (100 episodes) 98.865570\n",
      "best mean reward 101.657511\n",
      "running time 374.013059\n",
      "Train_EnvstepsSoFar : 232001\n",
      "Train_AverageReturn : 98.86556974634608\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 374.0130591392517\n",
      "Training Loss : 0.6949368715286255\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 233001\n",
      "mean reward (100 episodes) 99.964554\n",
      "best mean reward 101.657511\n",
      "running time 375.427709\n",
      "Train_EnvstepsSoFar : 233001\n",
      "Train_AverageReturn : 99.96455411592744\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 375.42770886421204\n",
      "Training Loss : 0.2119903713464737\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 234001\n",
      "mean reward (100 episodes) 99.882775\n",
      "best mean reward 101.657511\n",
      "running time 376.377042\n",
      "Train_EnvstepsSoFar : 234001\n",
      "Train_AverageReturn : 99.88277504262933\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 376.37704205513\n",
      "Training Loss : 0.4886762499809265\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 235001\n",
      "mean reward (100 episodes) 100.201738\n",
      "best mean reward 101.657511\n",
      "running time 377.543066\n",
      "Train_EnvstepsSoFar : 235001\n",
      "Train_AverageReturn : 100.20173832820791\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 377.54306626319885\n",
      "Training Loss : 0.17101356387138367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 236001\n",
      "mean reward (100 episodes) 99.542441\n",
      "best mean reward 101.657511\n",
      "running time 378.853231\n",
      "Train_EnvstepsSoFar : 236001\n",
      "Train_AverageReturn : 99.54244137507656\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 378.85323119163513\n",
      "Training Loss : 1.9060461521148682\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 237001\n",
      "mean reward (100 episodes) 98.925989\n",
      "best mean reward 101.657511\n",
      "running time 379.752226\n",
      "Train_EnvstepsSoFar : 237001\n",
      "Train_AverageReturn : 98.92598894220407\n",
      "Train_BestReturn : 101.65751079082142\n",
      "TimeSinceStart : 379.75222611427307\n",
      "Training Loss : 0.19553914666175842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 238001\n",
      "mean reward (100 episodes) 101.890332\n",
      "best mean reward 101.890332\n",
      "running time 380.999006\n",
      "Train_EnvstepsSoFar : 238001\n",
      "Train_AverageReturn : 101.89033163965125\n",
      "Train_BestReturn : 101.89033163965125\n",
      "TimeSinceStart : 380.9990060329437\n",
      "Training Loss : 0.09791502356529236\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 239001\n",
      "mean reward (100 episodes) 102.190668\n",
      "best mean reward 102.190668\n",
      "running time 382.240336\n",
      "Train_EnvstepsSoFar : 239001\n",
      "Train_AverageReturn : 102.19066777970093\n",
      "Train_BestReturn : 102.19066777970093\n",
      "TimeSinceStart : 382.2403361797333\n",
      "Training Loss : 1.5283122062683105\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 100.815885\n",
      "best mean reward 102.190668\n",
      "running time 383.786477\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 100.81588507601762\n",
      "Train_BestReturn : 102.19066777970093\n",
      "TimeSinceStart : 383.7864770889282\n",
      "Training Loss : 0.08654584735631943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 241001\n",
      "mean reward (100 episodes) 103.434794\n",
      "best mean reward 103.434794\n",
      "running time 385.113275\n",
      "Train_EnvstepsSoFar : 241001\n",
      "Train_AverageReturn : 103.43479386464078\n",
      "Train_BestReturn : 103.43479386464078\n",
      "TimeSinceStart : 385.11327481269836\n",
      "Training Loss : 1.5775103569030762\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 242001\n",
      "mean reward (100 episodes) 106.778998\n",
      "best mean reward 106.778998\n",
      "running time 386.385877\n",
      "Train_EnvstepsSoFar : 242001\n",
      "Train_AverageReturn : 106.77899844248267\n",
      "Train_BestReturn : 106.77899844248267\n",
      "TimeSinceStart : 386.38587713241577\n",
      "Training Loss : 1.1683144569396973\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 243001\n",
      "mean reward (100 episodes) 109.502105\n",
      "best mean reward 109.502105\n",
      "running time 387.723080\n",
      "Train_EnvstepsSoFar : 243001\n",
      "Train_AverageReturn : 109.50210536473973\n",
      "Train_BestReturn : 109.50210536473973\n",
      "TimeSinceStart : 387.72307991981506\n",
      "Training Loss : 0.31884264945983887\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 244001\n",
      "mean reward (100 episodes) 105.015640\n",
      "best mean reward 109.502105\n",
      "running time 389.002146\n",
      "Train_EnvstepsSoFar : 244001\n",
      "Train_AverageReturn : 105.01564039903563\n",
      "Train_BestReturn : 109.50210536473973\n",
      "TimeSinceStart : 389.0021460056305\n",
      "Training Loss : 0.14726270735263824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 245001\n",
      "mean reward (100 episodes) 107.738487\n",
      "best mean reward 109.502105\n",
      "running time 390.070864\n",
      "Train_EnvstepsSoFar : 245001\n",
      "Train_AverageReturn : 107.73848745513858\n",
      "Train_BestReturn : 109.50210536473973\n",
      "TimeSinceStart : 390.07086420059204\n",
      "Training Loss : 2.0116000175476074\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 246001\n",
      "mean reward (100 episodes) 108.687846\n",
      "best mean reward 109.502105\n",
      "running time 391.171863\n",
      "Train_EnvstepsSoFar : 246001\n",
      "Train_AverageReturn : 108.68784580183973\n",
      "Train_BestReturn : 109.50210536473973\n",
      "TimeSinceStart : 391.17186307907104\n",
      "Training Loss : 0.3164021372795105\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 247001\n",
      "mean reward (100 episodes) 111.412432\n",
      "best mean reward 111.412432\n",
      "running time 392.227179\n",
      "Train_EnvstepsSoFar : 247001\n",
      "Train_AverageReturn : 111.41243169897675\n",
      "Train_BestReturn : 111.41243169897675\n",
      "TimeSinceStart : 392.22717905044556\n",
      "Training Loss : 0.5268093943595886\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 248001\n",
      "mean reward (100 episodes) 110.305843\n",
      "best mean reward 111.412432\n",
      "running time 393.526633\n",
      "Train_EnvstepsSoFar : 248001\n",
      "Train_AverageReturn : 110.30584278900716\n",
      "Train_BestReturn : 111.41243169897675\n",
      "TimeSinceStart : 393.5266330242157\n",
      "Training Loss : 0.5995739698410034\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 249001\n",
      "mean reward (100 episodes) 111.469323\n",
      "best mean reward 111.469323\n",
      "running time 395.121761\n",
      "Train_EnvstepsSoFar : 249001\n",
      "Train_AverageReturn : 111.46932302996329\n",
      "Train_BestReturn : 111.46932302996329\n",
      "TimeSinceStart : 395.1217610836029\n",
      "Training Loss : 0.11773649603128433\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 110.412785\n",
      "best mean reward 111.469323\n",
      "running time 396.692004\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 110.41278469255224\n",
      "Train_BestReturn : 111.46932302996329\n",
      "TimeSinceStart : 396.6920042037964\n",
      "Training Loss : 0.15890784561634064\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 251001\n",
      "mean reward (100 episodes) 110.867483\n",
      "best mean reward 111.469323\n",
      "running time 399.079388\n",
      "Train_EnvstepsSoFar : 251001\n",
      "Train_AverageReturn : 110.86748302891357\n",
      "Train_BestReturn : 111.46932302996329\n",
      "TimeSinceStart : 399.0793879032135\n",
      "Training Loss : 0.09276273101568222\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 252001\n",
      "mean reward (100 episodes) 112.624366\n",
      "best mean reward 112.624366\n",
      "running time 400.610530\n",
      "Train_EnvstepsSoFar : 252001\n",
      "Train_AverageReturn : 112.62436588276381\n",
      "Train_BestReturn : 112.62436588276381\n",
      "TimeSinceStart : 400.61053013801575\n",
      "Training Loss : 0.6281700134277344\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 253001\n",
      "mean reward (100 episodes) 113.056205\n",
      "best mean reward 113.056205\n",
      "running time 401.891708\n",
      "Train_EnvstepsSoFar : 253001\n",
      "Train_AverageReturn : 113.05620510787693\n",
      "Train_BestReturn : 113.05620510787693\n",
      "TimeSinceStart : 401.8917078971863\n",
      "Training Loss : 0.8655786514282227\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 254001\n",
      "mean reward (100 episodes) 113.522026\n",
      "best mean reward 113.522026\n",
      "running time 403.342834\n",
      "Train_EnvstepsSoFar : 254001\n",
      "Train_AverageReturn : 113.52202563464056\n",
      "Train_BestReturn : 113.52202563464056\n",
      "TimeSinceStart : 403.3428339958191\n",
      "Training Loss : 0.47240549325942993\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 255001\n",
      "mean reward (100 episodes) 113.543319\n",
      "best mean reward 113.543319\n",
      "running time 404.870200\n",
      "Train_EnvstepsSoFar : 255001\n",
      "Train_AverageReturn : 113.54331926119411\n",
      "Train_BestReturn : 113.54331926119411\n",
      "TimeSinceStart : 404.8702001571655\n",
      "Training Loss : 0.09277558326721191\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 256001\n",
      "mean reward (100 episodes) 112.507016\n",
      "best mean reward 113.543319\n",
      "running time 407.042057\n",
      "Train_EnvstepsSoFar : 256001\n",
      "Train_AverageReturn : 112.50701627976909\n",
      "Train_BestReturn : 113.54331926119411\n",
      "TimeSinceStart : 407.0420570373535\n",
      "Training Loss : 0.3072504997253418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 257001\n",
      "mean reward (100 episodes) 109.154233\n",
      "best mean reward 113.543319\n",
      "running time 408.555914\n",
      "Train_EnvstepsSoFar : 257001\n",
      "Train_AverageReturn : 109.1542329647964\n",
      "Train_BestReturn : 113.54331926119411\n",
      "TimeSinceStart : 408.5559139251709\n",
      "Training Loss : 0.19656576216220856\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 258001\n",
      "mean reward (100 episodes) 111.734177\n",
      "best mean reward 113.543319\n",
      "running time 409.526187\n",
      "Train_EnvstepsSoFar : 258001\n",
      "Train_AverageReturn : 111.73417707026921\n",
      "Train_BestReturn : 113.54331926119411\n",
      "TimeSinceStart : 409.5261869430542\n",
      "Training Loss : 1.1785739660263062\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 259001\n",
      "mean reward (100 episodes) 114.357421\n",
      "best mean reward 114.357421\n",
      "running time 410.566339\n",
      "Train_EnvstepsSoFar : 259001\n",
      "Train_AverageReturn : 114.35742143861513\n",
      "Train_BestReturn : 114.35742143861513\n",
      "TimeSinceStart : 410.5663390159607\n",
      "Training Loss : 0.28680309653282166\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 119.889257\n",
      "best mean reward 119.889257\n",
      "running time 411.552249\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 119.88925653892358\n",
      "Train_BestReturn : 119.88925653892358\n",
      "TimeSinceStart : 411.5522491931915\n",
      "Training Loss : 2.59724760055542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 261001\n",
      "mean reward (100 episodes) 123.190119\n",
      "best mean reward 123.190119\n",
      "running time 412.671156\n",
      "Train_EnvstepsSoFar : 261001\n",
      "Train_AverageReturn : 123.19011859282357\n",
      "Train_BestReturn : 123.19011859282357\n",
      "TimeSinceStart : 412.67115592956543\n",
      "Training Loss : 0.1222565621137619\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 262001\n",
      "mean reward (100 episodes) 123.586392\n",
      "best mean reward 123.586392\n",
      "running time 413.728171\n",
      "Train_EnvstepsSoFar : 262001\n",
      "Train_AverageReturn : 123.5863920055153\n",
      "Train_BestReturn : 123.5863920055153\n",
      "TimeSinceStart : 413.7281711101532\n",
      "Training Loss : 0.3239564299583435\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 263001\n",
      "mean reward (100 episodes) 123.449226\n",
      "best mean reward 123.586392\n",
      "running time 414.726386\n",
      "Train_EnvstepsSoFar : 263001\n",
      "Train_AverageReturn : 123.44922619942409\n",
      "Train_BestReturn : 123.5863920055153\n",
      "TimeSinceStart : 414.72638607025146\n",
      "Training Loss : 0.14403223991394043\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 264001\n",
      "mean reward (100 episodes) 123.381775\n",
      "best mean reward 123.586392\n",
      "running time 415.607912\n",
      "Train_EnvstepsSoFar : 264001\n",
      "Train_AverageReturn : 123.38177524554656\n",
      "Train_BestReturn : 123.5863920055153\n",
      "TimeSinceStart : 415.60791206359863\n",
      "Training Loss : 0.35345402359962463\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 265001\n",
      "mean reward (100 episodes) 123.596272\n",
      "best mean reward 123.596272\n",
      "running time 418.507699\n",
      "Train_EnvstepsSoFar : 265001\n",
      "Train_AverageReturn : 123.59627173889061\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 418.50769901275635\n",
      "Training Loss : 0.3462256193161011\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 266001\n",
      "mean reward (100 episodes) 121.758764\n",
      "best mean reward 123.596272\n",
      "running time 420.437429\n",
      "Train_EnvstepsSoFar : 266001\n",
      "Train_AverageReturn : 121.75876388969787\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 420.4374289512634\n",
      "Training Loss : 0.2729945778846741\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 267001\n",
      "mean reward (100 episodes) 119.667399\n",
      "best mean reward 123.596272\n",
      "running time 421.681087\n",
      "Train_EnvstepsSoFar : 267001\n",
      "Train_AverageReturn : 119.66739868891214\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 421.6810870170593\n",
      "Training Loss : 0.38445261120796204\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 268001\n",
      "mean reward (100 episodes) 118.226965\n",
      "best mean reward 123.596272\n",
      "running time 424.954219\n",
      "Train_EnvstepsSoFar : 268001\n",
      "Train_AverageReturn : 118.22696490520916\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 424.9542191028595\n",
      "Training Loss : 0.12245793640613556\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 269001\n",
      "mean reward (100 episodes) 114.930577\n",
      "best mean reward 123.596272\n",
      "running time 427.070019\n",
      "Train_EnvstepsSoFar : 269001\n",
      "Train_AverageReturn : 114.93057746036914\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 427.0700190067291\n",
      "Training Loss : 0.29653364419937134\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 115.327119\n",
      "best mean reward 123.596272\n",
      "running time 429.419635\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 115.32711877130362\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 429.41963505744934\n",
      "Training Loss : 0.30433130264282227\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 271001\n",
      "mean reward (100 episodes) 113.883976\n",
      "best mean reward 123.596272\n",
      "running time 430.543034\n",
      "Train_EnvstepsSoFar : 271001\n",
      "Train_AverageReturn : 113.88397625008812\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 430.5430340766907\n",
      "Training Loss : 0.8085376024246216\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 272001\n",
      "mean reward (100 episodes) 111.851280\n",
      "best mean reward 123.596272\n",
      "running time 431.888014\n",
      "Train_EnvstepsSoFar : 272001\n",
      "Train_AverageReturn : 111.85128046908291\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 431.88801407814026\n",
      "Training Loss : 1.5516934394836426\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 273001\n",
      "mean reward (100 episodes) 111.499959\n",
      "best mean reward 123.596272\n",
      "running time 433.083355\n",
      "Train_EnvstepsSoFar : 273001\n",
      "Train_AverageReturn : 111.49995854208511\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 433.08335518836975\n",
      "Training Loss : 1.1241693496704102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 274001\n",
      "mean reward (100 episodes) 113.649607\n",
      "best mean reward 123.596272\n",
      "running time 433.965491\n",
      "Train_EnvstepsSoFar : 274001\n",
      "Train_AverageReturn : 113.64960704924971\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 433.96549105644226\n",
      "Training Loss : 0.1438756138086319\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 275001\n",
      "mean reward (100 episodes) 121.750931\n",
      "best mean reward 123.596272\n",
      "running time 435.063238\n",
      "Train_EnvstepsSoFar : 275001\n",
      "Train_AverageReturn : 121.75093116323347\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 435.0632379055023\n",
      "Training Loss : 0.5692411661148071\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 276001\n",
      "mean reward (100 episodes) 122.000824\n",
      "best mean reward 123.596272\n",
      "running time 436.477372\n",
      "Train_EnvstepsSoFar : 276001\n",
      "Train_AverageReturn : 122.00082375266965\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 436.47737193107605\n",
      "Training Loss : 0.3021060824394226\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 277001\n",
      "mean reward (100 episodes) 121.598593\n",
      "best mean reward 123.596272\n",
      "running time 438.140083\n",
      "Train_EnvstepsSoFar : 277001\n",
      "Train_AverageReturn : 121.59859284608208\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 438.1400830745697\n",
      "Training Loss : 0.10378918051719666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 278001\n",
      "mean reward (100 episodes) 119.328954\n",
      "best mean reward 123.596272\n",
      "running time 439.672305\n",
      "Train_EnvstepsSoFar : 278001\n",
      "Train_AverageReturn : 119.32895432339129\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 439.6723048686981\n",
      "Training Loss : 0.143739253282547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 279001\n",
      "mean reward (100 episodes) 118.162783\n",
      "best mean reward 123.596272\n",
      "running time 442.551178\n",
      "Train_EnvstepsSoFar : 279001\n",
      "Train_AverageReturn : 118.16278289255308\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 442.5511779785156\n",
      "Training Loss : 0.267264723777771\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 116.846286\n",
      "best mean reward 123.596272\n",
      "running time 444.765528\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 116.8462861681178\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 444.7655282020569\n",
      "Training Loss : 0.26090067625045776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 281001\n",
      "mean reward (100 episodes) 116.759229\n",
      "best mean reward 123.596272\n",
      "running time 446.355023\n",
      "Train_EnvstepsSoFar : 281001\n",
      "Train_AverageReturn : 116.75922948823946\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 446.35502314567566\n",
      "Training Loss : 0.12011826038360596\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 282001\n",
      "mean reward (100 episodes) 114.102283\n",
      "best mean reward 123.596272\n",
      "running time 447.769128\n",
      "Train_EnvstepsSoFar : 282001\n",
      "Train_AverageReturn : 114.10228252067449\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 447.76912808418274\n",
      "Training Loss : 1.069538950920105\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 283001\n",
      "mean reward (100 episodes) 116.326968\n",
      "best mean reward 123.596272\n",
      "running time 448.835240\n",
      "Train_EnvstepsSoFar : 283001\n",
      "Train_AverageReturn : 116.32696777729312\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 448.83523988723755\n",
      "Training Loss : 2.8848066329956055\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 284001\n",
      "mean reward (100 episodes) 115.492113\n",
      "best mean reward 123.596272\n",
      "running time 451.573716\n",
      "Train_EnvstepsSoFar : 284001\n",
      "Train_AverageReturn : 115.49211289443079\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 451.5737159252167\n",
      "Training Loss : 1.2068296670913696\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 285001\n",
      "mean reward (100 episodes) 119.403695\n",
      "best mean reward 123.596272\n",
      "running time 454.915065\n",
      "Train_EnvstepsSoFar : 285001\n",
      "Train_AverageReturn : 119.4036950471034\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 454.9150650501251\n",
      "Training Loss : 0.16483785212039948\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 286001\n",
      "mean reward (100 episodes) 119.040253\n",
      "best mean reward 123.596272\n",
      "running time 455.933906\n",
      "Train_EnvstepsSoFar : 286001\n",
      "Train_AverageReturn : 119.04025257068518\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 455.9339060783386\n",
      "Training Loss : 0.0990215465426445\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 287001\n",
      "mean reward (100 episodes) 118.523038\n",
      "best mean reward 123.596272\n",
      "running time 458.805860\n",
      "Train_EnvstepsSoFar : 287001\n",
      "Train_AverageReturn : 118.52303764535309\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 458.805860042572\n",
      "Training Loss : 0.08334778994321823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 288001\n",
      "mean reward (100 episodes) 117.758281\n",
      "best mean reward 123.596272\n",
      "running time 461.034349\n",
      "Train_EnvstepsSoFar : 288001\n",
      "Train_AverageReturn : 117.75828146346439\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 461.03434920310974\n",
      "Training Loss : 0.08507557958364487\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 289001\n",
      "mean reward (100 episodes) 115.310433\n",
      "best mean reward 123.596272\n",
      "running time 463.208456\n",
      "Train_EnvstepsSoFar : 289001\n",
      "Train_AverageReturn : 115.31043344916567\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 463.2084560394287\n",
      "Training Loss : 0.4461708664894104\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 114.009783\n",
      "best mean reward 123.596272\n",
      "running time 464.778575\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 114.00978261911436\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 464.77857518196106\n",
      "Training Loss : 0.07426069676876068\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 291001\n",
      "mean reward (100 episodes) 113.720403\n",
      "best mean reward 123.596272\n",
      "running time 467.746503\n",
      "Train_EnvstepsSoFar : 291001\n",
      "Train_AverageReturn : 113.72040315483127\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 467.74650287628174\n",
      "Training Loss : 0.11655620485544205\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 292001\n",
      "mean reward (100 episodes) 111.033603\n",
      "best mean reward 123.596272\n",
      "running time 469.734804\n",
      "Train_EnvstepsSoFar : 292001\n",
      "Train_AverageReturn : 111.0336029963682\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 469.7348039150238\n",
      "Training Loss : 1.3251324892044067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 293001\n",
      "mean reward (100 episodes) 110.615468\n",
      "best mean reward 123.596272\n",
      "running time 473.229549\n",
      "Train_EnvstepsSoFar : 293001\n",
      "Train_AverageReturn : 110.61546824401819\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 473.2295491695404\n",
      "Training Loss : 0.5092160701751709\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 294001\n",
      "mean reward (100 episodes) 113.199611\n",
      "best mean reward 123.596272\n",
      "running time 474.662827\n",
      "Train_EnvstepsSoFar : 294001\n",
      "Train_AverageReturn : 113.19961136614418\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 474.6628270149231\n",
      "Training Loss : 3.20573091506958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 295001\n",
      "mean reward (100 episodes) 112.880067\n",
      "best mean reward 123.596272\n",
      "running time 475.632475\n",
      "Train_EnvstepsSoFar : 295001\n",
      "Train_AverageReturn : 112.88006701026057\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 475.63247513771057\n",
      "Training Loss : 0.13995453715324402\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 296001\n",
      "mean reward (100 episodes) 112.923269\n",
      "best mean reward 123.596272\n",
      "running time 476.848682\n",
      "Train_EnvstepsSoFar : 296001\n",
      "Train_AverageReturn : 112.9232690834802\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 476.8486819267273\n",
      "Training Loss : 0.16006679832935333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 297001\n",
      "mean reward (100 episodes) 109.940742\n",
      "best mean reward 123.596272\n",
      "running time 479.027359\n",
      "Train_EnvstepsSoFar : 297001\n",
      "Train_AverageReturn : 109.94074184519246\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 479.02735900878906\n",
      "Training Loss : 2.888514518737793\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 298001\n",
      "mean reward (100 episodes) 112.863300\n",
      "best mean reward 123.596272\n",
      "running time 480.064791\n",
      "Train_EnvstepsSoFar : 298001\n",
      "Train_AverageReturn : 112.86330007565624\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 480.06479120254517\n",
      "Training Loss : 0.13364234566688538\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 299001\n",
      "mean reward (100 episodes) 112.149217\n",
      "best mean reward 123.596272\n",
      "running time 481.783045\n",
      "Train_EnvstepsSoFar : 299001\n",
      "Train_AverageReturn : 112.14921702717226\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 481.78304505348206\n",
      "Training Loss : 0.5516721606254578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 110.522354\n",
      "best mean reward 123.596272\n",
      "running time 482.992820\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 110.52235380549176\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 482.99282002449036\n",
      "Training Loss : 0.13890618085861206\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 301001\n",
      "mean reward (100 episodes) 110.564268\n",
      "best mean reward 123.596272\n",
      "running time 484.567953\n",
      "Train_EnvstepsSoFar : 301001\n",
      "Train_AverageReturn : 110.56426829868958\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 484.56795287132263\n",
      "Training Loss : 0.13184316456317902\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 302001\n",
      "mean reward (100 episodes) 109.606436\n",
      "best mean reward 123.596272\n",
      "running time 486.115905\n",
      "Train_EnvstepsSoFar : 302001\n",
      "Train_AverageReturn : 109.60643592643859\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 486.115905046463\n",
      "Training Loss : 0.13205832242965698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 303001\n",
      "mean reward (100 episodes) 110.084897\n",
      "best mean reward 123.596272\n",
      "running time 488.565439\n",
      "Train_EnvstepsSoFar : 303001\n",
      "Train_AverageReturn : 110.08489695496118\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 488.5654389858246\n",
      "Training Loss : 0.45645976066589355\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 304001\n",
      "mean reward (100 episodes) 108.045163\n",
      "best mean reward 123.596272\n",
      "running time 490.804660\n",
      "Train_EnvstepsSoFar : 304001\n",
      "Train_AverageReturn : 108.04516304406103\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 490.8046600818634\n",
      "Training Loss : 0.07141581177711487\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 305001\n",
      "mean reward (100 episodes) 106.072429\n",
      "best mean reward 123.596272\n",
      "running time 493.115355\n",
      "Train_EnvstepsSoFar : 305001\n",
      "Train_AverageReturn : 106.07242887147206\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 493.1153552532196\n",
      "Training Loss : 3.19466495513916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 306001\n",
      "mean reward (100 episodes) 104.966927\n",
      "best mean reward 123.596272\n",
      "running time 494.952808\n",
      "Train_EnvstepsSoFar : 306001\n",
      "Train_AverageReturn : 104.96692744604458\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 494.9528079032898\n",
      "Training Loss : 0.12775123119354248\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 307001\n",
      "mean reward (100 episodes) 105.126015\n",
      "best mean reward 123.596272\n",
      "running time 497.747707\n",
      "Train_EnvstepsSoFar : 307001\n",
      "Train_AverageReturn : 105.12601521296676\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 497.7477068901062\n",
      "Training Loss : 0.17952151596546173\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 308001\n",
      "mean reward (100 episodes) 103.072664\n",
      "best mean reward 123.596272\n",
      "running time 499.923738\n",
      "Train_EnvstepsSoFar : 308001\n",
      "Train_AverageReturn : 103.07266391170162\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 499.9237382411957\n",
      "Training Loss : 0.10659483820199966\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 309001\n",
      "mean reward (100 episodes) 100.071971\n",
      "best mean reward 123.596272\n",
      "running time 501.061095\n",
      "Train_EnvstepsSoFar : 309001\n",
      "Train_AverageReturn : 100.07197100577324\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 501.06109499931335\n",
      "Training Loss : 1.003617763519287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 103.009458\n",
      "best mean reward 123.596272\n",
      "running time 502.623866\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 103.00945793353971\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 502.6238660812378\n",
      "Training Loss : 0.09713858366012573\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 311001\n",
      "mean reward (100 episodes) 100.548235\n",
      "best mean reward 123.596272\n",
      "running time 504.893860\n",
      "Train_EnvstepsSoFar : 311001\n",
      "Train_AverageReturn : 100.54823468085033\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 504.89386010169983\n",
      "Training Loss : 0.08204521983861923\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 312001\n",
      "mean reward (100 episodes) 99.639386\n",
      "best mean reward 123.596272\n",
      "running time 507.090641\n",
      "Train_EnvstepsSoFar : 312001\n",
      "Train_AverageReturn : 99.6393857775964\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 507.0906410217285\n",
      "Training Loss : 0.07699155807495117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 313001\n",
      "mean reward (100 episodes) 103.260492\n",
      "best mean reward 123.596272\n",
      "running time 508.569734\n",
      "Train_EnvstepsSoFar : 313001\n",
      "Train_AverageReturn : 103.26049231674702\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 508.5697338581085\n",
      "Training Loss : 0.14573746919631958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 314001\n",
      "mean reward (100 episodes) 103.836893\n",
      "best mean reward 123.596272\n",
      "running time 510.986457\n",
      "Train_EnvstepsSoFar : 314001\n",
      "Train_AverageReturn : 103.83689267653091\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 510.9864571094513\n",
      "Training Loss : 0.7541592121124268\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 315001\n",
      "mean reward (100 episodes) 104.216266\n",
      "best mean reward 123.596272\n",
      "running time 513.269906\n",
      "Train_EnvstepsSoFar : 315001\n",
      "Train_AverageReturn : 104.21626585850407\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 513.2699060440063\n",
      "Training Loss : 0.0997784212231636\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 316001\n",
      "mean reward (100 episodes) 101.814275\n",
      "best mean reward 123.596272\n",
      "running time 514.440594\n",
      "Train_EnvstepsSoFar : 316001\n",
      "Train_AverageReturn : 101.81427493853458\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 514.440593957901\n",
      "Training Loss : 0.0798855870962143\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 317001\n",
      "mean reward (100 episodes) 100.384889\n",
      "best mean reward 123.596272\n",
      "running time 516.227044\n",
      "Train_EnvstepsSoFar : 317001\n",
      "Train_AverageReturn : 100.3848891303107\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 516.2270441055298\n",
      "Training Loss : 0.09336461871862411\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 318001\n",
      "mean reward (100 episodes) 98.971925\n",
      "best mean reward 123.596272\n",
      "running time 518.191839\n",
      "Train_EnvstepsSoFar : 318001\n",
      "Train_AverageReturn : 98.97192452873031\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 518.1918389797211\n",
      "Training Loss : 2.184757709503174\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 319001\n",
      "mean reward (100 episodes) 100.374558\n",
      "best mean reward 123.596272\n",
      "running time 519.526853\n",
      "Train_EnvstepsSoFar : 319001\n",
      "Train_AverageReturn : 100.37455830775922\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 519.5268530845642\n",
      "Training Loss : 0.6030813455581665\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 102.046903\n",
      "best mean reward 123.596272\n",
      "running time 520.957768\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 102.04690349675035\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 520.9577679634094\n",
      "Training Loss : 0.056395649909973145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 321001\n",
      "mean reward (100 episodes) 99.454142\n",
      "best mean reward 123.596272\n",
      "running time 521.776517\n",
      "Train_EnvstepsSoFar : 321001\n",
      "Train_AverageReturn : 99.45414177216807\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 521.7765171527863\n",
      "Training Loss : 0.10831204801797867\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 322001\n",
      "mean reward (100 episodes) 97.863355\n",
      "best mean reward 123.596272\n",
      "running time 524.192313\n",
      "Train_EnvstepsSoFar : 322001\n",
      "Train_AverageReturn : 97.8633550996754\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 524.1923131942749\n",
      "Training Loss : 0.894156277179718\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 323001\n",
      "mean reward (100 episodes) 94.675510\n",
      "best mean reward 123.596272\n",
      "running time 525.204383\n",
      "Train_EnvstepsSoFar : 323001\n",
      "Train_AverageReturn : 94.67550968513427\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 525.2043831348419\n",
      "Training Loss : 0.07615575194358826\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 324001\n",
      "mean reward (100 episodes) 91.258861\n",
      "best mean reward 123.596272\n",
      "running time 525.944868\n",
      "Train_EnvstepsSoFar : 324001\n",
      "Train_AverageReturn : 91.2588614006179\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 525.9448680877686\n",
      "Training Loss : 0.1519896239042282\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 325001\n",
      "mean reward (100 episodes) 93.320207\n",
      "best mean reward 123.596272\n",
      "running time 527.163715\n",
      "Train_EnvstepsSoFar : 325001\n",
      "Train_AverageReturn : 93.32020672756283\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 527.1637151241302\n",
      "Training Loss : 1.4274005889892578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 326001\n",
      "mean reward (100 episodes) 91.438951\n",
      "best mean reward 123.596272\n",
      "running time 528.973788\n",
      "Train_EnvstepsSoFar : 326001\n",
      "Train_AverageReturn : 91.4389512737533\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 528.973788022995\n",
      "Training Loss : 0.08240874111652374\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 327001\n",
      "mean reward (100 episodes) 89.373481\n",
      "best mean reward 123.596272\n",
      "running time 530.434522\n",
      "Train_EnvstepsSoFar : 327001\n",
      "Train_AverageReturn : 89.37348077763305\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 530.434522151947\n",
      "Training Loss : 0.38072469830513\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 328001\n",
      "mean reward (100 episodes) 87.596964\n",
      "best mean reward 123.596272\n",
      "running time 531.426497\n",
      "Train_EnvstepsSoFar : 328001\n",
      "Train_AverageReturn : 87.59696371295612\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 531.4264969825745\n",
      "Training Loss : 0.09727891534566879\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 329001\n",
      "mean reward (100 episodes) 86.362393\n",
      "best mean reward 123.596272\n",
      "running time 532.902181\n",
      "Train_EnvstepsSoFar : 329001\n",
      "Train_AverageReturn : 86.36239328634275\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 532.902181148529\n",
      "Training Loss : 0.1541358381509781\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 86.420811\n",
      "best mean reward 123.596272\n",
      "running time 534.172451\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 86.42081071374176\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 534.1724510192871\n",
      "Training Loss : 0.09776235371828079\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 331001\n",
      "mean reward (100 episodes) 86.274960\n",
      "best mean reward 123.596272\n",
      "running time 536.874447\n",
      "Train_EnvstepsSoFar : 331001\n",
      "Train_AverageReturn : 86.27496041390053\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 536.8744471073151\n",
      "Training Loss : 0.08508888632059097\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 332001\n",
      "mean reward (100 episodes) 87.688326\n",
      "best mean reward 123.596272\n",
      "running time 537.751303\n",
      "Train_EnvstepsSoFar : 332001\n",
      "Train_AverageReturn : 87.68832565328266\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 537.7513029575348\n",
      "Training Loss : 0.10734785348176956\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 333001\n",
      "mean reward (100 episodes) 83.189181\n",
      "best mean reward 123.596272\n",
      "running time 539.743242\n",
      "Train_EnvstepsSoFar : 333001\n",
      "Train_AverageReturn : 83.18918147588181\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 539.7432420253754\n",
      "Training Loss : 0.22778646647930145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 334001\n",
      "mean reward (100 episodes) 82.925559\n",
      "best mean reward 123.596272\n",
      "running time 541.613643\n",
      "Train_EnvstepsSoFar : 334001\n",
      "Train_AverageReturn : 82.92555877884543\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 541.6136431694031\n",
      "Training Loss : 0.24620437622070312\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 335001\n",
      "mean reward (100 episodes) 80.519415\n",
      "best mean reward 123.596272\n",
      "running time 542.705555\n",
      "Train_EnvstepsSoFar : 335001\n",
      "Train_AverageReturn : 80.51941490203903\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 542.7055549621582\n",
      "Training Loss : 1.5614607334136963\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 336001\n",
      "mean reward (100 episodes) 75.740794\n",
      "best mean reward 123.596272\n",
      "running time 545.421684\n",
      "Train_EnvstepsSoFar : 336001\n",
      "Train_AverageReturn : 75.74079384708661\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 545.4216840267181\n",
      "Training Loss : 0.09176813066005707\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 337001\n",
      "mean reward (100 episodes) 72.322347\n",
      "best mean reward 123.596272\n",
      "running time 546.528663\n",
      "Train_EnvstepsSoFar : 337001\n",
      "Train_AverageReturn : 72.322347316741\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 546.5286631584167\n",
      "Training Loss : 1.3503342866897583\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 338001\n",
      "mean reward (100 episodes) 69.326738\n",
      "best mean reward 123.596272\n",
      "running time 547.703874\n",
      "Train_EnvstepsSoFar : 338001\n",
      "Train_AverageReturn : 69.32673770986493\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 547.7038741111755\n",
      "Training Loss : 0.16912494599819183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 339001\n",
      "mean reward (100 episodes) 71.450605\n",
      "best mean reward 123.596272\n",
      "running time 549.199336\n",
      "Train_EnvstepsSoFar : 339001\n",
      "Train_AverageReturn : 71.45060456464398\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 549.1993360519409\n",
      "Training Loss : 0.10326891392469406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 71.791860\n",
      "best mean reward 123.596272\n",
      "running time 550.214049\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 71.79185994129948\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 550.2140491008759\n",
      "Training Loss : 0.2574869394302368\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 341001\n",
      "mean reward (100 episodes) 77.830270\n",
      "best mean reward 123.596272\n",
      "running time 551.635486\n",
      "Train_EnvstepsSoFar : 341001\n",
      "Train_AverageReturn : 77.8302704733592\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 551.6354858875275\n",
      "Training Loss : 0.10972057282924652\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 342001\n",
      "mean reward (100 episodes) 81.334126\n",
      "best mean reward 123.596272\n",
      "running time 552.707109\n",
      "Train_EnvstepsSoFar : 342001\n",
      "Train_AverageReturn : 81.33412569205338\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 552.7071092128754\n",
      "Training Loss : 0.07986778020858765\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 343001\n",
      "mean reward (100 episodes) 86.577290\n",
      "best mean reward 123.596272\n",
      "running time 553.725498\n",
      "Train_EnvstepsSoFar : 343001\n",
      "Train_AverageReturn : 86.57728983362405\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 553.7254981994629\n",
      "Training Loss : 0.16813141107559204\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 344001\n",
      "mean reward (100 episodes) 79.540650\n",
      "best mean reward 123.596272\n",
      "running time 554.552095\n",
      "Train_EnvstepsSoFar : 344001\n",
      "Train_AverageReturn : 79.54065030472056\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 554.5520949363708\n",
      "Training Loss : 1.0990500450134277\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 345001\n",
      "mean reward (100 episodes) 81.821433\n",
      "best mean reward 123.596272\n",
      "running time 555.465643\n",
      "Train_EnvstepsSoFar : 345001\n",
      "Train_AverageReturn : 81.82143273905261\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 555.4656431674957\n",
      "Training Loss : 0.12548142671585083\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 346001\n",
      "mean reward (100 episodes) 83.197196\n",
      "best mean reward 123.596272\n",
      "running time 556.405647\n",
      "Train_EnvstepsSoFar : 346001\n",
      "Train_AverageReturn : 83.197196058076\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 556.4056470394135\n",
      "Training Loss : 0.15156571567058563\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 347001\n",
      "mean reward (100 episodes) 77.728699\n",
      "best mean reward 123.596272\n",
      "running time 557.192320\n",
      "Train_EnvstepsSoFar : 347001\n",
      "Train_AverageReturn : 77.72869882564234\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 557.1923201084137\n",
      "Training Loss : 1.1349241733551025\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 348001\n",
      "mean reward (100 episodes) 76.520759\n",
      "best mean reward 123.596272\n",
      "running time 558.530367\n",
      "Train_EnvstepsSoFar : 348001\n",
      "Train_AverageReturn : 76.52075891185308\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 558.5303671360016\n",
      "Training Loss : 0.09002265334129333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 349001\n",
      "mean reward (100 episodes) 78.198701\n",
      "best mean reward 123.596272\n",
      "running time 559.767015\n",
      "Train_EnvstepsSoFar : 349001\n",
      "Train_AverageReturn : 78.19870060277157\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 559.7670149803162\n",
      "Training Loss : 0.13507580757141113\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 72.367151\n",
      "best mean reward 123.596272\n",
      "running time 560.599694\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 72.36715054608918\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 560.5996940135956\n",
      "Training Loss : 0.4601733982563019\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 351001\n",
      "mean reward (100 episodes) 72.057219\n",
      "best mean reward 123.596272\n",
      "running time 562.738121\n",
      "Train_EnvstepsSoFar : 351001\n",
      "Train_AverageReturn : 72.05721929287436\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 562.7381210327148\n",
      "Training Loss : 0.11075112968683243\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 352001\n",
      "mean reward (100 episodes) 68.656528\n",
      "best mean reward 123.596272\n",
      "running time 564.830065\n",
      "Train_EnvstepsSoFar : 352001\n",
      "Train_AverageReturn : 68.65652776732152\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 564.8300650119781\n",
      "Training Loss : 0.23615385591983795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 353001\n",
      "mean reward (100 episodes) 64.108462\n",
      "best mean reward 123.596272\n",
      "running time 566.049364\n",
      "Train_EnvstepsSoFar : 353001\n",
      "Train_AverageReturn : 64.10846246051572\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 566.0493640899658\n",
      "Training Loss : 0.1473853588104248\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 354001\n",
      "mean reward (100 episodes) 66.896383\n",
      "best mean reward 123.596272\n",
      "running time 568.221971\n",
      "Train_EnvstepsSoFar : 354001\n",
      "Train_AverageReturn : 66.8963826929225\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 568.2219710350037\n",
      "Training Loss : 0.1733275055885315\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 355001\n",
      "mean reward (100 episodes) 68.441615\n",
      "best mean reward 123.596272\n",
      "running time 569.353403\n",
      "Train_EnvstepsSoFar : 355001\n",
      "Train_AverageReturn : 68.44161482435231\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 569.3534028530121\n",
      "Training Loss : 0.6454225778579712\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 356001\n",
      "mean reward (100 episodes) 67.885740\n",
      "best mean reward 123.596272\n",
      "running time 571.004738\n",
      "Train_EnvstepsSoFar : 356001\n",
      "Train_AverageReturn : 67.88573998745252\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 571.0047380924225\n",
      "Training Loss : 0.9754905104637146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 357001\n",
      "mean reward (100 episodes) 63.075322\n",
      "best mean reward 123.596272\n",
      "running time 572.079789\n",
      "Train_EnvstepsSoFar : 357001\n",
      "Train_AverageReturn : 63.07532238574661\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 572.0797891616821\n",
      "Training Loss : 0.31588253378868103\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 358001\n",
      "mean reward (100 episodes) 61.050457\n",
      "best mean reward 123.596272\n",
      "running time 573.478087\n",
      "Train_EnvstepsSoFar : 358001\n",
      "Train_AverageReturn : 61.05045661697195\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 573.4780869483948\n",
      "Training Loss : 0.18671685457229614\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 359001\n",
      "mean reward (100 episodes) 58.666395\n",
      "best mean reward 123.596272\n",
      "running time 576.541709\n",
      "Train_EnvstepsSoFar : 359001\n",
      "Train_AverageReturn : 58.666394728054314\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 576.5417091846466\n",
      "Training Loss : 0.40616264939308167\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 48.936405\n",
      "best mean reward 123.596272\n",
      "running time 577.417304\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 48.9364053241149\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 577.4173040390015\n",
      "Training Loss : 3.5885202884674072\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 361001\n",
      "mean reward (100 episodes) 37.938360\n",
      "best mean reward 123.596272\n",
      "running time 578.398504\n",
      "Train_EnvstepsSoFar : 361001\n",
      "Train_AverageReturn : 37.93835985688472\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 578.3985040187836\n",
      "Training Loss : 0.132223442196846\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 362001\n",
      "mean reward (100 episodes) 34.455680\n",
      "best mean reward 123.596272\n",
      "running time 579.694762\n",
      "Train_EnvstepsSoFar : 362001\n",
      "Train_AverageReturn : 34.45568012705433\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 579.6947619915009\n",
      "Training Loss : 0.14792947471141815\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 363001\n",
      "mean reward (100 episodes) 33.152882\n",
      "best mean reward 123.596272\n",
      "running time 581.348178\n",
      "Train_EnvstepsSoFar : 363001\n",
      "Train_AverageReturn : 33.152881881706826\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 581.3481779098511\n",
      "Training Loss : 0.39932727813720703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 364001\n",
      "mean reward (100 episodes) 29.500216\n",
      "best mean reward 123.596272\n",
      "running time 583.803574\n",
      "Train_EnvstepsSoFar : 364001\n",
      "Train_AverageReturn : 29.500215763274017\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 583.8035740852356\n",
      "Training Loss : 0.18007627129554749\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 365001\n",
      "mean reward (100 episodes) 26.928648\n",
      "best mean reward 123.596272\n",
      "running time 585.822175\n",
      "Train_EnvstepsSoFar : 365001\n",
      "Train_AverageReturn : 26.928647723200548\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 585.8221750259399\n",
      "Training Loss : 0.10065194219350815\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 366001\n",
      "mean reward (100 episodes) 24.065195\n",
      "best mean reward 123.596272\n",
      "running time 587.372021\n",
      "Train_EnvstepsSoFar : 366001\n",
      "Train_AverageReturn : 24.065195220322895\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 587.3720209598541\n",
      "Training Loss : 0.26595398783683777\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 367001\n",
      "mean reward (100 episodes) 13.027016\n",
      "best mean reward 123.596272\n",
      "running time 588.597925\n",
      "Train_EnvstepsSoFar : 367001\n",
      "Train_AverageReturn : 13.02701611298805\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 588.5979251861572\n",
      "Training Loss : 0.371398389339447\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 368001\n",
      "mean reward (100 episodes) 12.270705\n",
      "best mean reward 123.596272\n",
      "running time 590.559038\n",
      "Train_EnvstepsSoFar : 368001\n",
      "Train_AverageReturn : 12.270705098841734\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 590.5590379238129\n",
      "Training Loss : 0.19480153918266296\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 369001\n",
      "mean reward (100 episodes) 12.785928\n",
      "best mean reward 123.596272\n",
      "running time 593.351093\n",
      "Train_EnvstepsSoFar : 369001\n",
      "Train_AverageReturn : 12.785928438159994\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 593.3510930538177\n",
      "Training Loss : 0.20605115592479706\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 12.420445\n",
      "best mean reward 123.596272\n",
      "running time 595.504318\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 12.420445234741841\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 595.5043179988861\n",
      "Training Loss : 0.15307217836380005\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 371001\n",
      "mean reward (100 episodes) 10.260435\n",
      "best mean reward 123.596272\n",
      "running time 598.437652\n",
      "Train_EnvstepsSoFar : 371001\n",
      "Train_AverageReturn : 10.260435007006704\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 598.4376521110535\n",
      "Training Loss : 0.18992963433265686\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 372001\n",
      "mean reward (100 episodes) 13.427571\n",
      "best mean reward 123.596272\n",
      "running time 600.888557\n",
      "Train_EnvstepsSoFar : 372001\n",
      "Train_AverageReturn : 13.427570781827415\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 600.8885571956635\n",
      "Training Loss : 0.08598609268665314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 373001\n",
      "mean reward (100 episodes) 17.432334\n",
      "best mean reward 123.596272\n",
      "running time 601.939342\n",
      "Train_EnvstepsSoFar : 373001\n",
      "Train_AverageReturn : 17.43233376497086\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 601.9393420219421\n",
      "Training Loss : 0.18084672093391418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 374001\n",
      "mean reward (100 episodes) 22.229661\n",
      "best mean reward 123.596272\n",
      "running time 603.538597\n",
      "Train_EnvstepsSoFar : 374001\n",
      "Train_AverageReturn : 22.229661342906684\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 603.5385971069336\n",
      "Training Loss : 0.11934512853622437\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 375001\n",
      "mean reward (100 episodes) 19.608593\n",
      "best mean reward 123.596272\n",
      "running time 605.802809\n",
      "Train_EnvstepsSoFar : 375001\n",
      "Train_AverageReturn : 19.608593065551137\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 605.8028092384338\n",
      "Training Loss : 0.2440500110387802\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 376001\n",
      "mean reward (100 episodes) 15.647641\n",
      "best mean reward 123.596272\n",
      "running time 608.374160\n",
      "Train_EnvstepsSoFar : 376001\n",
      "Train_AverageReturn : 15.647641087845791\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 608.3741602897644\n",
      "Training Loss : 0.1484004706144333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 377001\n",
      "mean reward (100 episodes) 12.966990\n",
      "best mean reward 123.596272\n",
      "running time 609.142980\n",
      "Train_EnvstepsSoFar : 377001\n",
      "Train_AverageReturn : 12.966990010762009\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 609.1429800987244\n",
      "Training Loss : 0.3335944414138794\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 378001\n",
      "mean reward (100 episodes) 14.446643\n",
      "best mean reward 123.596272\n",
      "running time 610.011264\n",
      "Train_EnvstepsSoFar : 378001\n",
      "Train_AverageReturn : 14.446642533796137\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 610.0112640857697\n",
      "Training Loss : 0.13523633778095245\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 379001\n",
      "mean reward (100 episodes) 17.210772\n",
      "best mean reward 123.596272\n",
      "running time 611.666818\n",
      "Train_EnvstepsSoFar : 379001\n",
      "Train_AverageReturn : 17.210772279119585\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 611.6668181419373\n",
      "Training Loss : 1.1971713304519653\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 17.610808\n",
      "best mean reward 123.596272\n",
      "running time 612.632297\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 17.610808189129056\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 612.632297039032\n",
      "Training Loss : 1.5097469091415405\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 381001\n",
      "mean reward (100 episodes) 10.570015\n",
      "best mean reward 123.596272\n",
      "running time 613.470583\n",
      "Train_EnvstepsSoFar : 381001\n",
      "Train_AverageReturn : 10.570015376839155\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 613.4705832004547\n",
      "Training Loss : 0.12471899390220642\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 382001\n",
      "mean reward (100 episodes) 15.413763\n",
      "best mean reward 123.596272\n",
      "running time 614.772072\n",
      "Train_EnvstepsSoFar : 382001\n",
      "Train_AverageReturn : 15.413762664715069\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 614.7720720767975\n",
      "Training Loss : 6.124704360961914\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 383001\n",
      "mean reward (100 episodes) 13.446182\n",
      "best mean reward 123.596272\n",
      "running time 616.211292\n",
      "Train_EnvstepsSoFar : 383001\n",
      "Train_AverageReturn : 13.446181730187096\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 616.2112920284271\n",
      "Training Loss : 6.49060583114624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 384001\n",
      "mean reward (100 episodes) 11.338538\n",
      "best mean reward 123.596272\n",
      "running time 617.692763\n",
      "Train_EnvstepsSoFar : 384001\n",
      "Train_AverageReturn : 11.33853804195718\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 617.6927630901337\n",
      "Training Loss : 2.1842639446258545\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 385001\n",
      "mean reward (100 episodes) 13.713658\n",
      "best mean reward 123.596272\n",
      "running time 618.702299\n",
      "Train_EnvstepsSoFar : 385001\n",
      "Train_AverageReturn : 13.713658131543113\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 618.702299118042\n",
      "Training Loss : 0.338987797498703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 386001\n",
      "mean reward (100 episodes) 22.359152\n",
      "best mean reward 123.596272\n",
      "running time 619.853034\n",
      "Train_EnvstepsSoFar : 386001\n",
      "Train_AverageReturn : 22.3591516254177\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 619.8530340194702\n",
      "Training Loss : 0.8453428745269775\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 387001\n",
      "mean reward (100 episodes) 31.191021\n",
      "best mean reward 123.596272\n",
      "running time 620.683444\n",
      "Train_EnvstepsSoFar : 387001\n",
      "Train_AverageReturn : 31.1910211404354\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 620.6834440231323\n",
      "Training Loss : 0.410136878490448\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 388001\n",
      "mean reward (100 episodes) 32.315483\n",
      "best mean reward 123.596272\n",
      "running time 623.054159\n",
      "Train_EnvstepsSoFar : 388001\n",
      "Train_AverageReturn : 32.31548259741766\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 623.0541591644287\n",
      "Training Loss : 0.7647867202758789\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 389001\n",
      "mean reward (100 episodes) 37.809038\n",
      "best mean reward 123.596272\n",
      "running time 624.169959\n",
      "Train_EnvstepsSoFar : 389001\n",
      "Train_AverageReturn : 37.809037528774354\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 624.1699590682983\n",
      "Training Loss : 2.6674647331237793\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 44.679028\n",
      "best mean reward 123.596272\n",
      "running time 625.149380\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 44.67902782648239\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 625.1493802070618\n",
      "Training Loss : 0.15856362879276276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 391001\n",
      "mean reward (100 episodes) 44.858069\n",
      "best mean reward 123.596272\n",
      "running time 626.019751\n",
      "Train_EnvstepsSoFar : 391001\n",
      "Train_AverageReturn : 44.858069099276534\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 626.0197510719299\n",
      "Training Loss : 0.11470093578100204\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 392001\n",
      "mean reward (100 episodes) 47.401915\n",
      "best mean reward 123.596272\n",
      "running time 627.461770\n",
      "Train_EnvstepsSoFar : 392001\n",
      "Train_AverageReturn : 47.40191508851905\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 627.4617700576782\n",
      "Training Loss : 0.0913650393486023\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 393001\n",
      "mean reward (100 episodes) 57.266251\n",
      "best mean reward 123.596272\n",
      "running time 628.259025\n",
      "Train_EnvstepsSoFar : 393001\n",
      "Train_AverageReturn : 57.26625120078921\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 628.2590248584747\n",
      "Training Loss : 5.047081470489502\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 394001\n",
      "mean reward (100 episodes) 61.013552\n",
      "best mean reward 123.596272\n",
      "running time 629.657336\n",
      "Train_EnvstepsSoFar : 394001\n",
      "Train_AverageReturn : 61.0135520479505\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 629.6573359966278\n",
      "Training Loss : 0.20227530598640442\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 395001\n",
      "mean reward (100 episodes) 67.956092\n",
      "best mean reward 123.596272\n",
      "running time 630.573574\n",
      "Train_EnvstepsSoFar : 395001\n",
      "Train_AverageReturn : 67.9560916209494\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 630.5735738277435\n",
      "Training Loss : 0.13993020355701447\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 396001\n",
      "mean reward (100 episodes) 68.611647\n",
      "best mean reward 123.596272\n",
      "running time 631.648883\n",
      "Train_EnvstepsSoFar : 396001\n",
      "Train_AverageReturn : 68.6116472527018\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 631.6488831043243\n",
      "Training Loss : 0.14270563423633575\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 397001\n",
      "mean reward (100 episodes) 73.743106\n",
      "best mean reward 123.596272\n",
      "running time 632.485050\n",
      "Train_EnvstepsSoFar : 397001\n",
      "Train_AverageReturn : 73.74310634585346\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 632.485050201416\n",
      "Training Loss : 0.19394777715206146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 398001\n",
      "mean reward (100 episodes) 68.255545\n",
      "best mean reward 123.596272\n",
      "running time 633.394466\n",
      "Train_EnvstepsSoFar : 398001\n",
      "Train_AverageReturn : 68.2555450095672\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 633.3944661617279\n",
      "Training Loss : 0.2467050999403\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 399001\n",
      "mean reward (100 episodes) 76.809368\n",
      "best mean reward 123.596272\n",
      "running time 634.234067\n",
      "Train_EnvstepsSoFar : 399001\n",
      "Train_AverageReturn : 76.80936760180828\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 634.2340669631958\n",
      "Training Loss : 0.2652418315410614\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 77.151342\n",
      "best mean reward 123.596272\n",
      "running time 635.076196\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 77.15134215423113\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 635.0761961936951\n",
      "Training Loss : 0.1352558135986328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 401001\n",
      "mean reward (100 episodes) 84.119881\n",
      "best mean reward 123.596272\n",
      "running time 635.983000\n",
      "Train_EnvstepsSoFar : 401001\n",
      "Train_AverageReturn : 84.11988067883652\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 635.9830000400543\n",
      "Training Loss : 0.19721239805221558\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 402001\n",
      "mean reward (100 episodes) 89.390838\n",
      "best mean reward 123.596272\n",
      "running time 636.923415\n",
      "Train_EnvstepsSoFar : 402001\n",
      "Train_AverageReturn : 89.39083753311628\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 636.923415184021\n",
      "Training Loss : 0.2308560013771057\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 403001\n",
      "mean reward (100 episodes) 94.887896\n",
      "best mean reward 123.596272\n",
      "running time 637.748738\n",
      "Train_EnvstepsSoFar : 403001\n",
      "Train_AverageReturn : 94.88789573396863\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 637.7487380504608\n",
      "Training Loss : 0.3972901701927185\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 404001\n",
      "mean reward (100 episodes) 103.692579\n",
      "best mean reward 123.596272\n",
      "running time 638.723359\n",
      "Train_EnvstepsSoFar : 404001\n",
      "Train_AverageReturn : 103.69257870048595\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 638.7233591079712\n",
      "Training Loss : 4.674533367156982\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 405001\n",
      "mean reward (100 episodes) 109.897193\n",
      "best mean reward 123.596272\n",
      "running time 639.521616\n",
      "Train_EnvstepsSoFar : 405001\n",
      "Train_AverageReturn : 109.89719304384326\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 639.5216162204742\n",
      "Training Loss : 0.2086755633354187\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 406001\n",
      "mean reward (100 episodes) 99.757959\n",
      "best mean reward 123.596272\n",
      "running time 640.325321\n",
      "Train_EnvstepsSoFar : 406001\n",
      "Train_AverageReturn : 99.75795864460652\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 640.3253209590912\n",
      "Training Loss : 0.17251376807689667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 407001\n",
      "mean reward (100 episodes) 95.820284\n",
      "best mean reward 123.596272\n",
      "running time 641.280686\n",
      "Train_EnvstepsSoFar : 407001\n",
      "Train_AverageReturn : 95.82028433766841\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 641.2806861400604\n",
      "Training Loss : 1.12297523021698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 408001\n",
      "mean reward (100 episodes) 93.500554\n",
      "best mean reward 123.596272\n",
      "running time 642.332103\n",
      "Train_EnvstepsSoFar : 408001\n",
      "Train_AverageReturn : 93.50055370118648\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 642.3321030139923\n",
      "Training Loss : 1.2899460792541504\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 409001\n",
      "mean reward (100 episodes) 95.984424\n",
      "best mean reward 123.596272\n",
      "running time 643.243607\n",
      "Train_EnvstepsSoFar : 409001\n",
      "Train_AverageReturn : 95.98442374850654\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 643.24360704422\n",
      "Training Loss : 0.17245474457740784\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 99.201701\n",
      "best mean reward 123.596272\n",
      "running time 644.333824\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 99.2017009958164\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 644.3338241577148\n",
      "Training Loss : 0.9480980038642883\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 411001\n",
      "mean reward (100 episodes) 94.593910\n",
      "best mean reward 123.596272\n",
      "running time 645.194171\n",
      "Train_EnvstepsSoFar : 411001\n",
      "Train_AverageReturn : 94.59390981351986\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 645.1941709518433\n",
      "Training Loss : 0.19505076110363007\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 412001\n",
      "mean reward (100 episodes) 89.943945\n",
      "best mean reward 123.596272\n",
      "running time 646.053990\n",
      "Train_EnvstepsSoFar : 412001\n",
      "Train_AverageReturn : 89.94394543429163\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 646.0539898872375\n",
      "Training Loss : 0.09931106120347977\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 413001\n",
      "mean reward (100 episodes) 89.960693\n",
      "best mean reward 123.596272\n",
      "running time 647.237365\n",
      "Train_EnvstepsSoFar : 413001\n",
      "Train_AverageReturn : 89.96069307669478\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 647.2373650074005\n",
      "Training Loss : 1.1874003410339355\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 414001\n",
      "mean reward (100 episodes) 95.514573\n",
      "best mean reward 123.596272\n",
      "running time 648.158687\n",
      "Train_EnvstepsSoFar : 414001\n",
      "Train_AverageReturn : 95.51457337921987\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 648.1586871147156\n",
      "Training Loss : 1.133180022239685\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 415001\n",
      "mean reward (100 episodes) 88.722939\n",
      "best mean reward 123.596272\n",
      "running time 649.026189\n",
      "Train_EnvstepsSoFar : 415001\n",
      "Train_AverageReturn : 88.72293885238619\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 649.0261890888214\n",
      "Training Loss : 6.333013534545898\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 416001\n",
      "mean reward (100 episodes) 91.327049\n",
      "best mean reward 123.596272\n",
      "running time 651.587705\n",
      "Train_EnvstepsSoFar : 416001\n",
      "Train_AverageReturn : 91.3270492464696\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 651.5877051353455\n",
      "Training Loss : 6.448319435119629\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 417001\n",
      "mean reward (100 episodes) 87.402540\n",
      "best mean reward 123.596272\n",
      "running time 652.416001\n",
      "Train_EnvstepsSoFar : 417001\n",
      "Train_AverageReturn : 87.40253974588165\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 652.4160010814667\n",
      "Training Loss : 0.8322956562042236\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 418001\n",
      "mean reward (100 episodes) 88.430894\n",
      "best mean reward 123.596272\n",
      "running time 653.150853\n",
      "Train_EnvstepsSoFar : 418001\n",
      "Train_AverageReturn : 88.4308939500974\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 653.1508531570435\n",
      "Training Loss : 0.2306702882051468\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 419001\n",
      "mean reward (100 episodes) 83.710990\n",
      "best mean reward 123.596272\n",
      "running time 654.076651\n",
      "Train_EnvstepsSoFar : 419001\n",
      "Train_AverageReturn : 83.71098957097047\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 654.076651096344\n",
      "Training Loss : 0.259268194437027\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 76.698712\n",
      "best mean reward 123.596272\n",
      "running time 654.983371\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 76.69871184414541\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 654.9833710193634\n",
      "Training Loss : 0.2937612533569336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 421001\n",
      "mean reward (100 episodes) 68.803769\n",
      "best mean reward 123.596272\n",
      "running time 656.003434\n",
      "Train_EnvstepsSoFar : 421001\n",
      "Train_AverageReturn : 68.80376894528334\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 656.0034339427948\n",
      "Training Loss : 0.1535460501909256\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 422001\n",
      "mean reward (100 episodes) 72.898751\n",
      "best mean reward 123.596272\n",
      "running time 656.861628\n",
      "Train_EnvstepsSoFar : 422001\n",
      "Train_AverageReturn : 72.8987506311056\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 656.8616280555725\n",
      "Training Loss : 0.21568022668361664\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 423001\n",
      "mean reward (100 episodes) 69.556796\n",
      "best mean reward 123.596272\n",
      "running time 657.779529\n",
      "Train_EnvstepsSoFar : 423001\n",
      "Train_AverageReturn : 69.55679569833991\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 657.779529094696\n",
      "Training Loss : 0.29633715748786926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 424001\n",
      "mean reward (100 episodes) 77.314763\n",
      "best mean reward 123.596272\n",
      "running time 658.938883\n",
      "Train_EnvstepsSoFar : 424001\n",
      "Train_AverageReturn : 77.3147631467544\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 658.9388830661774\n",
      "Training Loss : 0.1806887984275818\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 425001\n",
      "mean reward (100 episodes) 79.220721\n",
      "best mean reward 123.596272\n",
      "running time 659.810892\n",
      "Train_EnvstepsSoFar : 425001\n",
      "Train_AverageReturn : 79.22072078507817\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 659.8108921051025\n",
      "Training Loss : 1.6348356008529663\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 426001\n",
      "mean reward (100 episodes) 77.455570\n",
      "best mean reward 123.596272\n",
      "running time 661.056834\n",
      "Train_EnvstepsSoFar : 426001\n",
      "Train_AverageReturn : 77.45557004428352\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 661.0568339824677\n",
      "Training Loss : 0.2867443561553955\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 427001\n",
      "mean reward (100 episodes) 75.512182\n",
      "best mean reward 123.596272\n",
      "running time 661.933137\n",
      "Train_EnvstepsSoFar : 427001\n",
      "Train_AverageReturn : 75.51218178583873\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 661.9331369400024\n",
      "Training Loss : 0.3284159302711487\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 428001\n",
      "mean reward (100 episodes) 73.026926\n",
      "best mean reward 123.596272\n",
      "running time 662.725966\n",
      "Train_EnvstepsSoFar : 428001\n",
      "Train_AverageReturn : 73.02692601372661\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 662.7259662151337\n",
      "Training Loss : 1.2344920635223389\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 429001\n",
      "mean reward (100 episodes) 75.931428\n",
      "best mean reward 123.596272\n",
      "running time 663.719559\n",
      "Train_EnvstepsSoFar : 429001\n",
      "Train_AverageReturn : 75.93142780784272\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 663.7195589542389\n",
      "Training Loss : 0.21332947909832\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 74.118755\n",
      "best mean reward 123.596272\n",
      "running time 664.497596\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 74.11875501716094\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 664.4975960254669\n",
      "Training Loss : 1.5663081407546997\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 431001\n",
      "mean reward (100 episodes) 79.032092\n",
      "best mean reward 123.596272\n",
      "running time 665.451412\n",
      "Train_EnvstepsSoFar : 431001\n",
      "Train_AverageReturn : 79.03209187430255\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 665.4514119625092\n",
      "Training Loss : 2.2626190185546875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 432001\n",
      "mean reward (100 episodes) 81.952119\n",
      "best mean reward 123.596272\n",
      "running time 666.431622\n",
      "Train_EnvstepsSoFar : 432001\n",
      "Train_AverageReturn : 81.95211875895764\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 666.4316220283508\n",
      "Training Loss : 0.7558554410934448\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 433001\n",
      "mean reward (100 episodes) 78.063728\n",
      "best mean reward 123.596272\n",
      "running time 667.437450\n",
      "Train_EnvstepsSoFar : 433001\n",
      "Train_AverageReturn : 78.06372752780608\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 667.4374499320984\n",
      "Training Loss : 1.9154083728790283\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 434001\n",
      "mean reward (100 episodes) 71.517785\n",
      "best mean reward 123.596272\n",
      "running time 668.265876\n",
      "Train_EnvstepsSoFar : 434001\n",
      "Train_AverageReturn : 71.51778496933989\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 668.2658760547638\n",
      "Training Loss : 0.4140235185623169\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 435001\n",
      "mean reward (100 episodes) 69.349634\n",
      "best mean reward 123.596272\n",
      "running time 669.295376\n",
      "Train_EnvstepsSoFar : 435001\n",
      "Train_AverageReturn : 69.34963398031122\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 669.2953760623932\n",
      "Training Loss : 0.18978868424892426\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 436001\n",
      "mean reward (100 episodes) 75.806563\n",
      "best mean reward 123.596272\n",
      "running time 670.316467\n",
      "Train_EnvstepsSoFar : 436001\n",
      "Train_AverageReturn : 75.80656316200934\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 670.3164670467377\n",
      "Training Loss : 0.2742280662059784\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 437001\n",
      "mean reward (100 episodes) 78.714388\n",
      "best mean reward 123.596272\n",
      "running time 671.092646\n",
      "Train_EnvstepsSoFar : 437001\n",
      "Train_AverageReturn : 78.71438840615306\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 671.0926458835602\n",
      "Training Loss : 2.283435583114624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 438001\n",
      "mean reward (100 episodes) 80.589598\n",
      "best mean reward 123.596272\n",
      "running time 672.116929\n",
      "Train_EnvstepsSoFar : 438001\n",
      "Train_AverageReturn : 80.58959750753692\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 672.1169290542603\n",
      "Training Loss : 0.2101743519306183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 439001\n",
      "mean reward (100 episodes) 82.274977\n",
      "best mean reward 123.596272\n",
      "running time 672.870284\n",
      "Train_EnvstepsSoFar : 439001\n",
      "Train_AverageReturn : 82.27497714369642\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 672.8702840805054\n",
      "Training Loss : 2.092646598815918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 78.400921\n",
      "best mean reward 123.596272\n",
      "running time 673.623844\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 78.40092147025287\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 673.6238441467285\n",
      "Training Loss : 0.32610464096069336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 441001\n",
      "mean reward (100 episodes) 71.559010\n",
      "best mean reward 123.596272\n",
      "running time 674.384965\n",
      "Train_EnvstepsSoFar : 441001\n",
      "Train_AverageReturn : 71.55901043512802\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 674.3849651813507\n",
      "Training Loss : 1.380495309829712\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 442001\n",
      "mean reward (100 episodes) 72.362789\n",
      "best mean reward 123.596272\n",
      "running time 675.203299\n",
      "Train_EnvstepsSoFar : 442001\n",
      "Train_AverageReturn : 72.36278914795241\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 675.2032990455627\n",
      "Training Loss : 0.546567440032959\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 443001\n",
      "mean reward (100 episodes) 72.002102\n",
      "best mean reward 123.596272\n",
      "running time 676.002537\n",
      "Train_EnvstepsSoFar : 443001\n",
      "Train_AverageReturn : 72.00210173147377\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 676.0025370121002\n",
      "Training Loss : 0.19082805514335632\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 444001\n",
      "mean reward (100 episodes) 70.578441\n",
      "best mean reward 123.596272\n",
      "running time 676.796560\n",
      "Train_EnvstepsSoFar : 444001\n",
      "Train_AverageReturn : 70.5784406068152\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 676.7965602874756\n",
      "Training Loss : 0.2320370227098465\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 445001\n",
      "mean reward (100 episodes) 73.953356\n",
      "best mean reward 123.596272\n",
      "running time 677.989549\n",
      "Train_EnvstepsSoFar : 445001\n",
      "Train_AverageReturn : 73.95335595702598\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 677.9895491600037\n",
      "Training Loss : 0.08637780696153641\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 446001\n",
      "mean reward (100 episodes) 74.417361\n",
      "best mean reward 123.596272\n",
      "running time 679.286540\n",
      "Train_EnvstepsSoFar : 446001\n",
      "Train_AverageReturn : 74.41736069966593\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 679.2865400314331\n",
      "Training Loss : 0.5442086458206177\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 447001\n",
      "mean reward (100 episodes) 75.214124\n",
      "best mean reward 123.596272\n",
      "running time 680.629773\n",
      "Train_EnvstepsSoFar : 447001\n",
      "Train_AverageReturn : 75.21412426651801\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 680.6297731399536\n",
      "Training Loss : 2.3373560905456543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 448001\n",
      "mean reward (100 episodes) 70.947545\n",
      "best mean reward 123.596272\n",
      "running time 681.797878\n",
      "Train_EnvstepsSoFar : 448001\n",
      "Train_AverageReturn : 70.94754512318642\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 681.7978780269623\n",
      "Training Loss : 0.18002009391784668\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 449001\n",
      "mean reward (100 episodes) 80.361290\n",
      "best mean reward 123.596272\n",
      "running time 682.604284\n",
      "Train_EnvstepsSoFar : 449001\n",
      "Train_AverageReturn : 80.36129004524767\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 682.6042840480804\n",
      "Training Loss : 0.28834137320518494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 79.831385\n",
      "best mean reward 123.596272\n",
      "running time 684.114120\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 79.8313845406598\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 684.1141200065613\n",
      "Training Loss : 1.0176019668579102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 451001\n",
      "mean reward (100 episodes) 77.157581\n",
      "best mean reward 123.596272\n",
      "running time 685.389354\n",
      "Train_EnvstepsSoFar : 451001\n",
      "Train_AverageReturn : 77.15758116396135\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 685.3893539905548\n",
      "Training Loss : 0.838544487953186\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 452001\n",
      "mean reward (100 episodes) 69.596690\n",
      "best mean reward 123.596272\n",
      "running time 686.441361\n",
      "Train_EnvstepsSoFar : 452001\n",
      "Train_AverageReturn : 69.59668974949153\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 686.44136095047\n",
      "Training Loss : 0.2539471387863159\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 453001\n",
      "mean reward (100 episodes) 76.574204\n",
      "best mean reward 123.596272\n",
      "running time 687.335284\n",
      "Train_EnvstepsSoFar : 453001\n",
      "Train_AverageReturn : 76.57420351774445\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 687.3352839946747\n",
      "Training Loss : 0.2434197962284088\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 454001\n",
      "mean reward (100 episodes) 75.420154\n",
      "best mean reward 123.596272\n",
      "running time 688.211616\n",
      "Train_EnvstepsSoFar : 454001\n",
      "Train_AverageReturn : 75.42015352257498\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 688.2116160392761\n",
      "Training Loss : 0.2713589072227478\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 455001\n",
      "mean reward (100 episodes) 78.050179\n",
      "best mean reward 123.596272\n",
      "running time 689.084619\n",
      "Train_EnvstepsSoFar : 455001\n",
      "Train_AverageReturn : 78.05017894773516\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 689.0846190452576\n",
      "Training Loss : 3.3596715927124023\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 456001\n",
      "mean reward (100 episodes) 80.754314\n",
      "best mean reward 123.596272\n",
      "running time 690.569884\n",
      "Train_EnvstepsSoFar : 456001\n",
      "Train_AverageReturn : 80.75431440287306\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 690.5698840618134\n",
      "Training Loss : 7.4424238204956055\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 457001\n",
      "mean reward (100 episodes) 78.908311\n",
      "best mean reward 123.596272\n",
      "running time 692.453636\n",
      "Train_EnvstepsSoFar : 457001\n",
      "Train_AverageReturn : 78.90831073071497\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 692.4536361694336\n",
      "Training Loss : 0.4621066749095917\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 458001\n",
      "mean reward (100 episodes) 77.868779\n",
      "best mean reward 123.596272\n",
      "running time 693.324458\n",
      "Train_EnvstepsSoFar : 458001\n",
      "Train_AverageReturn : 77.86877947708244\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 693.3244581222534\n",
      "Training Loss : 0.4474748373031616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 459001\n",
      "mean reward (100 episodes) 75.542455\n",
      "best mean reward 123.596272\n",
      "running time 695.120687\n",
      "Train_EnvstepsSoFar : 459001\n",
      "Train_AverageReturn : 75.54245463928712\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 695.1206872463226\n",
      "Training Loss : 0.28507760167121887\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 74.998567\n",
      "best mean reward 123.596272\n",
      "running time 695.923261\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 74.9985666041624\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 695.9232609272003\n",
      "Training Loss : 0.9822582006454468\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 461001\n",
      "mean reward (100 episodes) 74.565810\n",
      "best mean reward 123.596272\n",
      "running time 697.132047\n",
      "Train_EnvstepsSoFar : 461001\n",
      "Train_AverageReturn : 74.56580976519516\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 697.1320471763611\n",
      "Training Loss : 1.29695725440979\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 462001\n",
      "mean reward (100 episodes) 82.765850\n",
      "best mean reward 123.596272\n",
      "running time 698.750183\n",
      "Train_EnvstepsSoFar : 462001\n",
      "Train_AverageReturn : 82.76585027039837\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 698.7501828670502\n",
      "Training Loss : 0.1241263747215271\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 463001\n",
      "mean reward (100 episodes) 81.552382\n",
      "best mean reward 123.596272\n",
      "running time 700.804224\n",
      "Train_EnvstepsSoFar : 463001\n",
      "Train_AverageReturn : 81.55238188810385\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 700.8042240142822\n",
      "Training Loss : 0.21366585791110992\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 464001\n",
      "mean reward (100 episodes) 75.309753\n",
      "best mean reward 123.596272\n",
      "running time 701.866546\n",
      "Train_EnvstepsSoFar : 464001\n",
      "Train_AverageReturn : 75.30975276976214\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 701.8665459156036\n",
      "Training Loss : 0.19616805016994476\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 465001\n",
      "mean reward (100 episodes) 80.436815\n",
      "best mean reward 123.596272\n",
      "running time 703.326083\n",
      "Train_EnvstepsSoFar : 465001\n",
      "Train_AverageReturn : 80.43681535048239\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 703.3260831832886\n",
      "Training Loss : 1.4297783374786377\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 466001\n",
      "mean reward (100 episodes) 77.896081\n",
      "best mean reward 123.596272\n",
      "running time 705.192584\n",
      "Train_EnvstepsSoFar : 466001\n",
      "Train_AverageReturn : 77.8960810272612\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 705.1925840377808\n",
      "Training Loss : 1.1520814895629883\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 467001\n",
      "mean reward (100 episodes) 78.904439\n",
      "best mean reward 123.596272\n",
      "running time 706.879853\n",
      "Train_EnvstepsSoFar : 467001\n",
      "Train_AverageReturn : 78.90443868809255\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 706.8798530101776\n",
      "Training Loss : 0.5915653109550476\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 468001\n",
      "mean reward (100 episodes) 78.216570\n",
      "best mean reward 123.596272\n",
      "running time 708.502632\n",
      "Train_EnvstepsSoFar : 468001\n",
      "Train_AverageReturn : 78.21656988377084\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 708.5026319026947\n",
      "Training Loss : 0.30097219347953796\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 469001\n",
      "mean reward (100 episodes) 75.026488\n",
      "best mean reward 123.596272\n",
      "running time 709.631617\n",
      "Train_EnvstepsSoFar : 469001\n",
      "Train_AverageReturn : 75.02648750023609\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 709.6316170692444\n",
      "Training Loss : 6.254317760467529\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 76.958989\n",
      "best mean reward 123.596272\n",
      "running time 710.850923\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 76.9589889982302\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 710.8509230613708\n",
      "Training Loss : 2.549699544906616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 471001\n",
      "mean reward (100 episodes) 80.885118\n",
      "best mean reward 123.596272\n",
      "running time 712.512229\n",
      "Train_EnvstepsSoFar : 471001\n",
      "Train_AverageReturn : 80.88511786698822\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 712.5122292041779\n",
      "Training Loss : 0.3708735406398773\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 472001\n",
      "mean reward (100 episodes) 68.402650\n",
      "best mean reward 123.596272\n",
      "running time 713.272309\n",
      "Train_EnvstepsSoFar : 472001\n",
      "Train_AverageReturn : 68.40264964127705\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 713.2723090648651\n",
      "Training Loss : 0.19773878157138824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 473001\n",
      "mean reward (100 episodes) 73.258124\n",
      "best mean reward 123.596272\n",
      "running time 714.188056\n",
      "Train_EnvstepsSoFar : 473001\n",
      "Train_AverageReturn : 73.25812385865704\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 714.1880559921265\n",
      "Training Loss : 1.1864726543426514\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 474001\n",
      "mean reward (100 episodes) 72.888132\n",
      "best mean reward 123.596272\n",
      "running time 715.544515\n",
      "Train_EnvstepsSoFar : 474001\n",
      "Train_AverageReturn : 72.88813167181412\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 715.544515132904\n",
      "Training Loss : 0.7889869213104248\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 475001\n",
      "mean reward (100 episodes) 66.969892\n",
      "best mean reward 123.596272\n",
      "running time 717.043344\n",
      "Train_EnvstepsSoFar : 475001\n",
      "Train_AverageReturn : 66.96989186658338\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 717.0433442592621\n",
      "Training Loss : 6.518763542175293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 476001\n",
      "mean reward (100 episodes) 63.324333\n",
      "best mean reward 123.596272\n",
      "running time 718.231245\n",
      "Train_EnvstepsSoFar : 476001\n",
      "Train_AverageReturn : 63.32433252501636\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 718.2312450408936\n",
      "Training Loss : 0.2975592017173767\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 477001\n",
      "mean reward (100 episodes) 70.845254\n",
      "best mean reward 123.596272\n",
      "running time 719.385419\n",
      "Train_EnvstepsSoFar : 477001\n",
      "Train_AverageReturn : 70.84525359621853\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 719.3854191303253\n",
      "Training Loss : 0.6465221047401428\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 478001\n",
      "mean reward (100 episodes) 59.603995\n",
      "best mean reward 123.596272\n",
      "running time 720.187001\n",
      "Train_EnvstepsSoFar : 478001\n",
      "Train_AverageReturn : 59.60399475326777\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 720.1870012283325\n",
      "Training Loss : 0.13579212129116058\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 479001\n",
      "mean reward (100 episodes) 63.660123\n",
      "best mean reward 123.596272\n",
      "running time 721.275723\n",
      "Train_EnvstepsSoFar : 479001\n",
      "Train_AverageReturn : 63.66012314537803\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 721.2757229804993\n",
      "Training Loss : 1.177985429763794\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 68.765015\n",
      "best mean reward 123.596272\n",
      "running time 722.303790\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 68.76501514334984\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 722.3037900924683\n",
      "Training Loss : 0.9982867240905762\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 481001\n",
      "mean reward (100 episodes) 72.215359\n",
      "best mean reward 123.596272\n",
      "running time 723.465506\n",
      "Train_EnvstepsSoFar : 481001\n",
      "Train_AverageReturn : 72.21535881681285\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 723.4655060768127\n",
      "Training Loss : 2.6649580001831055\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 482001\n",
      "mean reward (100 episodes) 66.582870\n",
      "best mean reward 123.596272\n",
      "running time 724.355452\n",
      "Train_EnvstepsSoFar : 482001\n",
      "Train_AverageReturn : 66.58287017509508\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 724.3554520606995\n",
      "Training Loss : 0.34041741490364075\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 483001\n",
      "mean reward (100 episodes) 68.732538\n",
      "best mean reward 123.596272\n",
      "running time 725.797964\n",
      "Train_EnvstepsSoFar : 483001\n",
      "Train_AverageReturn : 68.73253796455005\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 725.7979640960693\n",
      "Training Loss : 0.3155249357223511\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 484001\n",
      "mean reward (100 episodes) 67.976067\n",
      "best mean reward 123.596272\n",
      "running time 727.141780\n",
      "Train_EnvstepsSoFar : 484001\n",
      "Train_AverageReturn : 67.97606690943844\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 727.1417798995972\n",
      "Training Loss : 0.3497954308986664\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 485001\n",
      "mean reward (100 episodes) 75.217775\n",
      "best mean reward 123.596272\n",
      "running time 728.259745\n",
      "Train_EnvstepsSoFar : 485001\n",
      "Train_AverageReturn : 75.21777525311694\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 728.2597451210022\n",
      "Training Loss : 0.590576171875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 486001\n",
      "mean reward (100 episodes) 77.569021\n",
      "best mean reward 123.596272\n",
      "running time 729.048434\n",
      "Train_EnvstepsSoFar : 486001\n",
      "Train_AverageReturn : 77.56902097894957\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 729.0484340190887\n",
      "Training Loss : 0.1956545114517212\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 487001\n",
      "mean reward (100 episodes) 80.447087\n",
      "best mean reward 123.596272\n",
      "running time 730.037598\n",
      "Train_EnvstepsSoFar : 487001\n",
      "Train_AverageReturn : 80.44708725720808\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 730.0375978946686\n",
      "Training Loss : 0.24202606081962585\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 488001\n",
      "mean reward (100 episodes) 90.684368\n",
      "best mean reward 123.596272\n",
      "running time 731.080380\n",
      "Train_EnvstepsSoFar : 488001\n",
      "Train_AverageReturn : 90.68436813965239\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 731.0803799629211\n",
      "Training Loss : 0.21094678342342377\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 489001\n",
      "mean reward (100 episodes) 81.116679\n",
      "best mean reward 123.596272\n",
      "running time 731.816682\n",
      "Train_EnvstepsSoFar : 489001\n",
      "Train_AverageReturn : 81.11667861123716\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 731.816682100296\n",
      "Training Loss : 0.8531103134155273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 86.222831\n",
      "best mean reward 123.596272\n",
      "running time 732.612834\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 86.22283099938171\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 732.6128339767456\n",
      "Training Loss : 0.25516822934150696\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 491001\n",
      "mean reward (100 episodes) 77.052880\n",
      "best mean reward 123.596272\n",
      "running time 733.575458\n",
      "Train_EnvstepsSoFar : 491001\n",
      "Train_AverageReturn : 77.05288014016463\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 733.5754580497742\n",
      "Training Loss : 0.16363705694675446\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 492001\n",
      "mean reward (100 episodes) 81.033208\n",
      "best mean reward 123.596272\n",
      "running time 735.412132\n",
      "Train_EnvstepsSoFar : 492001\n",
      "Train_AverageReturn : 81.0332083619267\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 735.412132024765\n",
      "Training Loss : 1.419625163078308\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 493001\n",
      "mean reward (100 episodes) 80.672371\n",
      "best mean reward 123.596272\n",
      "running time 736.645810\n",
      "Train_EnvstepsSoFar : 493001\n",
      "Train_AverageReturn : 80.67237139412148\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 736.6458098888397\n",
      "Training Loss : 0.18916559219360352\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 494001\n",
      "mean reward (100 episodes) 86.819600\n",
      "best mean reward 123.596272\n",
      "running time 737.781759\n",
      "Train_EnvstepsSoFar : 494001\n",
      "Train_AverageReturn : 86.81960043011661\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 737.7817590236664\n",
      "Training Loss : 1.958222508430481\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 495001\n",
      "mean reward (100 episodes) 90.621581\n",
      "best mean reward 123.596272\n",
      "running time 739.481043\n",
      "Train_EnvstepsSoFar : 495001\n",
      "Train_AverageReturn : 90.62158085535025\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 739.481043100357\n",
      "Training Loss : 1.6734400987625122\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 496001\n",
      "mean reward (100 episodes) 93.760526\n",
      "best mean reward 123.596272\n",
      "running time 740.576219\n",
      "Train_EnvstepsSoFar : 496001\n",
      "Train_AverageReturn : 93.76052618565836\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 740.5762190818787\n",
      "Training Loss : 0.9059174060821533\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 497001\n",
      "mean reward (100 episodes) 95.906797\n",
      "best mean reward 123.596272\n",
      "running time 741.377495\n",
      "Train_EnvstepsSoFar : 497001\n",
      "Train_AverageReturn : 95.90679748600319\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 741.3774950504303\n",
      "Training Loss : 0.16415925323963165\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 498001\n",
      "mean reward (100 episodes) 98.973170\n",
      "best mean reward 123.596272\n",
      "running time 742.222952\n",
      "Train_EnvstepsSoFar : 498001\n",
      "Train_AverageReturn : 98.97316999606191\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 742.2229518890381\n",
      "Training Loss : 0.30046406388282776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 499001\n",
      "mean reward (100 episodes) 95.634933\n",
      "best mean reward 123.596272\n",
      "running time 742.969313\n",
      "Train_EnvstepsSoFar : 499001\n",
      "Train_AverageReturn : 95.6349333674098\n",
      "Train_BestReturn : 123.59627173889061\n",
      "TimeSinceStart : 742.9693131446838\n",
      "Training Loss : 0.25114867091178894\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = False\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/dqn/{}/vanilla_dqn'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running DQN experiment with seed\", seed)\n",
    "    dqn_args['seed'] = seed\n",
    "    dqn_args['logdir'] = 'logs/dqn/{}/vanilla_dqn/seed{}'.format(env_str, seed)\n",
    "    dqntrainer = DQN_Trainer(dqn_args)\n",
    "    dqntrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "Address already in use\n",
       "Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualize vanilla DQN results on Lunar Lander\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/dqn/LunarLander/vanilla_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN\n",
    "One potential issue with learning our Q functions with bootstrapping is _maximization bias_, where the learned Q-values tend to overestimate the actual expected future returns. The main idea is that when there is estimation error in the next state's Q-values, even if the values were correct on average, picking the action with the maximum Q-value would tend to select one where the value is overestimated. This overoptimistic value would then also get propagated via the Bellman backups to other states and actions, and can potentially slow down learning.\n",
    "\n",
    "Double DQN (https://arxiv.org/abs/1509.06461) proposes a simple solution to alleviate this _maximization bias_. Instead of taking the next action that maximizes the target network's Q-value, it selects the action to maximize the _current_ Q function at the next state, and then takes the target network's estimate of that action's value. \n",
    "\n",
    "Implement the double DQN target value in the update method in <code>critics/dqn_critic.py</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test DQN target value with double Q\n",
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = True\n",
    "dqntrainer = DQN_Trainer(dqn_args)\n",
    "dqnagent = dqntrainer.rl_trainer.agent\n",
    "critic = dqnagent.critic\n",
    "\n",
    "ob_dim = critic.ob_dim\n",
    "ac_dim = 6\n",
    "N = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(N, ob_dim))\n",
    "acts = np.random.choice(ac_dim, size=(N,))\n",
    "next_obs = np.random.normal(size=(N, ob_dim))\n",
    "rewards = np.random.normal(size=N)\n",
    "terminals = np.zeros(N)\n",
    "terminals[0] = 1\n",
    "\n",
    "first_weight_before = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n",
    "print(\"Weight before update (first row)\", first_weight_before[0])\n",
    "\n",
    "\n",
    "loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n",
    "expected_loss = 0.93894196\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Initial loss\", loss)\n",
    "print(\"Initial Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "for i in range(4):\n",
    "    loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n",
    "    print(loss)\n",
    "\n",
    "expected_loss = 0.7871182\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "\n",
    "first_weight_after = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n",
    "print(\"Weight after update (first row)\", first_weight_after.shape)\n",
    "# Test DQN gradient\n",
    "print(first_weight_after[0])\n",
    "weight_change_partial = first_weight_after[0] - first_weight_before[0]\n",
    "print(weight_change_partial)\n",
    "expected_weight_change = np.array([-0.0049137, -0.00500057, -0.00499138, -0.00491226, -0.00490116,  0.00489506,\n",
    " -0.00284088, -0.00171939,  0.00485736])\n",
    "\n",
    "\n",
    "updated_weight_error = rel_error(weight_change_partial, expected_weight_change)\n",
    "print(\"Weight Update Error\", updated_weight_error, \"should be on the order of 1e-6 or lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now also run some experiments on LunarLander with Double DQN. You may be able to see that double DQN performs slightly better and more stably, but as there is very high variance, dont' worry if you do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with double DQN\n",
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = True\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/dqn/{}/double_dqn'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running DQN experiment with seed\", seed)\n",
    "    dqn_args['seed'] = seed\n",
    "    dqn_args['logdir'] = 'logs/dqn/{}/double_dqn/seed{}'.format(env_str, seed)\n",
    "    dqntrainer = DQN_Trainer(dqn_args)\n",
    "    dqntrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize all DQN results on Lunar Lander\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/dqn/LunarLander/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
