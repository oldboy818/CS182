{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning with Neural Network Policies\n",
    "In this notebook, you will implement the supervised losses for behavior cloning and use it to train policies for locomotion tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title imports\n",
    "# As usual, a bit of setup\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import deeprl.infrastructure.pytorch_util as ptu\n",
    "from deeprl.infrastructure.rl_trainer import RL_Trainer\n",
    "from deeprl.infrastructure.trainers import BC_Trainer\n",
    "from deeprl.agents.bc_agent import BCAgent\n",
    "from deeprl.policies.loaded_gaussian_policy import LoadedGaussianPolicy\n",
    "from deeprl.policies.MLP_policy import MLPPolicySL\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def remove_folder(path):\n",
    "    # check if folder exists\n",
    "    if os.path.exists(path): \n",
    "        print(\"Clearing old results at {}\".format(path))\n",
    "        # remove if exists\n",
    "        shutil.rmtree(path)\n",
    "    else:\n",
    "        print(\"Folder {} does not exist yet. No old results to delete\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_base_args_dict = dict(\n",
    "    expert_policy_file = 'deeprl/policies/experts/Hopper.pkl', #@param\n",
    "    expert_data = 'deeprl/expert_data/expert_data_Hopper-v2.pkl', #@param\n",
    "    env_name = 'Hopper-v2', #@param ['Ant-v2', 'Humanoid-v2', 'Walker2d-v2', 'HalfCheetah-v2', 'Hopper-v2']\n",
    "    exp_name = 'test_bc', #@param\n",
    "    do_dagger = True, #@param {type: \"boolean\"}\n",
    "    ep_len = 1000, #@param {type: \"integer\"}\n",
    "    save_params = False, #@param {type: \"boolean\"}\n",
    "\n",
    "    # Training\n",
    "    num_agent_train_steps_per_iter = 1000, #@param {type: \"integer\"})\n",
    "    n_iter = 1, #@param {type: \"integer\"})\n",
    "\n",
    "    # batches & buffers\n",
    "    batch_size = 10000, #@param {type: \"integer\"})\n",
    "    eval_batch_size = 1000, #@param {type: \"integer\"}\n",
    "    train_batch_size = 100, #@param {type: \"integer\"}\n",
    "    max_replay_buffer_size = 1000000, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown network\n",
    "    n_layers = 2, #@param {type: \"integer\"}\n",
    "    size = 64, #@param {type: \"integer\"}\n",
    "    learning_rate = 5e-3, #@param {type: \"number\"}\n",
    "\n",
    "    #@markdown logging\n",
    "    video_log_freq = -1, #@param {type: \"integer\"}\n",
    "    scalar_log_freq = 1, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown gpu & run-time settings\n",
    "    no_gpu = False, #@param {type: \"boolean\"}\n",
    "    which_gpu = 0, #@param {type: \"integer\"}\n",
    "    seed = 2, #@param {type: \"integer\"}\n",
    "    logdir = 'test',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure\n",
    "**Policies**: We have provided implementations of simple neural network policies for your convenience. For discrete environments, the neural network takes in the current state and outputs the logits of the policy's action distribution at this state. The policy then outputs a categorical distribution using those logits. In environments with continuous action spaces, the network will output the mean of a diagonal Gaussian distribution, as well as having a separate single parameter for the log standard deviations of the Gaussian. \n",
    "\n",
    "Calling forward on the policy will output a torch distribution object, so look at the documentation at https://pytorch.org/docs/stable/distributions.html.\n",
    "Look at <code>policies/MLP_policy</code> to make sure you understand the implementation.\n",
    "\n",
    "**RL Training Loop**: The reinforcement learning training loop, which alternates between gathering samples from the environment and updating the policy (and other learned functions) can be found in <code>infrastructure/rl_trainer.py</code>. While you won't need to understand this for the basic behavior cloning part (as you only use a fixed set of expert data), you should read through and understand the run_training_loop function before starting the Dagger implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Behavior Cloning\n",
    "The first part of the assignment will be a familiar exercise in supervised learning. Given a dataset of expert trajectories, we will simply train our policy to imitate the expert via maximum likelihood. Fill out the update method in the MLPPolicySL class in <code>policies/MLP_policy.py</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight before update [[-0.00432253  0.30971587 -0.47518533]\n",
      " [-0.42489457 -0.22236899  0.15482074]]\n",
      "2.6284192\n",
      "Loss Error 3.059688933264463e-08 should be on the order of 1e-6 or lower\n",
      "Weight after update [[ 0.03953297 -0.15170133 -1.5365069 ]\n",
      " [-0.21503037 -1.4284427  -0.8478559 ]]\n",
      "Change in weights [[ 0.0438555 -0.4614172 -1.0613215]\n",
      " [ 0.2098642 -1.2060738 -1.0026767]]\n",
      "Weight Update Error 4.925437266268277e-07 should be on the order of 1e-6 or lower\n"
     ]
    }
   ],
   "source": [
    "### Basic test for correctness of loss and gradients\n",
    "torch.manual_seed(0)\n",
    "ac_dim = 2\n",
    "ob_dim = 3\n",
    "batch_size = 5\n",
    "\n",
    "policy = MLPPolicySL(\n",
    "            ac_dim=ac_dim,\n",
    "            ob_dim=ob_dim,\n",
    "            n_layers=1,\n",
    "            size=2,\n",
    "            learning_rate=0.25)\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(batch_size, ob_dim))\n",
    "acts = np.random.normal(size=(batch_size, ac_dim))\n",
    "\n",
    "first_weight_before = np.array(ptu.to_numpy(next(policy.mean_net.parameters())))\n",
    "print(\"Weight before update\", first_weight_before)\n",
    "\n",
    "for i in range(5):\n",
    "    loss = policy.update(obs, acts)['Training Loss']\n",
    "\n",
    "print(loss)\n",
    "expected_loss = 2.628419\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "first_weight_after = ptu.to_numpy(next(policy.mean_net.parameters()))\n",
    "print('Weight after update', first_weight_after)\n",
    "\n",
    "weight_change = first_weight_after - first_weight_before\n",
    "print(\"Change in weights\", weight_change)\n",
    "\n",
    "expected_change = np.array([[ 0.04385546, -0.4614172,  -1.0613215 ],\n",
    "                            [ 0.20986436, -1.2060736,  -1.0026767 ]])\n",
    "updated_weight_error = rel_error(weight_change, expected_change)\n",
    "print(\"Weight Update Error\", updated_weight_error, \"should be on the order of 1e-6 or lower\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having implemented our behavior cloning loss, we can now start training some policies to imitate the expert policies provided. \n",
    "\n",
    "Run the following cell to train policies with simple behavior cloning on the HalfCheetah environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing old results at logs/behavior_cloning/HalfCheetah\n",
      "Running behavior cloning experiment with seed 0\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/HalfCheetah/seed0\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "HalfCheetah-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment HalfCheetah-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/Cython/Distutils/old_build_ext.py:14: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
      "  from distutils.dep_util import newer, newer_group\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expert policy from... deeprl/policies/experts/HalfCheetah.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3882.9365234375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3882.9365234375\n",
      "Eval_MinReturn : 3882.9365234375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4205.7783203125\n",
      "Train_StdReturn : 83.038818359375\n",
      "Train_MaxReturn : 4288.81689453125\n",
      "Train_MinReturn : 4122.7392578125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.0234589576721191\n",
      "Training Loss : -7.84713077545166\n",
      "Initial_DataCollection_AverageReturn : 4205.7783203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running behavior cloning experiment with seed 1\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/HalfCheetah/seed1\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "HalfCheetah-v2\n",
      "Loading expert policy from... deeprl/policies/experts/HalfCheetah.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment HalfCheetah-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3681.434814453125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3681.434814453125\n",
      "Eval_MinReturn : 3681.434814453125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4205.7783203125\n",
      "Train_StdReturn : 83.038818359375\n",
      "Train_MaxReturn : 4288.81689453125\n",
      "Train_MinReturn : 4122.7392578125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 0.9630799293518066\n",
      "Training Loss : -8.344711303710938\n",
      "Initial_DataCollection_AverageReturn : 4205.7783203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running behavior cloning experiment with seed 2\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/HalfCheetah/seed2\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "HalfCheetah-v2\n",
      "Loading expert policy from... deeprl/policies/experts/HalfCheetah.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3870.673583984375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3870.673583984375\n",
      "Eval_MinReturn : 3870.673583984375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4205.7783203125\n",
      "Train_StdReturn : 83.038818359375\n",
      "Train_MaxReturn : 4288.81689453125\n",
      "Train_MinReturn : 4122.7392578125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 0.9854416847229004\n",
      "Training Loss : -9.085315704345703\n",
      "Initial_DataCollection_AverageReturn : 4205.7783203125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bc_args = dict(bc_base_args_dict)\n",
    "\n",
    "env_str = 'HalfCheetah'\n",
    "bc_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "bc_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "bc_args['env_name'] = '{}-v2'.format(env_str)\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/behavior_cloning/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running behavior cloning experiment with seed\", seed)\n",
    "    bc_args['seed'] = seed\n",
    "    bc_args['logdir'] = 'logs/behavior_cloning/{}/seed{}'.format(env_str, seed)\n",
    "    bctrainer = BC_Trainer(bc_args)\n",
    "    bctrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your results using Tensorboard. You should see that on HalfCheetah, the returns of your learned policies (Eval_AverageReturn) are fairly similar (thought a bit lower) to that of the expert (Initial_DataCollection_Average_Return)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "Address already in use\n",
       "Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualize behavior cloning results on HalfCheetah\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/behavior_cloning/HalfCheetah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following cell to train policies with simple behavior cloning on Hopper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing old results at logs/behavior_cloning/Hopper\n",
      "Running behavior cloning experiment on Hopper with seed 0\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/Hopper/seed0\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 817.8258056640625\n",
      "Eval_StdReturn : 230.59039306640625\n",
      "Eval_MaxReturn : 1125.77392578125\n",
      "Eval_MinReturn : 477.5009765625\n",
      "Eval_AverageEpLen : 255.5\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 0.9768722057342529\n",
      "Training Loss : -3.9651520252227783\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running behavior cloning experiment on Hopper with seed 1\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/Hopper/seed1\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1243.0560302734375\n",
      "Eval_StdReturn : 243.83505249023438\n",
      "Eval_MaxReturn : 1574.0716552734375\n",
      "Eval_MinReturn : 993.8536376953125\n",
      "Eval_AverageEpLen : 362.0\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 0.9917051792144775\n",
      "Training Loss : -3.557894706726074\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running behavior cloning experiment on Hopper with seed 2\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/Hopper/seed2\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 960.552734375\n",
      "Eval_StdReturn : 115.48811340332031\n",
      "Eval_MaxReturn : 1159.6593017578125\n",
      "Eval_MinReturn : 884.0731811523438\n",
      "Eval_AverageEpLen : 281.75\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 0.9442811012268066\n",
      "Training Loss : -3.5970752239227295\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bc_args = dict(bc_base_args_dict)\n",
    "\n",
    "env_str = 'Hopper'\n",
    "bc_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "bc_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "bc_args['env_name'] = '{}-v2'.format(env_str)\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/behavior_cloning/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running behavior cloning experiment on Hopper with seed\", seed)\n",
    "    bc_args['seed'] = seed\n",
    "    bc_args['logdir'] = 'logs/behavior_cloning/{}/seed{}'.format(env_str, seed)\n",
    "    bctrainer = BC_Trainer(bc_args)\n",
    "    bctrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your results using Tensorboard. You should see that on Hopper, the returns of your learned policies (Eval_AverageReturn) are substantially lower than that of the expert (Initial_DataCollection_Average_Return), due to the distribution shift issues that arise when doing naive behavior cloning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "Address already in use\n",
       "Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualize behavior cloning results on Hopper\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/behavior_cloning/Hopper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Aggregation\n",
    "As discussed in lecture, behavior cloning can suffer from distribution shift, as a small mismatch between the learned and expert policy can take the learned policy to new states that were unseen during training, on which the learned policy hasn't been trained. In Dagger, we will address this issue iteratively, where we use our expert policy to provide labels for the new states we encounter with our learned policy, and then retrain our policy on these newly labeled states.\n",
    "\n",
    "Implement the <code>do_relabel_with_expert</code> function in <code>infrastructure/rl_trainer.py</code>. The errors in the expert actions should be on the order of 1e-6 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  test\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "Path 0 expert action error 5.44257278081802e-07\n",
      "Path 1 expert action error 1.1839725245207643e-07\n",
      "Path 2 expert action error 2.902328426302174e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/jangdong-eon/miniforge3/envs/hw4/lib/python3.8/site-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "### Test do relabel function\n",
    "bc_args = dict(bc_base_args_dict)\n",
    "\n",
    "env_str = 'Hopper'\n",
    "bc_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "bc_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "bc_args['env_name'] = '{}-v2'.format(env_str)\n",
    "bctrainer = BC_Trainer(bc_args)\n",
    "\n",
    "np.random.seed(0)\n",
    "T = 2\n",
    "ob_dim = 11\n",
    "ac_dim = 3\n",
    "\n",
    "paths = []\n",
    "for i in range(3):\n",
    "    obs = np.random.normal(size=(T, ob_dim))\n",
    "    acs = np.random.normal(size=(T, ac_dim))\n",
    "    paths.append(dict(observation=obs,\n",
    "                      action=acs))\n",
    "    \n",
    "rl_trainer = bctrainer.rl_trainer\n",
    "relabeled_paths = rl_trainer.do_relabel_with_expert(bctrainer.loaded_expert_policy, paths)\n",
    "\n",
    "expert_actions = np.array([[[-1.7814021, -0.11137983,  1.763353  ],\n",
    "                            [-2.589222,   -5.463195,    2.4301376 ]],\n",
    "                           [[-2.8287444, -5.298558,   3.0320463],\n",
    "                            [ 3.9611065,  2.626403,  -2.8639293]],\n",
    "                           [[-0.3055225,  -0.9865407,   0.80830705],\n",
    "                            [ 2.8788857,   3.5550566,  -0.92875874]]])\n",
    "\n",
    "for i, (path, relabeled_path) in enumerate(zip(paths, relabeled_paths)):\n",
    "    assert np.all(path['observation'] == relabeled_path['observation'])\n",
    "    print(\"Path {} expert action error\".format(i), rel_error(expert_actions[i], relabeled_path['action']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run Dagger on the Hopper env again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagger_args = dict(bc_base_args_dict)\n",
    "\n",
    "dagger_args['do_dagger'] = True\n",
    "dagger_args['n_iter'] = 10\n",
    "\n",
    "env_str = 'Hopper'\n",
    "dagger_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "dagger_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "dagger_args['env_name'] = '{}-v2'.format(env_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing old results at logs/dagger/Hopper\n",
      "Running Dagger experiment with seed 0\n",
      "########################\n",
      "logging outputs to  logs/dagger/Hopper/seed0\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 817.8258056640625\n",
      "Eval_StdReturn : 230.59039306640625\n",
      "Eval_MaxReturn : 1125.77392578125\n",
      "Eval_MinReturn : 477.5009765625\n",
      "Eval_AverageEpLen : 255.5\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.1065762042999268\n",
      "Training Loss : -3.9651520252227783\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10192 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2887.10888671875\n",
      "Eval_StdReturn : 783.0360107421875\n",
      "Eval_MaxReturn : 3670.14501953125\n",
      "Eval_MinReturn : 2104.072998046875\n",
      "Eval_AverageEpLen : 785.0\n",
      "Train_AverageReturn : 923.6061401367188\n",
      "Train_StdReturn : 257.99755859375\n",
      "Train_MaxReturn : 1444.689697265625\n",
      "Train_MinReturn : 421.38018798828125\n",
      "Train_AverageEpLen : 283.1111111111111\n",
      "Train_EnvstepsSoFar : 10192\n",
      "TimeSinceStart : 3.9765331745147705\n",
      "Training Loss : -2.0230913162231445\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10612 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3744.00927734375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3744.00927734375\n",
      "Eval_MinReturn : 3744.00927734375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 2171.603759765625\n",
      "Train_StdReturn : 811.900634765625\n",
      "Train_MaxReturn : 3726.9267578125\n",
      "Train_MinReturn : 987.6463012695312\n",
      "Train_AverageEpLen : 589.5555555555555\n",
      "Train_EnvstepsSoFar : 20804\n",
      "TimeSinceStart : 6.91974401473999\n",
      "Training Loss : -2.483067274093628\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10674 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3673.7119140625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3673.7119140625\n",
      "Eval_MinReturn : 3673.7119140625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 2477.743896484375\n",
      "Train_StdReturn : 765.3603515625\n",
      "Train_MaxReturn : 3769.0166015625\n",
      "Train_MinReturn : 1281.6641845703125\n",
      "Train_AverageEpLen : 667.125\n",
      "Train_EnvstepsSoFar : 31478\n",
      "TimeSinceStart : 10.85653829574585\n",
      "Training Loss : -2.260918378829956\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10000 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3682.181640625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3682.181640625\n",
      "Eval_MinReturn : 3682.181640625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3668.550048828125\n",
      "Train_StdReturn : 8.935335159301758\n",
      "Train_MaxReturn : 3684.41943359375\n",
      "Train_MinReturn : 3658.267578125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 41478\n",
      "TimeSinceStart : 13.939921379089355\n",
      "Training Loss : -2.7475550174713135\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10000 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3731.783203125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3731.783203125\n",
      "Eval_MinReturn : 3731.783203125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3697.151123046875\n",
      "Train_StdReturn : 11.87087631225586\n",
      "Train_MaxReturn : 3712.05419921875\n",
      "Train_MinReturn : 3675.150390625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 51478\n",
      "TimeSinceStart : 17.197091102600098\n",
      "Training Loss : -3.6367745399475098\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10000 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3732.9208984375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3732.9208984375\n",
      "Eval_MinReturn : 3732.9208984375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3726.29345703125\n",
      "Train_StdReturn : 5.017395973205566\n",
      "Train_MaxReturn : 3735.86669921875\n",
      "Train_MinReturn : 3717.41357421875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 61478\n",
      "TimeSinceStart : 20.677281141281128\n",
      "Training Loss : -2.9157352447509766\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10373 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3718.177001953125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3718.177001953125\n",
      "Eval_MinReturn : 3718.177001953125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3521.544677734375\n",
      "Train_StdReturn : 684.6495361328125\n",
      "Train_MaxReturn : 3745.2783203125\n",
      "Train_MinReturn : 1356.540771484375\n",
      "Train_AverageEpLen : 943.0\n",
      "Train_EnvstepsSoFar : 71851\n",
      "TimeSinceStart : 24.38618803024292\n",
      "Training Loss : -3.4016385078430176\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10000 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3711.91748046875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3711.91748046875\n",
      "Eval_MinReturn : 3711.91748046875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3711.036376953125\n",
      "Train_StdReturn : 3.922776699066162\n",
      "Train_MaxReturn : 3716.51806640625\n",
      "Train_MinReturn : 3703.2529296875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 81851\n",
      "TimeSinceStart : 28.112696170806885\n",
      "Training Loss : -4.170085430145264\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10000 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3713.00634765625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3713.00634765625\n",
      "Eval_MinReturn : 3713.00634765625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3716.23779296875\n",
      "Train_StdReturn : 3.8975372314453125\n",
      "Train_MaxReturn : 3721.8193359375\n",
      "Train_MinReturn : 3705.99365234375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 91851\n",
      "TimeSinceStart : 32.01294708251953\n",
      "Training Loss : -4.524947643280029\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running Dagger experiment with seed 1\n",
      "########################\n",
      "logging outputs to  logs/dagger/Hopper/seed1\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1243.0560302734375\n",
      "Eval_StdReturn : 243.83505249023438\n",
      "Eval_MaxReturn : 1574.0716552734375\n",
      "Eval_MinReturn : 993.8536376953125\n",
      "Eval_AverageEpLen : 362.0\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 0.9600539207458496\n",
      "Training Loss : -3.557894706726074\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10068 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1193.807373046875\n",
      "Eval_StdReturn : 285.7193298339844\n",
      "Eval_MaxReturn : 1501.6976318359375\n",
      "Eval_MinReturn : 849.52197265625\n",
      "Eval_AverageEpLen : 337.25\n",
      "Train_AverageReturn : 901.1127319335938\n",
      "Train_StdReturn : 143.27853393554688\n",
      "Train_MaxReturn : 1171.40478515625\n",
      "Train_MinReturn : 399.8245849609375\n",
      "Train_AverageEpLen : 272.1081081081081\n",
      "Train_EnvstepsSoFar : 10068\n",
      "TimeSinceStart : 3.990241050720215\n",
      "Training Loss : -2.6564993858337402\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10194 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2261.644287109375\n",
      "Eval_StdReturn : 576.8180541992188\n",
      "Eval_MaxReturn : 2838.46240234375\n",
      "Eval_MinReturn : 1684.8262939453125\n",
      "Eval_AverageEpLen : 606.5\n",
      "Train_AverageReturn : 1152.2647705078125\n",
      "Train_StdReturn : 228.5313262939453\n",
      "Train_MaxReturn : 1687.6533203125\n",
      "Train_MinReturn : 869.2161254882812\n",
      "Train_AverageEpLen : 328.83870967741933\n",
      "Train_EnvstepsSoFar : 20262\n",
      "TimeSinceStart : 7.094419240951538\n",
      "Training Loss : -2.3475241661071777\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10194 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3620.02197265625\n",
      "Eval_StdReturn : 92.36865234375\n",
      "Eval_MaxReturn : 3712.390625\n",
      "Eval_MinReturn : 3527.6533203125\n",
      "Eval_AverageEpLen : 966.0\n",
      "Train_AverageReturn : 1709.4730224609375\n",
      "Train_StdReturn : 562.64892578125\n",
      "Train_MaxReturn : 3245.060791015625\n",
      "Train_MinReturn : 1247.6727294921875\n",
      "Train_AverageEpLen : 463.3636363636364\n",
      "Train_EnvstepsSoFar : 30456\n",
      "TimeSinceStart : 10.336603164672852\n",
      "Training Loss : -3.006166458129883\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10616 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2497.3154296875\n",
      "Eval_StdReturn : 1238.3818359375\n",
      "Eval_MaxReturn : 3735.697265625\n",
      "Eval_MinReturn : 1258.9337158203125\n",
      "Eval_AverageEpLen : 677.0\n",
      "Train_AverageReturn : 3547.974853515625\n",
      "Train_StdReturn : 392.3143615722656\n",
      "Train_MaxReturn : 3708.90234375\n",
      "Train_MinReturn : 2309.37451171875\n",
      "Train_AverageEpLen : 965.0909090909091\n",
      "Train_EnvstepsSoFar : 41072\n",
      "TimeSinceStart : 13.79562497138977\n",
      "Training Loss : -3.0944268703460693\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10183 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3723.9033203125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3723.9033203125\n",
      "Eval_MinReturn : 3723.9033203125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 2205.4638671875\n",
      "Train_StdReturn : 1016.2681274414062\n",
      "Train_MaxReturn : 3731.216796875\n",
      "Train_MinReturn : 1237.0501708984375\n",
      "Train_AverageEpLen : 599.0\n",
      "Train_EnvstepsSoFar : 51255\n",
      "TimeSinceStart : 17.26396608352661\n",
      "Training Loss : -3.4759111404418945\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10479 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3719.7509765625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3719.7509765625\n",
      "Eval_MinReturn : 3719.7509765625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3249.375244140625\n",
      "Train_StdReturn : 769.6499633789062\n",
      "Train_MaxReturn : 3734.59765625\n",
      "Train_MinReturn : 1539.248291015625\n",
      "Train_AverageEpLen : 873.25\n",
      "Train_EnvstepsSoFar : 61734\n",
      "TimeSinceStart : 20.825922966003418\n",
      "Training Loss : -3.2359273433685303\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10000 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3723.32666015625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3723.32666015625\n",
      "Eval_MinReturn : 3723.32666015625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3714.01171875\n",
      "Train_StdReturn : 5.622567176818848\n",
      "Train_MaxReturn : 3722.93359375\n",
      "Train_MinReturn : 3706.26708984375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 71734\n",
      "TimeSinceStart : 24.311546087265015\n",
      "Training Loss : -3.68974232673645\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10488 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3724.36767578125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3724.36767578125\n",
      "Eval_MinReturn : 3724.36767578125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3547.68994140625\n",
      "Train_StdReturn : 544.4270629882812\n",
      "Train_MaxReturn : 3730.658447265625\n",
      "Train_MinReturn : 1826.173095703125\n",
      "Train_AverageEpLen : 953.4545454545455\n",
      "Train_EnvstepsSoFar : 82222\n",
      "TimeSinceStart : 28.75383710861206\n",
      "Training Loss : -4.139400005340576\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10000 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3727.856689453125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3727.856689453125\n",
      "Eval_MinReturn : 3727.856689453125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3718.39501953125\n",
      "Train_StdReturn : 4.180521011352539\n",
      "Train_MaxReturn : 3727.525634765625\n",
      "Train_MinReturn : 3714.24365234375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 92222\n",
      "TimeSinceStart : 32.64455509185791\n",
      "Training Loss : -4.0828857421875\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running Dagger experiment with seed 2\n",
      "########################\n",
      "logging outputs to  logs/dagger/Hopper/seed2\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 960.552734375\n",
      "Eval_StdReturn : 115.48811340332031\n",
      "Eval_MaxReturn : 1159.6593017578125\n",
      "Eval_MinReturn : 884.0731811523438\n",
      "Eval_AverageEpLen : 281.75\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.0486688613891602\n",
      "Training Loss : -3.5970752239227295\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10081 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 850.5194091796875\n",
      "Eval_StdReturn : 49.58414840698242\n",
      "Eval_MaxReturn : 944.8753662109375\n",
      "Eval_MinReturn : 804.1677856445312\n",
      "Eval_AverageEpLen : 254.0\n",
      "Train_AverageReturn : 1079.2138671875\n",
      "Train_StdReturn : 169.7096710205078\n",
      "Train_MaxReturn : 1508.4647216796875\n",
      "Train_MinReturn : 787.436279296875\n",
      "Train_AverageEpLen : 315.03125\n",
      "Train_EnvstepsSoFar : 10081\n",
      "TimeSinceStart : 3.868440866470337\n",
      "Training Loss : -2.420377731323242\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10178 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1244.243408203125\n",
      "Eval_StdReturn : 232.63699340820312\n",
      "Eval_MaxReturn : 1638.2467041015625\n",
      "Eval_MinReturn : 1034.521484375\n",
      "Eval_AverageEpLen : 347.0\n",
      "Train_AverageReturn : 884.2401733398438\n",
      "Train_StdReturn : 114.38370513916016\n",
      "Train_MaxReturn : 1130.4951171875\n",
      "Train_MinReturn : 701.3922119140625\n",
      "Train_AverageEpLen : 260.97435897435895\n",
      "Train_EnvstepsSoFar : 20259\n",
      "TimeSinceStart : 6.7989466190338135\n",
      "Training Loss : -2.4741146564483643\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10061 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3334.25048828125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3334.25048828125\n",
      "Eval_MinReturn : 3334.25048828125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1112.4915771484375\n",
      "Train_StdReturn : 129.50833129882812\n",
      "Train_MaxReturn : 1667.4046630859375\n",
      "Train_MinReturn : 956.0655517578125\n",
      "Train_AverageEpLen : 314.40625\n",
      "Train_EnvstepsSoFar : 30320\n",
      "TimeSinceStart : 9.728051900863647\n",
      "Training Loss : -1.870000958442688\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10000 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3698.1640625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3698.1640625\n",
      "Eval_MinReturn : 3698.1640625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3450.27490234375\n",
      "Train_StdReturn : 139.5648651123047\n",
      "Train_MaxReturn : 3688.96435546875\n",
      "Train_MinReturn : 3223.8662109375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 40320\n",
      "TimeSinceStart : 13.2306387424469\n",
      "Training Loss : -2.1240525245666504\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10000 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3697.0068359375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3697.0068359375\n",
      "Eval_MinReturn : 3697.0068359375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3682.489013671875\n",
      "Train_StdReturn : 12.819973945617676\n",
      "Train_MaxReturn : 3701.431640625\n",
      "Train_MinReturn : 3658.767822265625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 50320\n",
      "TimeSinceStart : 16.763389825820923\n",
      "Training Loss : -3.4148168563842773\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10817 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3563.144775390625\n",
      "Eval_StdReturn : 162.435791015625\n",
      "Eval_MaxReturn : 3725.58056640625\n",
      "Eval_MinReturn : 3400.708984375\n",
      "Eval_AverageEpLen : 951.0\n",
      "Train_AverageReturn : 3652.6328125\n",
      "Train_StdReturn : 178.28041076660156\n",
      "Train_MaxReturn : 3723.9326171875\n",
      "Train_MinReturn : 3089.3740234375\n",
      "Train_AverageEpLen : 983.3636363636364\n",
      "Train_EnvstepsSoFar : 61137\n",
      "TimeSinceStart : 20.974769830703735\n",
      "Training Loss : -3.077000617980957\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10695 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3718.566162109375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3718.566162109375\n",
      "Eval_MinReturn : 3718.566162109375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3329.222900390625\n",
      "Train_StdReturn : 791.3161010742188\n",
      "Train_MaxReturn : 3745.34130859375\n",
      "Train_MinReturn : 1501.1839599609375\n",
      "Train_AverageEpLen : 891.25\n",
      "Train_EnvstepsSoFar : 71832\n",
      "TimeSinceStart : 24.691675901412964\n",
      "Training Loss : -3.2575385570526123\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10000 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3720.63330078125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3720.63330078125\n",
      "Eval_MinReturn : 3720.63330078125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3711.491455078125\n",
      "Train_StdReturn : 5.541337966918945\n",
      "Train_MaxReturn : 3718.236083984375\n",
      "Train_MinReturn : 3698.39892578125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 81832\n",
      "TimeSinceStart : 28.463523864746094\n",
      "Training Loss : -2.9312920570373535\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10000 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3701.750244140625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3701.750244140625\n",
      "Eval_MinReturn : 3701.750244140625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3720.53955078125\n",
      "Train_StdReturn : 3.204453706741333\n",
      "Train_MaxReturn : 3727.68896484375\n",
      "Train_MinReturn : 3715.92822265625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 91832\n",
      "TimeSinceStart : 32.89722490310669\n",
      "Training Loss : -4.447797775268555\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete all previous logs\n",
    "remove_folder('logs/dagger/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running Dagger experiment with seed\", seed)\n",
    "    dagger_args['seed'] = seed\n",
    "    dagger_args['logdir'] = 'logs/dagger/{}/seed{}'.format(env_str, seed)\n",
    "    bctrainer = BC_Trainer(dagger_args)\n",
    "    bctrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the Dagger results on Hopper, we see that Dagger is able to recover the performance of the expert policy after a few iterations of online interaction and expert relabeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "Address already in use\n",
       "Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualize Dagger results on Hopper\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/dagger/Hopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
